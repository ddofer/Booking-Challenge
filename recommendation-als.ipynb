{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cc8f0a11-d9b4-48b2-8af4-b41500747c0b",
    "_uuid": "2f249cc6ef4f014a49bb0d114fbeec0afe8c96b3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import vstack\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8b5039de-2337-4424-9872-c4c05385df48",
    "_uuid": "0371c8f7a46066ee6d79432bba7426068c6dae7c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "events = pd.read_csv('../input/events.csv')\n",
    "category_tree = pd.read_csv('../input/category_tree.csv')\n",
    "items1 = pd.read_csv('../input/item_properties_part1.csv')\n",
    "items2 = pd.read_csv('../input/item_properties_part2.csv')\n",
    "items = pd.concat([items1, items2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9ce84595-9578-4faf-b9a7-624c9d1ca8fb",
    "_uuid": "a6d42f58028a860b595291f3bfd4c7694e58b5c0"
   },
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "eb95ae6e-ca1f-47b9-aa33-204f055439c5",
    "_uuid": "7eccc2f1a13a94592167d187dac0bbf4220afbf3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#user_with_buy = dict()\n",
    "#index = 0\n",
    "#for row in events.itertuples():\n",
    "#    if row.event == 'addtocart' or row.event == 'transaction':\n",
    "#        if row[2] not in user_with_buy:\n",
    "#            user_with_buy[row[2]] = index\n",
    "#            index = index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "54f117a6-9db8-4dce-85c8-de5ed8f7f63f",
    "_uuid": "35814dee33a59a6eb1621c7cb1c65dc04fdb900b"
   },
   "source": [
    "# Construct the user-to-item matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "39af0516-f258-4c0a-ab30-81aa8fc10c05",
    "_uuid": "148d08bd3ca27e964cabc1fd77341d9a885c035b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_users = events['visitorid'].unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1c093df2-f5cf-4c47-b1db-21985410a893",
    "_uuid": "d242b900d8898a34979515ae16324c04598e5b05",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_items = items['itemid'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fb2ea817-f8ad-40ac-aada-4c3efc7453c4",
    "_uuid": "5df7cdbe869ec5a0cf0ac61ee3445ec654e5ebe5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (str(n_users) +\" \" +  str(n_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "640411ce-a5ab-40bc-b928-a7be4a2437f1",
    "_uuid": "543e75df2f9cc1c101e750d728f626d766e55ce3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_to_item_matrix = sp.dok_matrix((n_users+1, n_items+2), dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2da7c395-a680-4b8b-9c58-3c8148a1af66",
    "_uuid": "14b6837541c5ec680ebc974d87f84f9b0325abd5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We need to check whether we need to add the frequency of view, addtocart and transation.\n",
    "# Currently we are only taking a single value for each row and column.\n",
    "\n",
    "action_weights = [1,2,3]\n",
    "for row in events.itertuples():\n",
    "#    if row[2] not in user_with_buy:\n",
    "#        continue\n",
    "#    mapped_user_key = user_with_buy[row[2]]\n",
    "    mapped_user_key = row[2]\n",
    "    if row.event == 'view':\n",
    "            user_to_item_matrix[mapped_user_key, row[4]] = action_weights[0]\n",
    "    elif row.event == 'addtocart':\n",
    "            user_to_item_matrix[mapped_user_key, row[4]] = action_weights[1]        \n",
    "    elif row.event == 'transaction':\n",
    "            user_to_item_matrix[mapped_user_key, row[4]] = action_weights[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2a807afb-f660-47cc-98d2-7a83fb3a0996",
    "_uuid": "76919903392744ddb0d52788da60cbb17e2da68a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_to_item_matrix = user_to_item_matrix.tocsr()\n",
    "print (user_to_item_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a975c0e2-0b64-46d7-87b1-ce7a83fd95ba",
    "_uuid": "3413418ff8dd936b1acb204496201126e3d15ef6"
   },
   "source": [
    "# Training and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "009bde3b-63e8-4ae3-9c77-b85f51467254",
    "_uuid": "d62bd82d0b7649ca91311895d06b066b2fe32afc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparsity = float(len(user_to_item_matrix.nonzero()[0]))\n",
    "sparsity /= (user_to_item_matrix.shape[0] * user_to_item_matrix.shape[1])\n",
    "sparsity *= 100\n",
    "print (sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "978c76d9-fa7c-4165-bda6-d999b767e540",
    "_uuid": "b4cb6fe62d377c2d435ddc1ba014d5e92f4528f3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(user_to_item_matrix, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "42b1e919-d865-4489-8d13-8518584493f6",
    "_uuid": "5e9a04ee6344368665bc8b4ebd9d8f5f5fea21f0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a4dc887c-1726-4cb1-847c-bd646581087b",
    "_uuid": "a1d7f9c5fb773b41f2e69619cbaf9af2329f0b5b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d6c41a32-bbce-42cb-98a6-4ef24f5ad42d",
    "_uuid": "732f4160cb28fd1d2986c6566796fd13cb31fa42"
   },
   "source": [
    "# Collaborative filtering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4c0db36b-cae1-403d-840d-e2d1b9d414a7",
    "_uuid": "eb5f9e6c0b24967cdd64a58c2001bf83cecb0394",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2a1753e0-4476-4cd4-a8d7-86ae3dfa26d8",
    "_uuid": "5627ee4eacd8de83c9ec4faecd05f73da18a65c7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: this is user to user similarity. check item to item similarity as well\n",
    "cosine_similarity_matrix = cosine_similarity(X_train, X_train, dense_output=False)\n",
    "cosine_similarity_matrix.setdiag(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7a377c50-4a72-448a-a770-427842013cfc",
    "_uuid": "395c2544474f33be6c62b0bf9d1baf687f1344a4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cosine_similarity_matrix_ll=cosine_similarity_matrix.tolil()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ef6ac637-308a-491a-b248-cfee7f0fdac4",
    "_uuid": "5772cdb3c61f9128cd5bec05d13e4d4504834c7e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cosine_similarity_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a12376a8-e7aa-4a7e-b563-7de37d9052c5",
    "_uuid": "07b6abc19a7263a085dc8c1d05b2a25726ea74be",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def implicit_weighted_ALS(training_set, lambda_val = 0.1, alpha = 40, iterations = 10, rank_size = 20, seed = 0):\n",
    "    '''\n",
    "    Implicit weighted ALS taken from Hu, Koren, and Volinsky 2008. Designed for alternating least squares and implicit\n",
    "    feedback based collaborative filtering. \n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    training_set - Our matrix of ratings with shape m x n, where m is the number of users and n is the number of items.\n",
    "    Should be a sparse csr matrix to save space. \n",
    "    \n",
    "    lambda_val - Used for regularization during alternating least squares. Increasing this value may increase bias\n",
    "    but decrease variance. Default is 0.1. \n",
    "    \n",
    "    alpha - The parameter associated with the confidence matrix discussed in the paper, where Cui = 1 + alpha*Rui. \n",
    "    The paper found a default of 40 most effective. Decreasing this will decrease the variability in confidence between\n",
    "    various ratings.\n",
    "    \n",
    "    iterations - The number of times to alternate between both user feature vector and item feature vector in\n",
    "    alternating least squares. More iterations will allow better convergence at the cost of increased computation. \n",
    "    The authors found 10 iterations was sufficient, but more may be required to converge. \n",
    "    \n",
    "    rank_size - The number of latent features in the user/item feature vectors. The paper recommends varying this \n",
    "    between 20-200. Increasing the number of features may overfit but could reduce bias. \n",
    "    \n",
    "    seed - Set the seed for reproducible results\n",
    "    \n",
    "    returns:\n",
    "    \n",
    "    The feature vectors for users and items. The dot product of these feature vectors should give you the expected \n",
    "    \"rating\" at each point in your original matrix. \n",
    "    '''\n",
    "    \n",
    "    # first set up our confidence matrix\n",
    "    \n",
    "    conf = (alpha*training_set) # To allow the matrix to stay sparse, I will add one later when each row is taken \n",
    "                                # and converted to dense. \n",
    "    num_user = conf.shape[0]\n",
    "    num_item = conf.shape[1] # Get the size of our original ratings matrix, m x n\n",
    "    \n",
    "    # initialize our X/Y feature vectors randomly with a set seed\n",
    "    rstate = np.random.RandomState(seed)\n",
    "    \n",
    "    X = sparse.csr_matrix(rstate.normal(size = (num_user, rank_size))) # Random numbers in a m x rank shape\n",
    "    Y = sparse.csr_matrix(rstate.normal(size = (num_item, rank_size))) # Normally this would be rank x n but we can \n",
    "                                                                 # transpose at the end. Makes calculation more simple.\n",
    "    X_eye = sparse.eye(num_user)\n",
    "    Y_eye = sparse.eye(num_item)\n",
    "    lambda_eye = lambda_val * sparse.eye(rank_size) # Our regularization term lambda*I. \n",
    "    \n",
    "    # We can compute this before iteration starts. \n",
    "    \n",
    "    # Begin iterations\n",
    "   \n",
    "    for iter_step in range(iterations): # Iterate back and forth between solving X given fixed Y and vice versa\n",
    "        # Compute yTy and xTx at beginning of each iteration to save computing time\n",
    "        yTy = Y.T.dot(Y)\n",
    "        xTx = X.T.dot(X)\n",
    "        # Being iteration to solve for X based on fixed Y\n",
    "        for u in range(num_user):\n",
    "            conf_samp = conf[u,:].toarray() # Grab user row from confidence matrix and convert to dense\n",
    "            pref = conf_samp.copy() \n",
    "            pref[pref != 0] = 1 # Create binarized preference vector \n",
    "            CuI = sparse.diags(conf_samp, [0]) # Get Cu - I term, don't need to subtract 1 since we never added it \n",
    "            yTCuIY = Y.T.dot(CuI).dot(Y) # This is the yT(Cu-I)Y term \n",
    "            yTCupu = Y.T.dot(CuI + Y_eye).dot(pref.T) # This is the yTCuPu term, where we add the eye back in\n",
    "                                                      # Cu - I + I = Cu\n",
    "            X[u] = spsolve(yTy + yTCuIY + lambda_eye, yTCupu) \n",
    "            # Solve for Xu = ((yTy + yT(Cu-I)Y + lambda*I)^-1)yTCuPu, equation 4 from the paper  \n",
    "        # Begin iteration to solve for Y based on fixed X \n",
    "        for i in range(num_item):\n",
    "            conf_samp = conf[:,i].T.toarray() # transpose to get it in row format and convert to dense\n",
    "            pref = conf_samp.copy()\n",
    "            pref[pref != 0] = 1 # Create binarized preference vector\n",
    "            CiI = sparse.diags(conf_samp, [0]) # Get Ci - I term, don't need to subtract 1 since we never added it\n",
    "            xTCiIX = X.T.dot(CiI).dot(X) # This is the xT(Cu-I)X term\n",
    "            xTCiPi = X.T.dot(CiI + X_eye).dot(pref.T) # This is the xTCiPi term\n",
    "            Y[i] = spsolve(xTx + xTCiIX + lambda_eye, xTCiPi)\n",
    "            # Solve for Yi = ((xTx + xT(Cu-I)X) + lambda*I)^-1)xTCiPi, equation 5 from the paper\n",
    "    # End iterations\n",
    "    return X, Y.T # Transpose at the end to make up for not being transposed at the beginning. \n",
    "                         # Y needs to be rank x n. Keep these as separate matrices for scale reasons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c6a50430-e2e9-42a7-8fda-7966d72aadeb",
    "_uuid": "e6f9d21c5b28ef91af6a4dcf03c38b94ed30391d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! pip install implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a0be1b12-9e8d-49cf-974f-8d149ce95577",
    "_uuid": "6f6fe919f181856eac32f3bcf4010cd7a8264f42",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_vecs, item_vecs = implicit_weighted_ALS(X_train, lambda_val = 0.1, alpha = 15, iterations = 1,\n",
    "                                            rank_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6719955a-cd21-41df-8824-00c97bba39b3",
    "_uuid": "33c6b4af517b0f1fc1bc3724a73ac0680b8ba30b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_n(row_data, row_indices, n):\n",
    "        i = row_data.argsort()[-n:]\n",
    "        # i = row_data.argpartition(-n)[-n:]\n",
    "        top_values = row_data[i]\n",
    "        top_indices = row_indices[i]  # do the sparse indices matter?\n",
    "        return top_values, top_indices, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9de67428-bbf8-411e-937a-5a39c6739121",
    "_uuid": "3af79ba586c58437686dd9dbefbed9d2bc1d30d3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_topk(ratings, similarity, kind='user', k=40):\n",
    "    pred = sp.csr_matrix((0,ratings.shape[1]), dtype=np.int8)\n",
    "    if kind == 'user':\n",
    "        for i in range(similarity.shape[0]):\n",
    "            top_k_values, top_k_users = max_n(np.array(similarity.data[i]),np.array(similarity.rows[i]),k)[:2]\n",
    "            current = top_k_values.reshape(1,-1).dot(ratings[top_k_users].todense())\n",
    "            current /= np.sum(np.abs(top_k_values))+1\n",
    "            vstack([pred, current])\n",
    "    if kind == 'item':\n",
    "        for j in range(ratings.shape[1]):\n",
    "            top_k_items = [np.argsort(similarity[:,j])[:-k-1:-1]]\n",
    "            for i in range(ratings.shape[0]):\n",
    "                pred[i, j] = similarity[j, :][top_k_items].dot(ratings[i, :][top_k_items].T) \n",
    "                pred[i, j] /= np.sum(np.abs(similarity[j, :][top_k_items]))        \n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "582daa1c-82d3-42cd-81b8-ca0ca6e58ac1",
    "_uuid": "d7a40c5c861b096719e1e5635bb19854c0058afd",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = predict_topk(X_train, cosine_similarity_matrix_ll, kind='user', k=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
