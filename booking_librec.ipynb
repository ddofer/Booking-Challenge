{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LibRecommender models experiments\n",
    "\n",
    "* https://github.com/massquantity/LibRecommender/tree/master/examples\n",
    "\n",
    "* Try different models. \n",
    "* Note that we explicitly create negatives.\n",
    "* Can do train test split by time or other things\n",
    "\n",
    "\n",
    "* Data Format\n",
    "    * One thing is important, the model assumes that user, item, and label column index are 0, 1, and 2, respectively. You may wish to change the column order if that's not the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible approahc + negatives - https://github.com/zhangruiskyline/DeepLearning/blob/master/doc/Recommendation.md#ranking \n",
    "\n",
    "\n",
    "* Negative sampling from the sparse user-item cooccurrence matrix\n",
    "    * https://stackoverflow.com/questions/49971318/how-to-generate-negative-samples-in-tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from catboost import CatBoostClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "import numpy as np\n",
    "import random\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, GroupShuffleSplit\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.metrics import TopKCategoricalAccuracy, Precision, SparseTopKCategoricalAccuracy # @4\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from libreco.data import split_by_ratio_chrono, DatasetFeat, split_by_ratio\n",
    "from libreco.algorithms import (\n",
    "    FM, WideDeep, DeepFM, AutoInt, DIN, YouTubeMatch, YouTubeRanking,ItemCF\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_state(name):\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    print(\"\\n\", \"=\" * 30, name, \"=\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: GeForce RTX 2060 (UUID: GPU-d8cefda9-d4cb-990c-cc01-a2a4f2416484)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\mixed_precision\\loss_scale.py:56: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.keras.layers.enable_v2_dtype_behavior()\n",
    "## https://www.tensorflow.org/guide/mixed_precision ## TF mixed precision - pytorch requires other setup\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)\n",
    "## will need to correct in places, e.g.: \n",
    "## outputs = layers.Activation('softmax', dtype='float32', name='predictions')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features to add:\n",
    "* Lag \n",
    "* Rank (popularity) of city, country (in general, +- given booker country)\n",
    "* Count of hotel; user, trip size ? (may be leaky )\n",
    "* Seasonal features - Holidays? , datetime\n",
    "\n",
    "Aggregate feats:\n",
    "* user changed country? last booking (lag 1) country change? \n",
    "* max/min/avg popularity rank of previous locations visited\n",
    "\n",
    "\n",
    "\n",
    "We should create a dictionary of the rank, count, city/country etc' feats, so we can easily merge them when making more \"negative\" samples/feats for ranking.\n",
    "\n",
    "\n",
    "* Consider using a df2 of df without dates + drop_duplicates, +- without user/trip id (After calcing that) .\n",
    "\n",
    "\n",
    "Leaky or potentially leaky (Dependso n test set): \n",
    "* Target freq features - frequency of target city, given source county +- affiliate +- month of year +- given country (and interactions of target freq). \n",
    "    * Risk of leaks - depends of test data has temporal split or not. \n",
    "    * cartboost can do target encode, but this lets us do it for interactions, e.g. target city freq given the 2 countries and affiliate.\n",
    "    * beware overfitting! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_TARGET_FREQ = 25 # drop target/city_id values that appear less than this many times, as final step's target \n",
    "KEEP_TOP_K_TARGETS = 0 # keep K most frequent city ID targets (redundnat with the above, )\n",
    "\n",
    "## (some) categorical variables that appear less than this many times will be replaced with a placeholder value!\n",
    "## Includes CITY id (but done after target filtering, to avoid creating a \"rare class\" target:\n",
    "LOW_COUNT_THRESH = 10\n",
    "\n",
    "RUN_TABNET = False\n",
    "max_epochs = 15\n",
    "\n",
    "GET_COUNT_AGG_FEATS = False ## disable getting count, rank etc' groupby features , for speedup\n",
    "\n",
    "DROP_FIRST_ROW =  True #False  ## drop first interaction per user from train data\n",
    "\n",
    "## for matrix factorization/CF:\n",
    "### morte possible ID_cols :  # last (last step in trip) - would double data per user incorrectly\n",
    "### hotel_country_lag1 , city_id_lag1  (very relevant - needs shared embeddingm and would increase cardinality a lot.. ) \n",
    "ID_COLS = [\n",
    "#     'device_class',\n",
    "#            'affiliate_id',\n",
    "           'booker_country',\n",
    "           'checkin_quarter',\n",
    "#            \"last\",\n",
    "          \"first_hotel_country\"] \n",
    "MF_KEEP_COLS = [\"ID\"]+ID_COLS+['city_id',\"hotel_country\"]\n",
    "\n",
    "SAVE_TO_DISK = False\n",
    "\n",
    "TARGET_COL =  'city_id' #\"city_id\"#'hotel_country' \n",
    "USER_ID_COL = \"utrip_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most basic categorical columns , without 'user_id', , 'utrip_id' ordevice_class - used for count encoding/filtering\n",
    "BASE_CAT_COLS = ['city_id',  'affiliate_id', 'booker_country', 'hotel_country']\n",
    "\n",
    "# ### features to get lags for. Not very robust. May want different feats for lags before -1\n",
    "# LAG_FEAT_COLS = ['city_id', 'device_class',\n",
    "#        'affiliate_id', 'booker_country', 'hotel_country', \n",
    "#        'duration', 'same_country', 'checkin_day', 'checkin_weekday',\n",
    "#        'checkin_week',\n",
    "#         'checkout_weekday','checkout_week',\n",
    "#        'city_id_count', 'affiliate_id_count',\n",
    "#        'booker_country_count', 'hotel_country_count', \n",
    "#        'checkin_month_count', 'checkin_week_count', 'city_id_nunique',\n",
    "#        'affiliate_id_nunique', 'booker_country_nunique',\n",
    "#        'hotel_country_nunique', 'city_id_rank_by_hotel_country',\n",
    "#        'city_id_rank_by_booker_country', 'city_id_rank_by_affiliate',\n",
    "#        'affiliate_id_rank_by_hotel_country',\n",
    "#        'affiliate_id_rank_by_booker_country', 'affiliate_id_rank_by_affiliate',\n",
    "#        'booker_country_rank_by_hotel_country',\n",
    "#        'booker_country_rank_by_booker_country',\n",
    "#        'booker_country_rank_by_affiliate',\n",
    "#        'hotel_country_rank_by_hotel_country',\n",
    "#        'hotel_country_rank_by_booker_country',\n",
    "#        'hotel_country_rank_by_affiliate',\n",
    "#        'checkin_month_rank_by_hotel_country',\n",
    "#        'checkin_month_rank_by_booker_country',\n",
    "#        'checkin_month_rank_by_affiliate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/33907537/groupby-and-lag-all-columns-of-a-dataframe\n",
    "# https://stackoverflow.com/questions/62924987/lag-multiple-variables-grouped-by-columns\n",
    "## lag features with groupby over many columns: \n",
    "def groupbyLagFeatures(df:pd.DataFrame,lag:[]=[1,2],group=\"utrip_id\",lag_feature_cols=[]):\n",
    "    \"\"\"\n",
    "    lag features with groupby over many columns.\n",
    "    Assumes sorted data!\n",
    "    https://stackoverflow.com/questions/62924987/lag-multiple-variables-grouped-by-columns\"\"\"\n",
    "    if len(lag_feature_cols)>0:\n",
    "        df=pd.concat([df]+[df.groupby(group)[lag_feature_cols].shift(x).add_prefix('lag'+str(x)+\"_\") for x in lag],axis=1)\n",
    "    else:\n",
    "         df=pd.concat([df]+[df.groupby(group).shift(x).add_prefix('lag'+str(x)+\"_\") for x in lag],axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def groupbyFirstLagFeatures(df:pd.DataFrame,group=\"user_id\",lag_feature_cols=[]):\n",
    "    \"\"\"\n",
    "    Get  first/head value lag-like of features with groupby over columns. Assumes sorted data!\n",
    "    \"\"\"\n",
    "    if len(lag_feature_cols)>0:\n",
    "        df=pd.concat([df]+[df.groupby(group)[lag_feature_cols].transform(\"first\").add_prefix(\"first_\")],axis=1)\n",
    "    else:\n",
    "#          df=pd.concat([df]+[df.groupby(group).first().add_prefix(\"first_\")],axis=1)\n",
    "        df=pd.concat([df]+[df.groupby(group).transform(\"first\").add_prefix(\"first_\")],axis=1)\n",
    "    return df\n",
    "\n",
    "######## Get n most popular items, per group\n",
    "def most_popular(group, n_max=4):\n",
    "    \"\"\"Find most popular hotel clusters by destination\n",
    "    Define a function to get most popular hotels for a destination group.\n",
    "\n",
    "    Previous version used nlargest() Series method to get indices of largest elements. But the method is rather slow.\n",
    "    Source: https://www.kaggle.com/dvasyukova/predict-hotel-type-with-pandas\n",
    "    \"\"\"\n",
    "    relevance = group['relevance'].values\n",
    "    hotel_cluster = group['hotel_cluster'].values\n",
    "    most_popular = hotel_cluster[np.argsort(relevance)[::-1]][:n_max]\n",
    "    return np.array_str(most_popular)[1:-1] # remove square brackets\n",
    "\n",
    "\n",
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=64,target_col=\"target\"):\n",
    "    \"\"\"\n",
    "    Wrap dataframes with tf.data. \n",
    "    This will enable us to use feature columns as a bridge to map from the columns in a dataframe to features used to train the model.\n",
    "    https://www.tensorflow.org/tutorials/structured_data/feature_columns#create_an_input_pipeline_using_tfdata\n",
    "    \"\"\"\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop(target_col)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds\n",
    "\n",
    "\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id           200153\n",
      "checkin              425\n",
      "checkout             425\n",
      "city_id            39901\n",
      "device_class           3\n",
      "affiliate_id        3254\n",
      "booker_country         5\n",
      "hotel_country        195\n",
      "utrip_id          217686\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>checkin</th>\n",
       "      <th>checkout</th>\n",
       "      <th>city_id</th>\n",
       "      <th>device_class</th>\n",
       "      <th>affiliate_id</th>\n",
       "      <th>booker_country</th>\n",
       "      <th>hotel_country</th>\n",
       "      <th>utrip_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>519017</th>\n",
       "      <td>727105</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>18820</td>\n",
       "      <td>mobile</td>\n",
       "      <td>359</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>727105_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986596</th>\n",
       "      <td>2000964</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>63341</td>\n",
       "      <td>mobile</td>\n",
       "      <td>8151</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>2000964_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7504</th>\n",
       "      <td>2595109</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>27404</td>\n",
       "      <td>mobile</td>\n",
       "      <td>359</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>2595109_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52884</th>\n",
       "      <td>110418</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>3763</td>\n",
       "      <td>desktop</td>\n",
       "      <td>9924</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Glubbdubdrib</td>\n",
       "      <td>110418_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068508</th>\n",
       "      <td>221863</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>45597</td>\n",
       "      <td>mobile</td>\n",
       "      <td>7774</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>221863_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908844</th>\n",
       "      <td>5755992</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>52860</td>\n",
       "      <td>desktop</td>\n",
       "      <td>4568</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Axphain</td>\n",
       "      <td>5755992_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154024</th>\n",
       "      <td>5842454</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>35850</td>\n",
       "      <td>desktop</td>\n",
       "      <td>384</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Rook Islands</td>\n",
       "      <td>5842454_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160065</th>\n",
       "      <td>5936647</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>20199</td>\n",
       "      <td>desktop</td>\n",
       "      <td>4541</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Kasnia</td>\n",
       "      <td>5936647_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244942</th>\n",
       "      <td>5955565</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>54384</td>\n",
       "      <td>desktop</td>\n",
       "      <td>8132</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>Patusan</td>\n",
       "      <td>5955565_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787249</th>\n",
       "      <td>6172320</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>6005</td>\n",
       "      <td>mobile</td>\n",
       "      <td>9452</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Patusan</td>\n",
       "      <td>6172320_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1166835 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id    checkin    checkout  city_id device_class  affiliate_id  \\\n",
       "519017    727105 2015-12-31  2016-01-01    18820       mobile           359   \n",
       "986596   2000964 2015-12-31  2016-01-01    63341       mobile          8151   \n",
       "7504     2595109 2015-12-31  2016-01-01    27404       mobile           359   \n",
       "52884     110418 2016-01-01  2016-01-02     3763      desktop          9924   \n",
       "1068508   221863 2016-01-01  2016-01-02    45597       mobile          7774   \n",
       "...          ...        ...         ...      ...          ...           ...   \n",
       "908844   5755992 2017-02-27  2017-02-28    52860      desktop          4568   \n",
       "1154024  5842454 2017-02-27  2017-02-28    35850      desktop           384   \n",
       "1160065  5936647 2017-02-27  2017-02-28    20199      desktop          4541   \n",
       "244942   5955565 2017-02-27  2017-02-28    54384      desktop          8132   \n",
       "787249   6172320 2017-02-27  2017-02-28     6005       mobile          9452   \n",
       "\n",
       "               booker_country hotel_country   utrip_id  \n",
       "519017   The Devilfire Empire  Cobra Island   727105_1  \n",
       "986596   The Devilfire Empire  Cobra Island  2000964_1  \n",
       "7504     The Devilfire Empire  Cobra Island  2595109_1  \n",
       "52884    The Devilfire Empire  Glubbdubdrib   110418_1  \n",
       "1068508                Gondal        Gondal   221863_1  \n",
       "...                       ...           ...        ...  \n",
       "908844                 Gondal       Axphain  5755992_4  \n",
       "1154024                Gondal  Rook Islands  5842454_6  \n",
       "1160065                Gondal        Kasnia  5936647_2  \n",
       "244942                Elbonia       Patusan  5955565_2  \n",
       "787249   The Devilfire Empire       Patusan  6172320_1  \n",
       "\n",
       "[1166835 rows x 9 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"booking_train_set.csv\",\n",
    "#                  nrows=123456,\n",
    "                 index_col=[0],\n",
    "                 parse_dates=[\"checkin\"],infer_datetime_format=True)\n",
    "\n",
    "df.sort_values([ \"checkin\",\n",
    "                \"user_id\"],inplace=True)\n",
    "print(df.nunique())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### i disabled most of thefeature eztraction here for simplicity\n",
    "\n",
    "# df[\"duration\"] = (df[\"checkout\"] - df[\"checkin\"]).dt.days\n",
    "# df[\"same_country\"] = (df[\"booker_country\"]==df[\"hotel_country\"]).astype(int)\n",
    "\n",
    "# df[\"checkin_day\"] = df[\"checkin\"].dt.day\n",
    "# df[\"checkin_weekday\"] = df[\"checkin\"].dt.weekday\n",
    "df[\"checkin_week\"] = df[\"checkin\"].dt.isocalendar().week.astype(int) ## week of year\n",
    "df[\"checkin_month\"] = df[\"checkin\"].dt.month\n",
    "# df[\"checkin_year\"] = df[\"checkin\"].dt.year-2016\n",
    "\n",
    "df[\"checkin_quarter\"] = df[\"checkin\"].dt.quarter # relatively redundant but may be used for \"id\"\n",
    "\n",
    "# df[\"checkin_quarter\"] = df[\"checkin_quarter\"]/4 # scale. could also do cos, sin extraction. makesi t a float instead of int/embedding\n",
    "\n",
    "\n",
    "# df[\"checkout_weekday\"] = df[\"checkout\"].dt.weekday\n",
    "# df[\"checkout_week\"] = df[\"checkout\"].dt.isocalendar().week.astype(int) ## week of year\n",
    "# df[\"checkout_day\"] = df[\"checkout\"].dt.day ## day of month\n",
    "\n",
    "## cyclical datetime embeddings\n",
    "## drop originakl variables? \n",
    "## TODO:L add for other variables, +- those that we'll embed (week?)\n",
    "\n",
    "# df['checkin_weekday_sin'] = np.sin(df[\"checkin_weekday\"]*(2.*np.pi/7))\n",
    "# df['checkin_weekday_cos'] = np.cos(df[\"checkin_weekday\"]*(2.*np.pi/7))\n",
    "# df['checkin_month_sin'] = np.sin((df[\"checkin_month\"]-1)*(2.*np.pi/12))\n",
    "# df['checkin_month_cos'] = np.cos((df[\"checkin_month\"]-1)*(2.*np.pi/12))\n",
    "df['checkin_week_sin'] = np.sin((df[\"checkin_week\"]-1)*(2.*np.pi/53))\n",
    "df['checkin_week_cos'] = np.cos((df[\"checkin_week\"]-1)*(2.*np.pi/53))\n",
    "\n",
    "\n",
    "# #############\n",
    "# # last number in utrip id - probably which trip number it is:\n",
    "# df[\"utrip_number\"] = df[\"utrip_id\"].str.split(\"_\",expand=True)[1].astype(int)\n",
    "\n",
    "### encode string columns - must be consistent with test data \n",
    "### IF we can concat test with train, we can just do a single transformation  for the NON TARGET cols\n",
    "# obj_cols_list = df.select_dtypes(\"O\").columns.values\n",
    "obj_cols_list = ['device_class','booker_country','hotel_country',\n",
    "#                 \"city_id\"\n",
    "                ] # we could also define when loading data, dtype\n",
    "\n",
    "# for c in obj_cols_list:\n",
    "#     df[c] = df[c].astype(\"category\")\n",
    "#     df[c] = df[c].cat.codes.astype(int)\n",
    "#     print(\"min\",df[c].min()) # min is 0 - which is what the embedding (indices) will expect\n",
    "\n",
    "## view steps of a trip per user & trip, in order. ## last step == 1.\n",
    "## count #/pct step in a trip (utrip_id) per user. Useful to get the \"final\" step per trip - for prediction\n",
    "## note that the order is ascending, so we would need to select by \"last\" . (i.e \"1\" is the first step, 2 the second, etc') , or we could use pct .rank(ascending=True,pct=True)\n",
    "#### this feature overlaps with the count of each trip id (for the final row)\n",
    "##  = df.sort_values([\"checkin\",\"checkout\"])... - df already sorted above\n",
    "# df[\"utrip_steps_from_end\"] = df.groupby(\"utrip_id\")[\"checkin\"].rank(ascending=True,pct=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### add features to be consistent with test set of row in trip, and total trips in trip\n",
    "\n",
    "df[\"row_num\"] = df.groupby(\"utrip_id\")[\"checkin\"].rank(ascending=True,pct=False).astype(int)\n",
    "utrip_counts = df[\"utrip_id\"].value_counts()\n",
    "df[\"total_rows\"] = df[\"utrip_id\"].map(utrip_counts)\n",
    "\n",
    "### last step in trip\n",
    "df[\"last\"] = (df[\"total_rows\"]==df[\"row_num\"]).astype(int)\n",
    "\n",
    "df[\"total_rows\"].describe();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### replace rare categorical variable(s) - affiliates\n",
    "* replace rare variables (under 2 occurrences) with \"-1\" dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: 9924     277775\n",
      "359      171385\n",
      "384       88137\n",
      "9452      85476\n",
      "4541      41504\n",
      "          ...  \n",
      "8351          1\n",
      "8464          1\n",
      "2202          1\n",
      "10513         1\n",
      "2047          1\n",
      "Name: affiliate_id, Length: 3254, dtype: int64\n",
      "uniques 3254\n",
      "after\n",
      " 9924    277775\n",
      "359     171385\n",
      "384      88137\n",
      "9452     85476\n",
      "4541     41504\n",
      "         ...  \n",
      "2615         3\n",
      "5963         3\n",
      "2618         3\n",
      "838          3\n",
      "176          3\n",
      "Name: affiliate_id, Length: 2152, dtype: int64\n",
      "uniques 2152\n"
     ]
    }
   ],
   "source": [
    "### replace rare variables (under 2 occurrences) with \"-1\" dummy\n",
    "\n",
    "affiliates_counts = df[\"affiliate_id\"].value_counts()\n",
    "print(\"before:\", affiliates_counts)\n",
    "print(\"uniques\",df[\"affiliate_id\"].nunique())\n",
    "affiliates_counts = affiliates_counts.to_dict()\n",
    "# df[\"affiliate_id\"] = df[\"affiliate_id\"].where(df[\"affiliate_id\"].apply(lambda x: x.map(x.value_counts()))>=3, -1)\n",
    "df[\"affiliate_id\"] = df[\"affiliate_id\"].where(df[\"affiliate_id\"].map(affiliates_counts)>=3, -2)\n",
    "df[\"affiliate_id\"] = df[\"affiliate_id\"].astype(int)\n",
    "\n",
    "print(\"after\\n\",df[\"affiliate_id\"].value_counts())\n",
    "print(\"uniques\",df[\"affiliate_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"total_rows\"].map(pd.cut(df[\"total_rows\"],bins=3,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add LAG feature(s)  + DROP First visited location rows\n",
    "* \"first\" hotel country (vs most recent country visited)\n",
    "    * `groupbyFirstLagFeatures(df,group=\"user_id\",lag_feature_cols=[\"hotel_country\",\"city_id\"])`\n",
    "* Can consider: lag1 hotel_country, hotel_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_hotel_country      174\n",
      "hotel_country            195\n",
      "city_id                39901\n",
      "first_city_id          19674\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## add the \"first\" place visited/values\n",
    "### nopte - will need to drop first row in trip, or impute nans when using this feature \n",
    "\n",
    "### first by user results in too much sparsity/rareness for our IDs purposes\n",
    "# df = groupbyFirstLagFeatures(df,group=\"user_id\",lag_feature_cols=[\"hotel_country\"]) # [\"hotel_country\",\"city_id\"]\n",
    "\n",
    "## alt - messy, but maybe good enough : \n",
    "# df = groupbyFirstLagFeatures(df,group=['device_class', 'affiliate_id',\n",
    "#                                        'booker_country','checkin_month',\"last\"],\n",
    "#                              lag_feature_cols=[\"hotel_country\",\"city_id\",'device_class', 'affiliate_id',\n",
    "#                                               \"checkin_quarter\",\"checkin_month\",\"booker_country\"])\n",
    "\n",
    "df = groupbyFirstLagFeatures(df,group=['utrip_id'],\n",
    "                             lag_feature_cols=[\"hotel_country\",\"city_id\",'device_class', 'affiliate_id',\n",
    "                                              \"checkin_month\",\"booker_country\"])\n",
    "\n",
    "print(df[[\"first_hotel_country\",\"hotel_country\",\"city_id\",\"first_city_id\"]].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>checkin</th>\n",
       "      <th>checkout</th>\n",
       "      <th>city_id</th>\n",
       "      <th>device_class</th>\n",
       "      <th>affiliate_id</th>\n",
       "      <th>booker_country</th>\n",
       "      <th>hotel_country</th>\n",
       "      <th>utrip_id</th>\n",
       "      <th>checkin_week</th>\n",
       "      <th>checkin_month</th>\n",
       "      <th>checkin_quarter</th>\n",
       "      <th>checkin_week_sin</th>\n",
       "      <th>checkin_week_cos</th>\n",
       "      <th>row_num</th>\n",
       "      <th>total_rows</th>\n",
       "      <th>last</th>\n",
       "      <th>first_hotel_country</th>\n",
       "      <th>first_city_id</th>\n",
       "      <th>first_device_class</th>\n",
       "      <th>first_affiliate_id</th>\n",
       "      <th>first_checkin_month</th>\n",
       "      <th>first_booker_country</th>\n",
       "      <th>lag1_hotel_country</th>\n",
       "      <th>lag1_city_id</th>\n",
       "      <th>lag2_hotel_country</th>\n",
       "      <th>lag2_city_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>519017</th>\n",
       "      <td>727105</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>18820</td>\n",
       "      <td>mobile</td>\n",
       "      <td>359</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>727105_1</td>\n",
       "      <td>53</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.118273</td>\n",
       "      <td>0.992981</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>18820</td>\n",
       "      <td>mobile</td>\n",
       "      <td>359</td>\n",
       "      <td>12</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986596</th>\n",
       "      <td>2000964</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>63341</td>\n",
       "      <td>mobile</td>\n",
       "      <td>8151</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>2000964_1</td>\n",
       "      <td>53</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.118273</td>\n",
       "      <td>0.992981</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>63341</td>\n",
       "      <td>mobile</td>\n",
       "      <td>8151</td>\n",
       "      <td>12</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7504</th>\n",
       "      <td>2595109</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>27404</td>\n",
       "      <td>mobile</td>\n",
       "      <td>359</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>2595109_1</td>\n",
       "      <td>53</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.118273</td>\n",
       "      <td>0.992981</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>27404</td>\n",
       "      <td>mobile</td>\n",
       "      <td>359</td>\n",
       "      <td>12</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52884</th>\n",
       "      <td>110418</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>3763</td>\n",
       "      <td>desktop</td>\n",
       "      <td>9924</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Glubbdubdrib</td>\n",
       "      <td>110418_1</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.118273</td>\n",
       "      <td>0.992981</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>Glubbdubdrib</td>\n",
       "      <td>3763</td>\n",
       "      <td>desktop</td>\n",
       "      <td>9924</td>\n",
       "      <td>1</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068508</th>\n",
       "      <td>221863</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>45597</td>\n",
       "      <td>mobile</td>\n",
       "      <td>7774</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>221863_1</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.118273</td>\n",
       "      <td>0.992981</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>45597</td>\n",
       "      <td>mobile</td>\n",
       "      <td>7774</td>\n",
       "      <td>1</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908844</th>\n",
       "      <td>5755992</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>52860</td>\n",
       "      <td>desktop</td>\n",
       "      <td>4568</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Axphain</td>\n",
       "      <td>5755992_4</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.812487</td>\n",
       "      <td>0.582979</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>36063</td>\n",
       "      <td>desktop</td>\n",
       "      <td>4568</td>\n",
       "      <td>2</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Axphain</td>\n",
       "      <td>52848</td>\n",
       "      <td>Axphain</td>\n",
       "      <td>6090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154024</th>\n",
       "      <td>5842454</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>35850</td>\n",
       "      <td>desktop</td>\n",
       "      <td>384</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Rook Islands</td>\n",
       "      <td>5842454_6</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.812487</td>\n",
       "      <td>0.582979</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Kumbolaland</td>\n",
       "      <td>18032</td>\n",
       "      <td>tablet</td>\n",
       "      <td>8822</td>\n",
       "      <td>2</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Rook Islands</td>\n",
       "      <td>56651</td>\n",
       "      <td>Rook Islands</td>\n",
       "      <td>56651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160065</th>\n",
       "      <td>5936647</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>20199</td>\n",
       "      <td>desktop</td>\n",
       "      <td>4541</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Kasnia</td>\n",
       "      <td>5936647_2</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.812487</td>\n",
       "      <td>0.582979</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>Kasnia</td>\n",
       "      <td>2078</td>\n",
       "      <td>desktop</td>\n",
       "      <td>4541</td>\n",
       "      <td>2</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Kasnia</td>\n",
       "      <td>38254</td>\n",
       "      <td>Kasnia</td>\n",
       "      <td>33946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244942</th>\n",
       "      <td>5955565</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>54384</td>\n",
       "      <td>desktop</td>\n",
       "      <td>8132</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>Patusan</td>\n",
       "      <td>5955565_2</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.812487</td>\n",
       "      <td>0.582979</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Patusan</td>\n",
       "      <td>60966</td>\n",
       "      <td>desktop</td>\n",
       "      <td>10332</td>\n",
       "      <td>2</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>Patusan</td>\n",
       "      <td>36073</td>\n",
       "      <td>Patusan</td>\n",
       "      <td>45485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787249</th>\n",
       "      <td>6172320</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>6005</td>\n",
       "      <td>mobile</td>\n",
       "      <td>9452</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Patusan</td>\n",
       "      <td>6172320_1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.812487</td>\n",
       "      <td>0.582979</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Patusan</td>\n",
       "      <td>60143</td>\n",
       "      <td>mobile</td>\n",
       "      <td>9452</td>\n",
       "      <td>2</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Patusan</td>\n",
       "      <td>40325</td>\n",
       "      <td>Patusan</td>\n",
       "      <td>19183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1166835 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id    checkin    checkout  city_id device_class  affiliate_id  \\\n",
       "519017    727105 2015-12-31  2016-01-01    18820       mobile           359   \n",
       "986596   2000964 2015-12-31  2016-01-01    63341       mobile          8151   \n",
       "7504     2595109 2015-12-31  2016-01-01    27404       mobile           359   \n",
       "52884     110418 2016-01-01  2016-01-02     3763      desktop          9924   \n",
       "1068508   221863 2016-01-01  2016-01-02    45597       mobile          7774   \n",
       "...          ...        ...         ...      ...          ...           ...   \n",
       "908844   5755992 2017-02-27  2017-02-28    52860      desktop          4568   \n",
       "1154024  5842454 2017-02-27  2017-02-28    35850      desktop           384   \n",
       "1160065  5936647 2017-02-27  2017-02-28    20199      desktop          4541   \n",
       "244942   5955565 2017-02-27  2017-02-28    54384      desktop          8132   \n",
       "787249   6172320 2017-02-27  2017-02-28     6005       mobile          9452   \n",
       "\n",
       "               booker_country hotel_country   utrip_id  checkin_week  \\\n",
       "519017   The Devilfire Empire  Cobra Island   727105_1            53   \n",
       "986596   The Devilfire Empire  Cobra Island  2000964_1            53   \n",
       "7504     The Devilfire Empire  Cobra Island  2595109_1            53   \n",
       "52884    The Devilfire Empire  Glubbdubdrib   110418_1            53   \n",
       "1068508                Gondal        Gondal   221863_1            53   \n",
       "...                       ...           ...        ...           ...   \n",
       "908844                 Gondal       Axphain  5755992_4             9   \n",
       "1154024                Gondal  Rook Islands  5842454_6             9   \n",
       "1160065                Gondal        Kasnia  5936647_2             9   \n",
       "244942                Elbonia       Patusan  5955565_2             9   \n",
       "787249   The Devilfire Empire       Patusan  6172320_1             9   \n",
       "\n",
       "         checkin_month  checkin_quarter  checkin_week_sin  checkin_week_cos  \\\n",
       "519017              12                4         -0.118273          0.992981   \n",
       "986596              12                4         -0.118273          0.992981   \n",
       "7504                12                4         -0.118273          0.992981   \n",
       "52884                1                1         -0.118273          0.992981   \n",
       "1068508              1                1         -0.118273          0.992981   \n",
       "...                ...              ...               ...               ...   \n",
       "908844               2                1          0.812487          0.582979   \n",
       "1154024              2                1          0.812487          0.582979   \n",
       "1160065              2                1          0.812487          0.582979   \n",
       "244942               2                1          0.812487          0.582979   \n",
       "787249               2                1          0.812487          0.582979   \n",
       "\n",
       "         row_num  total_rows  last first_hotel_country  first_city_id  \\\n",
       "519017         1           4     0        Cobra Island          18820   \n",
       "986596         1           5     0        Cobra Island          63341   \n",
       "7504           1           4     0        Cobra Island          27404   \n",
       "52884          1          10     0        Glubbdubdrib           3763   \n",
       "1068508        1           8     0              Gondal          45597   \n",
       "...          ...         ...   ...                 ...            ...   \n",
       "908844         5           5     1              Gondal          36063   \n",
       "1154024        9           9     1         Kumbolaland          18032   \n",
       "1160065       12          12     1              Kasnia           2078   \n",
       "244942         5           5     1             Patusan          60966   \n",
       "787249        10          10     1             Patusan          60143   \n",
       "\n",
       "        first_device_class  first_affiliate_id  first_checkin_month  \\\n",
       "519017              mobile                 359                   12   \n",
       "986596              mobile                8151                   12   \n",
       "7504                mobile                 359                   12   \n",
       "52884              desktop                9924                    1   \n",
       "1068508             mobile                7774                    1   \n",
       "...                    ...                 ...                  ...   \n",
       "908844             desktop                4568                    2   \n",
       "1154024             tablet                8822                    2   \n",
       "1160065            desktop                4541                    2   \n",
       "244942             desktop               10332                    2   \n",
       "787249              mobile                9452                    2   \n",
       "\n",
       "         first_booker_country lag1_hotel_country  lag1_city_id  \\\n",
       "519017   The Devilfire Empire                 -1            -1   \n",
       "986596   The Devilfire Empire                 -1            -1   \n",
       "7504     The Devilfire Empire                 -1            -1   \n",
       "52884    The Devilfire Empire                 -1            -1   \n",
       "1068508                Gondal                 -1            -1   \n",
       "...                       ...                ...           ...   \n",
       "908844                 Gondal            Axphain         52848   \n",
       "1154024                Gondal       Rook Islands         56651   \n",
       "1160065                Gondal             Kasnia         38254   \n",
       "244942                Elbonia            Patusan         36073   \n",
       "787249   The Devilfire Empire            Patusan         40325   \n",
       "\n",
       "        lag2_hotel_country  lag2_city_id  \n",
       "519017                  -1            -1  \n",
       "986596                  -1            -1  \n",
       "7504                    -1            -1  \n",
       "52884                   -1            -1  \n",
       "1068508                 -1            -1  \n",
       "...                    ...           ...  \n",
       "908844             Axphain          6090  \n",
       "1154024       Rook Islands         56651  \n",
       "1160065             Kasnia         33946  \n",
       "244942             Patusan         45485  \n",
       "787249             Patusan         19183  \n",
       "\n",
       "[1166835 rows x 27 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## add lag features\n",
    "df = groupbyLagFeatures(df,lag=[1,2],lag_feature_cols=[\"hotel_country\",\"city_id\"])\n",
    "## HAcky: reparse as integers\n",
    "df = df.fillna(-1) ## might be best to fill with max + 1 or something ? negative may cause error ? \n",
    "df = df.infer_objects()\n",
    "df[\"lag1_city_id\"] = df[\"lag1_city_id\"].astype(int)\n",
    "df[\"lag2_city_id\"] = df[\"lag2_city_id\"].astype(int)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id                          int64\n",
       "checkin                 datetime64[ns]\n",
       "checkout                        object\n",
       "city_id                          int64\n",
       "device_class                    object\n",
       "affiliate_id                     int32\n",
       "booker_country                  object\n",
       "hotel_country                   object\n",
       "utrip_id                        object\n",
       "checkin_week                     int32\n",
       "checkin_month                    int64\n",
       "checkin_quarter                  int64\n",
       "checkin_week_sin               float64\n",
       "checkin_week_cos               float64\n",
       "row_num                          int32\n",
       "total_rows                       int64\n",
       "last                             int32\n",
       "first_hotel_country             object\n",
       "first_city_id                    int64\n",
       "first_device_class              object\n",
       "first_affiliate_id               int32\n",
       "first_checkin_month              int64\n",
       "first_booker_country            object\n",
       "lag1_hotel_country              object\n",
       "lag1_city_id                     int32\n",
       "lag2_hotel_country              object\n",
       "lag2_city_id                     int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id                 200153\n",
       "checkin                    425\n",
       "checkout                   425\n",
       "city_id                  39901\n",
       "device_class                 3\n",
       "affiliate_id              2152\n",
       "booker_country               5\n",
       "hotel_country              195\n",
       "utrip_id                217686\n",
       "checkin_week                53\n",
       "checkin_month               12\n",
       "checkin_quarter              4\n",
       "checkin_week_sin            53\n",
       "checkin_week_cos            45\n",
       "row_num                     48\n",
       "total_rows                  41\n",
       "last                         2\n",
       "first_hotel_country        174\n",
       "first_city_id            19674\n",
       "first_device_class           3\n",
       "first_affiliate_id        1916\n",
       "first_checkin_month         12\n",
       "first_booker_country         5\n",
       "lag1_hotel_country         192\n",
       "lag1_city_id             37469\n",
       "lag2_hotel_country         184\n",
       "lag2_city_id             34097\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    39901.000000\n",
      "mean        29.243252\n",
      "std        218.801654\n",
      "min          1.000000\n",
      "25%          1.000000\n",
      "50%          3.000000\n",
      "75%          9.000000\n",
      "max      11242.000000\n",
      "Name: city_id, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>checkin</th>\n",
       "      <th>checkout</th>\n",
       "      <th>city_id</th>\n",
       "      <th>device_class</th>\n",
       "      <th>affiliate_id</th>\n",
       "      <th>booker_country</th>\n",
       "      <th>hotel_country</th>\n",
       "      <th>utrip_id</th>\n",
       "      <th>checkin_week</th>\n",
       "      <th>checkin_month</th>\n",
       "      <th>checkin_quarter</th>\n",
       "      <th>checkin_week_sin</th>\n",
       "      <th>checkin_week_cos</th>\n",
       "      <th>row_num</th>\n",
       "      <th>total_rows</th>\n",
       "      <th>last</th>\n",
       "      <th>first_hotel_country</th>\n",
       "      <th>first_city_id</th>\n",
       "      <th>first_device_class</th>\n",
       "      <th>first_affiliate_id</th>\n",
       "      <th>first_checkin_month</th>\n",
       "      <th>first_booker_country</th>\n",
       "      <th>lag1_hotel_country</th>\n",
       "      <th>lag1_city_id</th>\n",
       "      <th>lag2_hotel_country</th>\n",
       "      <th>lag2_city_id</th>\n",
       "      <th>city_id_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>519017</th>\n",
       "      <td>727105</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>18820</td>\n",
       "      <td>mobile</td>\n",
       "      <td>359</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>727105_1</td>\n",
       "      <td>53</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.118273</td>\n",
       "      <td>0.992981</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>18820</td>\n",
       "      <td>mobile</td>\n",
       "      <td>359</td>\n",
       "      <td>12</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986596</th>\n",
       "      <td>2000964</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>63341</td>\n",
       "      <td>mobile</td>\n",
       "      <td>8151</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>2000964_1</td>\n",
       "      <td>53</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.118273</td>\n",
       "      <td>0.992981</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>63341</td>\n",
       "      <td>mobile</td>\n",
       "      <td>8151</td>\n",
       "      <td>12</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7504</th>\n",
       "      <td>2595109</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>27404</td>\n",
       "      <td>mobile</td>\n",
       "      <td>359</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>2595109_1</td>\n",
       "      <td>53</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.118273</td>\n",
       "      <td>0.992981</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>27404</td>\n",
       "      <td>mobile</td>\n",
       "      <td>359</td>\n",
       "      <td>12</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>3614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52884</th>\n",
       "      <td>110418</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>3763</td>\n",
       "      <td>desktop</td>\n",
       "      <td>9924</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Glubbdubdrib</td>\n",
       "      <td>110418_1</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.118273</td>\n",
       "      <td>0.992981</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>Glubbdubdrib</td>\n",
       "      <td>3763</td>\n",
       "      <td>desktop</td>\n",
       "      <td>9924</td>\n",
       "      <td>1</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>5544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068508</th>\n",
       "      <td>221863</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>45597</td>\n",
       "      <td>mobile</td>\n",
       "      <td>7774</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>221863_1</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.118273</td>\n",
       "      <td>0.992981</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>45597</td>\n",
       "      <td>mobile</td>\n",
       "      <td>7774</td>\n",
       "      <td>1</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908844</th>\n",
       "      <td>5755992</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>52860</td>\n",
       "      <td>desktop</td>\n",
       "      <td>4568</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Axphain</td>\n",
       "      <td>5755992_4</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.812487</td>\n",
       "      <td>0.582979</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>36063</td>\n",
       "      <td>desktop</td>\n",
       "      <td>4568</td>\n",
       "      <td>2</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Axphain</td>\n",
       "      <td>52848</td>\n",
       "      <td>Axphain</td>\n",
       "      <td>6090</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154024</th>\n",
       "      <td>5842454</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>35850</td>\n",
       "      <td>desktop</td>\n",
       "      <td>384</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Rook Islands</td>\n",
       "      <td>5842454_6</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.812487</td>\n",
       "      <td>0.582979</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Kumbolaland</td>\n",
       "      <td>18032</td>\n",
       "      <td>tablet</td>\n",
       "      <td>8822</td>\n",
       "      <td>2</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Rook Islands</td>\n",
       "      <td>56651</td>\n",
       "      <td>Rook Islands</td>\n",
       "      <td>56651</td>\n",
       "      <td>692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160065</th>\n",
       "      <td>5936647</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>20199</td>\n",
       "      <td>desktop</td>\n",
       "      <td>4541</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Kasnia</td>\n",
       "      <td>5936647_2</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.812487</td>\n",
       "      <td>0.582979</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>Kasnia</td>\n",
       "      <td>2078</td>\n",
       "      <td>desktop</td>\n",
       "      <td>4541</td>\n",
       "      <td>2</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Kasnia</td>\n",
       "      <td>38254</td>\n",
       "      <td>Kasnia</td>\n",
       "      <td>33946</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244942</th>\n",
       "      <td>5955565</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>54384</td>\n",
       "      <td>desktop</td>\n",
       "      <td>8132</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>Patusan</td>\n",
       "      <td>5955565_2</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.812487</td>\n",
       "      <td>0.582979</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Patusan</td>\n",
       "      <td>60966</td>\n",
       "      <td>desktop</td>\n",
       "      <td>10332</td>\n",
       "      <td>2</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>Patusan</td>\n",
       "      <td>36073</td>\n",
       "      <td>Patusan</td>\n",
       "      <td>45485</td>\n",
       "      <td>526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787249</th>\n",
       "      <td>6172320</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>6005</td>\n",
       "      <td>mobile</td>\n",
       "      <td>9452</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Patusan</td>\n",
       "      <td>6172320_1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.812487</td>\n",
       "      <td>0.582979</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Patusan</td>\n",
       "      <td>60143</td>\n",
       "      <td>mobile</td>\n",
       "      <td>9452</td>\n",
       "      <td>2</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Patusan</td>\n",
       "      <td>40325</td>\n",
       "      <td>Patusan</td>\n",
       "      <td>19183</td>\n",
       "      <td>367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1166835 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id    checkin    checkout  city_id device_class  affiliate_id  \\\n",
       "519017    727105 2015-12-31  2016-01-01    18820       mobile           359   \n",
       "986596   2000964 2015-12-31  2016-01-01    63341       mobile          8151   \n",
       "7504     2595109 2015-12-31  2016-01-01    27404       mobile           359   \n",
       "52884     110418 2016-01-01  2016-01-02     3763      desktop          9924   \n",
       "1068508   221863 2016-01-01  2016-01-02    45597       mobile          7774   \n",
       "...          ...        ...         ...      ...          ...           ...   \n",
       "908844   5755992 2017-02-27  2017-02-28    52860      desktop          4568   \n",
       "1154024  5842454 2017-02-27  2017-02-28    35850      desktop           384   \n",
       "1160065  5936647 2017-02-27  2017-02-28    20199      desktop          4541   \n",
       "244942   5955565 2017-02-27  2017-02-28    54384      desktop          8132   \n",
       "787249   6172320 2017-02-27  2017-02-28     6005       mobile          9452   \n",
       "\n",
       "               booker_country hotel_country   utrip_id  checkin_week  \\\n",
       "519017   The Devilfire Empire  Cobra Island   727105_1            53   \n",
       "986596   The Devilfire Empire  Cobra Island  2000964_1            53   \n",
       "7504     The Devilfire Empire  Cobra Island  2595109_1            53   \n",
       "52884    The Devilfire Empire  Glubbdubdrib   110418_1            53   \n",
       "1068508                Gondal        Gondal   221863_1            53   \n",
       "...                       ...           ...        ...           ...   \n",
       "908844                 Gondal       Axphain  5755992_4             9   \n",
       "1154024                Gondal  Rook Islands  5842454_6             9   \n",
       "1160065                Gondal        Kasnia  5936647_2             9   \n",
       "244942                Elbonia       Patusan  5955565_2             9   \n",
       "787249   The Devilfire Empire       Patusan  6172320_1             9   \n",
       "\n",
       "         checkin_month  checkin_quarter  checkin_week_sin  checkin_week_cos  \\\n",
       "519017              12                4         -0.118273          0.992981   \n",
       "986596              12                4         -0.118273          0.992981   \n",
       "7504                12                4         -0.118273          0.992981   \n",
       "52884                1                1         -0.118273          0.992981   \n",
       "1068508              1                1         -0.118273          0.992981   \n",
       "...                ...              ...               ...               ...   \n",
       "908844               2                1          0.812487          0.582979   \n",
       "1154024              2                1          0.812487          0.582979   \n",
       "1160065              2                1          0.812487          0.582979   \n",
       "244942               2                1          0.812487          0.582979   \n",
       "787249               2                1          0.812487          0.582979   \n",
       "\n",
       "         row_num  total_rows  last first_hotel_country  first_city_id  \\\n",
       "519017         1           4     0        Cobra Island          18820   \n",
       "986596         1           5     0        Cobra Island          63341   \n",
       "7504           1           4     0        Cobra Island          27404   \n",
       "52884          1          10     0        Glubbdubdrib           3763   \n",
       "1068508        1           8     0              Gondal          45597   \n",
       "...          ...         ...   ...                 ...            ...   \n",
       "908844         5           5     1              Gondal          36063   \n",
       "1154024        9           9     1         Kumbolaland          18032   \n",
       "1160065       12          12     1              Kasnia           2078   \n",
       "244942         5           5     1             Patusan          60966   \n",
       "787249        10          10     1             Patusan          60143   \n",
       "\n",
       "        first_device_class  first_affiliate_id  first_checkin_month  \\\n",
       "519017              mobile                 359                   12   \n",
       "986596              mobile                8151                   12   \n",
       "7504                mobile                 359                   12   \n",
       "52884              desktop                9924                    1   \n",
       "1068508             mobile                7774                    1   \n",
       "...                    ...                 ...                  ...   \n",
       "908844             desktop                4568                    2   \n",
       "1154024             tablet                8822                    2   \n",
       "1160065            desktop                4541                    2   \n",
       "244942             desktop               10332                    2   \n",
       "787249              mobile                9452                    2   \n",
       "\n",
       "         first_booker_country lag1_hotel_country  lag1_city_id  \\\n",
       "519017   The Devilfire Empire                 -1            -1   \n",
       "986596   The Devilfire Empire                 -1            -1   \n",
       "7504     The Devilfire Empire                 -1            -1   \n",
       "52884    The Devilfire Empire                 -1            -1   \n",
       "1068508                Gondal                 -1            -1   \n",
       "...                       ...                ...           ...   \n",
       "908844                 Gondal            Axphain         52848   \n",
       "1154024                Gondal       Rook Islands         56651   \n",
       "1160065                Gondal             Kasnia         38254   \n",
       "244942                Elbonia            Patusan         36073   \n",
       "787249   The Devilfire Empire            Patusan         40325   \n",
       "\n",
       "        lag2_hotel_country  lag2_city_id  city_id_count  \n",
       "519017                  -1            -1            535  \n",
       "986596                  -1            -1            196  \n",
       "7504                    -1            -1           3614  \n",
       "52884                   -1            -1           5544  \n",
       "1068508                 -1            -1            101  \n",
       "...                    ...           ...            ...  \n",
       "908844             Axphain          6090             12  \n",
       "1154024       Rook Islands         56651            692  \n",
       "1160065             Kasnia         33946             12  \n",
       "244942             Patusan         45485            526  \n",
       "787249             Patusan         19183            367  \n",
       "\n",
       "[1166835 rows x 28 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Following aggregation features - would be best to use time window (sort data) to generate, otherwise they will LEAK! (e.g. nunique countries visited)\n",
    "\n",
    "### count features (can also later add rank inside groups).\n",
    "### Some may be leaks (# visits in a trip should use time window?) , and do users repeat? \n",
    "### can add more counts of group X time period (e.g. affiliate X month of year)\n",
    "\n",
    "## alt way to get counts/freq :\n",
    "\n",
    "if GET_COUNT_AGG_FEATS:\n",
    "    count_cols = [ 'city_id','affiliate_id', 'booker_country', 'hotel_country', \n",
    "    #               'utrip_id','user_id', \n",
    "     \"checkin_month\",\"checkin_week\"]\n",
    "    for c in count_cols:\n",
    "        df[f\"{c}_count\"] = df.groupby([c])[\"duration\"].transform(\"size\")\n",
    "\n",
    "    ########################################################\n",
    "    ## nunique per trip\n",
    "    ### https://stackoverflow.com/questions/46470743/how-to-efficiently-compute-a-rolling-unique-count-in-a-pandas-time-series\n",
    "\n",
    "    nunique_cols = [ 'city_id','affiliate_id', 'booker_country', 'hotel_country']\n",
    "    # df[\"nunique_booker_countries\"] = df.groupby(\"utrip_id\")[\"booker_country\"].nunique()\n",
    "    # df[\"nunique_hotel_country\"] = df.groupby(\"utrip_id\")[\"hotel_country\"].nunique()\n",
    "    for c in nunique_cols:\n",
    "        df[f\"{c}_nunique\"] = df.groupby([\"utrip_id\"])[c].transform(\"nunique\")\n",
    "    print(df.nunique())\n",
    "\n",
    "    ########################################################\n",
    "    ## get frequency/count feature's rank within a group - e.g. within a country (or affiliate) \n",
    "    ## add \"_count\" to column name to get count col name, then add rank col \n",
    "\n",
    "    ### ALT/ duplicate feat - add percent rank (instead or in addition)\n",
    "\n",
    "    rank_cols = ['city_id','affiliate_id', 'booker_country','hotel_country',\n",
    "     \"checkin_month\"]\n",
    "    ### what is meaning of groupby and rank of smae variable by same var? Surely should be 1 / unary? \n",
    "    for c in rank_cols:\n",
    "        df[f\"{c}_rank_by_hotel_country\"] = df.groupby(['hotel_country'])[f\"{c}_count\"].transform(\"rank\")\n",
    "        df[f\"{c}_rank_by_booker_country\"] = df.groupby(['booker_country'])[f\"{c}_count\"].transform(\"rank\")\n",
    "        df[f\"{c}_rank_by_affiliate\"] = df.groupby(['affiliate_id'])[f\"{c}_count\"].transform(\"rank\")     \n",
    "else:\n",
    "    freq = df[\"city_id\"].value_counts()\n",
    "    df[\"city_id_count\"] = df[\"city_id\"].map(freq)\n",
    "    print(freq.describe())\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cities with >= 7 occurences: 12345\n",
      "cities with >= MIN_TARGET_FREQ (25) occurences: 4822\n",
      "rows left if filtering by MIN_TARGET_FREQ : 1010884\n"
     ]
    }
   ],
   "source": [
    "print(\"cities with >= 7 occurences:\",df.loc[df[\"city_id_count\"]>=7][\"city_id\"].nunique())\n",
    "print(f\"cities with >= MIN_TARGET_FREQ ({MIN_TARGET_FREQ}) occurences:\",df.loc[df[\"city_id_count\"]>=MIN_TARGET_FREQ][\"city_id\"].nunique())\n",
    "print(f\"rows left if filtering by MIN_TARGET_FREQ :\",df.loc[df[\"city_id_count\"]>=MIN_TARGET_FREQ].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df.isna().sum().max() ==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = df.drop_duplicates(subset=[\"user_id\",\"city_id\"])[\"city_id\"].copy()\n",
    "# print(df2.shape[0])\n",
    "# print(\"df2 nunique (cities without duplicate user visits)\",df2.nunique())\n",
    "\n",
    "# c2_counts = df2.value_counts()\n",
    "\n",
    "# print(\"city counts\")\n",
    "# print(c2_counts)\n",
    "# print(c2_counts.describe())\n",
    "# print(\"cities with at least 2:\",(c2_counts>=2).sum())\n",
    "# print(\"cities with at least 3:\",(c2_counts>=3).sum())\n",
    "# print(\"cities with at least 5:\",(c2_counts>=5).sum())\n",
    "# print(\"cities with at least 10:\",(c2_counts>=10).sum())\n",
    "# print(\"cities with at least 15:\",(c2_counts>=15).sum())\n",
    "# print(\"cities with at least 20:\",(c2_counts>=20).sum())\n",
    "# print(\"cities with at least 30:\",(c2_counts>=30).sum())\n",
    "# print(\"cities with at least 50:\",(c2_counts>=50).sum())\n",
    "# print(\"cities with at least 100:\",(c2_counts>=100).sum())\n",
    "\n",
    "# c2_freq = df2.value_counts(normalize=True)\n",
    "# print(\"top 4 sum coverage (normalized): \",c2_freq[0:4].sum().round(3))\n",
    "# print(\"top 50 sum coverage (normalized): \",c2_freq[0:50].sum().round(3))\n",
    "# print(\"top 100 sum coverage (normalized): \",c2_freq[0:100].sum().round(3))\n",
    "# print(\"top 400 sum coverage (normalized): \",c2_freq[0:400].sum().round(3))\n",
    "# print(\"top 1,000 sum coverage (normalized): \",c2_freq[0:1000].sum().round(3))\n",
    "# print(\"top 5,000 sum coverage (normalized): \",c2_freq[0:5000].sum().round(3))\n",
    "# print(\"top 8,000 sum coverage (normalized): \",c2_freq[0:8000].sum().round(3))\n",
    "# print(\"top 15,000 sum coverage (normalized): \",c2_freq[0:15000].sum().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent city target List + City count encoding\n",
    "* Get the K most frequent target city IDs - selected based on frequency as final destination (not just overall)\n",
    "* +- Also after this, replace rare city IDs categorical features with count encoding to reduce dimensionality\n",
    "    * Keep them as count, or aggregate all of them as \"under_K\"?\n",
    "\n",
    "##### Output  : `TOP_TARGETS` - filter data by this *after* creation of lag features ! \n",
    "\n",
    "* Drop duplicates by the same user (reduce possible bias of frequent users? Only relevant if test is seperater from \"frequent travellers\") \n",
    "    * results in 216,633 , vs 217,686 without dropping duplicates by users\n",
    "    * ~19.9k unique cities\n",
    "    \n",
    "* Could do other encodings - https://contrib.scikit-learn.org/category_encoders/count.html\n",
    "\n",
    "* Note that all this is after we've added rank, count features beforehand, so that information won't be lost for these variables, despite these transforms\n",
    "\n",
    "\n",
    "\n",
    "* **NOTE** he most frequent final destinations are NOT the same as the most popular overall destinations +- first location ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if KEEP_TOP_K_TARGETS > 0 :\n",
    "    df_end = df.loc[df[\"utrip_steps_from_end\"]==1].drop_duplicates(subset=[\"city_id\",\"hotel_country\",\"user_id\"])[[\"city_id\",\"hotel_country\"]].copy()\n",
    "    print(df_end.shape[0])\n",
    "    end_city_counts = df_end.city_id.value_counts()\n",
    "    print(end_city_counts)\n",
    "    \n",
    "    TOP_TARGETS = end_city_counts.head(KEEP_TOP_K_TARGETS).index.values\n",
    "    print(f\"top {KEEP_TOP_K_TARGETS} targets \\n\",TOP_TARGETS)\n",
    "    \n",
    "#     assert df.loc[df[\"city_id\"].isin(TOP_TARGETS)][\"city_id\"].nunique() == KEEP_TOP_K_TARGETS\n",
    "\n",
    "####        \n",
    "# replace low frequency categoircal features    \n",
    "\n",
    "# ##replace with count encoding if have at least k, group rarest as \"-1\":# df[BASE_CAT_COLS] = df[BASE_CAT_COLS].where(df[BASE_CAT_COLS].apply(lambda x: x.map(x.value_counts()))>=LOW_COUNT_THRESH, -1)   \n",
    "# ## replace/group only the rare variables : \n",
    "# df[BASE_CAT_COLS] = df[BASE_CAT_COLS].where(df[BASE_CAT_COLS].apply(lambda x: x.map(x.value_counts()))>=LOW_COUNT_THRESH, -1)\n",
    "# df[BASE_CAT_COLS].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter data by city id frequency\n",
    "#### df2 - smaller df (may not ben ecessary to make\n",
    "\n",
    "* drop rows if it's city id appears less than X times - this is prior to CF\n",
    "* We could also add inclusion/exclusion based on target appearing/frequency as target in final stage of rtip - optional\n",
    "* maybe also drop (end exclude in freq counting) thefirst point in a trip ? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropping users with less than 4 trips\n",
      "dropping cities with less than 25 occurences:\n",
      "1008673\n",
      "dropping users with less than 4 instances, after previous filters:\n",
      "rows left: 857829\n",
      "nunique cities after freq filt city_id            3667\n",
      "utrip_id         162904\n",
      "user_id          152608\n",
      "hotel_country       105\n",
      "dtype: int64\n",
      "nunique city_id per hotel_country:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    105.00000\n",
       "mean      34.92381\n",
       "std       69.87058\n",
       "min        1.00000\n",
       "25%        2.00000\n",
       "50%       10.00000\n",
       "75%       34.00000\n",
       "max      429.00000\n",
       "Name: city_id, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### unsure about this filtering - depends if data points are real or mistake\n",
    "print(\"dropping users with less than 4 trips\")\n",
    "df = df.loc[df[\"total_rows\"]>=4]#.copy()\n",
    "# print(\"abnormal users dropped\",df.shape[0]-df.shape[0])\n",
    "\n",
    "print(f\"dropping cities with less than {MIN_TARGET_FREQ} occurences:\")\n",
    "df = df.loc[df.groupby([\"city_id\"])[\"hotel_country\"].transform(\"count\")>=MIN_TARGET_FREQ] ## update count\n",
    "# df = df.loc[df[\"city_id_count\"]>=MIN_TARGET_FREQ]\n",
    "print(df.shape[0])\n",
    "\n",
    "# print(f\"dropping users with less than 4 instances, after previous city filter:\")\n",
    "df = df.loc[df.groupby([\"utrip_id\"])[TARGET_COL].transform(\"count\")>=4]\n",
    "\n",
    "# print(f\"dropping cities with less than {MIN_TARGET_FREQ} occurences:\")\n",
    "df = df.loc[df.groupby([\"city_id\"])[\"hotel_country\"].transform(\"count\")>=MIN_TARGET_FREQ]\n",
    "\n",
    "print(f\"dropping users with less than 4 instances, after previous filters:\")\n",
    "df = df.loc[df.groupby([\"utrip_id\"])[TARGET_COL].transform(\"count\")>=4]\n",
    "\n",
    "df = df.loc[df.groupby([\"city_id\"])[\"hotel_country\"].transform(\"count\")>=MIN_TARGET_FREQ]\n",
    "df = df.loc[df.groupby([\"utrip_id\"])[TARGET_COL].transform(\"count\")>=4]\n",
    "df = df.loc[df.groupby([\"city_id\"])[\"hotel_country\"].transform(\"count\")>=MIN_TARGET_FREQ]\n",
    "df = df.loc[df.groupby([\"utrip_id\"])[TARGET_COL].transform(\"count\")>=4]\n",
    "# df = df.loc[df.groupby([\"city_id\"])[\"hotel_country\"].transform(\"count\")>=MIN_TARGET_FREQ]\n",
    "# df = df.loc[df.groupby([\"utrip_id\"])[TARGET_COL].transform(\"count\")>=4]\n",
    "\n",
    "print(\"rows left:\",df.shape[0])\n",
    "\n",
    "print(\"nunique cities after freq filt\",df[[\"city_id\",\"utrip_id\",\"user_id\",\"hotel_country\"]].nunique())\n",
    "print(\"nunique city_id per hotel_country:\")\n",
    "df.groupby([\"hotel_country\"])[\"city_id\"].nunique().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* drop the first location visited per trip, as that shares the \"first country/hotel/city id feature\" ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702158\n"
     ]
    }
   ],
   "source": [
    "if DROP_FIRST_ROW: ## drop the first location visited per trip, (row_num = 1) from df data/interactions\n",
    "    df = df.loc[df[\"row_num\"]>1]\n",
    "    print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values([ \"checkin\",\n",
    "                \"utrip_id\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROP_COLS = [ 'user_id', 'checkout',\n",
    "            'row_num',\"last\"] ## dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['utrip_id', 'city_id', 'label', 'checkin', 'device_class',\n",
      "       'affiliate_id', 'booker_country', 'hotel_country', 'checkin_week',\n",
      "       'checkin_month', 'checkin_quarter', 'checkin_week_sin',\n",
      "       'checkin_week_cos', 'total_rows', 'first_hotel_country',\n",
      "       'first_city_id', 'first_device_class', 'first_affiliate_id',\n",
      "       'first_checkin_month', 'first_booker_country', 'lag1_hotel_country',\n",
      "       'lag1_city_id', 'lag2_hotel_country', 'lag2_city_id', 'city_id_count'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df[\"label\"] = 1\n",
    "\n",
    "## rearrange columns - user, item, and label column index are 0, 1, and 2\n",
    "# new_columns = cols_to_order + (frame.columns.drop(cols_to_order).tolist())\n",
    "cols_to_order = [\"utrip_id\",\"city_id\",\"label\",\"checkin\"]\n",
    "new_columns = cols_to_order + (df.columns.drop(cols_to_order+DROP_COLS).tolist())\n",
    "\n",
    "df = df[new_columns]\n",
    "print(df.columns)\n",
    "# df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(702158, 25)\n",
      "702158\n",
      "['user', 'item', 'time', 'device_class', 'affiliate_id', 'booker_country', 'hotel_country', 'checkin_week', 'checkin_month', 'checkin_quarter', 'checkin_week_sin', 'checkin_week_cos', 'total_rows', 'first_hotel_country', 'first_city_id', 'first_device_class', 'first_affiliate_id', 'first_checkin_month', 'first_booker_country', 'lag1_hotel_country', 'lag1_city_id', 'lag2_hotel_country', 'lag2_city_id', 'city_id_count']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('device_class', 'affiliate_id', 'booker_country')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KEEP_COLS = ['utrip_id', 'city_id', 'label',\n",
    "             \"checkin\",\n",
    "             \"hotel_country\", ## should be item feature \n",
    "             \n",
    "#               'user_id', 'checkin', 'checkout','device_class', 'affiliate_id',  'booker_country', \n",
    "#        'checkin_week', 'checkin_month', \n",
    "              'row_num', \n",
    "             'total_rows',\n",
    "#               'last',\n",
    "       'first_hotel_country',\n",
    "             'first_city_id', 'first_device_class','first_affiliate_id',\n",
    "#              'first_checkin_quarter',\n",
    "#              'first_checkin_month',\n",
    "       'first_booker_country'\n",
    "            ]\n",
    "\n",
    "df_feat = df.copy()   \n",
    "# df_feat = df_feat[KEEP_COLS] \n",
    "\n",
    "df_feat.rename(columns={\"checkin\":\"time\",\n",
    "                  'utrip_id':\"user\",\n",
    "                   'city_id':\"item\"\n",
    "                  },inplace=True)\n",
    "print(df_feat.shape)\n",
    "# ### opt: drop duplicates, +- time ? \n",
    "# df_feat.drop_duplicates(subset=['user', 'item', 'hotel_country', 'first_hotel_country',\n",
    "#                            'first_city_id', 'first_booker_country'],inplace=True)\n",
    "\n",
    "print(df_feat.shape[0])\n",
    "print(df_feat.columns.drop(cols_to_order,errors=\"ignore\").tolist())\n",
    "df_feat\n",
    "\n",
    "'device_class', 'affiliate_id', 'booker_country',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object cols:\n",
      "['user', 'device_class', 'booker_country', 'hotel_country', 'first_hotel_country', 'first_device_class', 'first_booker_country', 'lag1_hotel_country', 'lag2_hotel_country']\n",
      "\n",
      "numerical cols:\n",
      "['item', 'label', 'time', 'affiliate_id', 'checkin_week', 'checkin_month', 'checkin_quarter', 'checkin_week_sin', 'checkin_week_cos', 'total_rows', 'first_city_id', 'first_affiliate_id', 'first_checkin_month', 'lag1_city_id', 'lag2_city_id', 'city_id_count']\n"
     ]
    }
   ],
   "source": [
    "print(\"object cols:\")\n",
    "print(list(df_feat.select_dtypes(\"O\").columns))\n",
    "print()\n",
    "print(\"numerical cols:\")\n",
    "print(list(df_feat.select_dtypes(exclude=\"O\").columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_columns = ['device_class', 'booker_country', 'hotel_country',\n",
    "       'first_hotel_country', 'first_device_class', 'first_booker_country',\n",
    "        'affiliate_id','first_city_id', 'first_affiliate_id',\n",
    "              'lag1_city_id','lag2_city_id',\n",
    "       'lag1_hotel_country', 'lag2_hotel_country'\n",
    "             , 'checkin_week']\n",
    "\n",
    "dense_columns = ['checkin_month', 'checkin_quarter', 'checkin_week_sin', 'checkin_week_cos',\n",
    "#              'row_num', \n",
    "                 'total_rows',\n",
    "#              'last',\n",
    "             'first_city_id', 'first_affiliate_id', 'first_checkin_month', 'city_id_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device_class            object\n",
       "booker_country          object\n",
       "hotel_country           object\n",
       "first_hotel_country     object\n",
       "first_device_class      object\n",
       "first_booker_country    object\n",
       "affiliate_id            object\n",
       "first_city_id           object\n",
       "first_affiliate_id      object\n",
       "lag1_city_id            object\n",
       "lag2_city_id            object\n",
       "lag1_hotel_country      object\n",
       "lag2_hotel_country      object\n",
       "checkin_week            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat[sparse_columns] = df_feat[sparse_columns].astype(str)\n",
    "df_feat[sparse_columns].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "checkin_month            int64\n",
       "checkin_quarter          int64\n",
       "checkin_week_sin       float64\n",
       "checkin_week_cos       float64\n",
       "total_rows               int64\n",
       "first_city_id           object\n",
       "first_affiliate_id      object\n",
       "first_checkin_month      int64\n",
       "city_id_count            int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat[dense_columns].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: (total positives/interactions, divided by all possible user/item combs in %, )\n",
      "0.1175417602207954\n"
     ]
    }
   ],
   "source": [
    "print(\"Sparsity: (total positives/interactions, divided by all possible user/item combs in %, )\")\n",
    "# print(100*df.shape[0] / (162904 * 3667))\n",
    "print(100*df_feat.shape[0] / (df_feat['user'].nunique() * df_feat['item'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"KMP_WARNINGS\"] = \"FALSE\"\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ============================== YouTubeRanking ==============================\n"
     ]
    }
   ],
   "source": [
    "reset_state(\"YouTubeRanking\")\n",
    "# reset_state(\"YouTubeMatch\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset made\n",
      "testset made\n",
      "random neg item sampling elapsed: 1.012s\n",
      "train negs done\n",
      "random neg item sampling elapsed: 0.092s\n",
      "test negs done\n",
      "n_users: 162904, n_items: 3667, data sparsity: 0.1005 %\n",
      "Training start time: \u001b[35m2021-01-06 20:58:21\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|███████████████████████████████████████████████████████████████████████| 4691/4691 [03:43<00:00, 21.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 elapsed: 224.468s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|███████████████████████████████████████████████████████████████████████| 4691/4691 [03:54<00:00, 20.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 elapsed: 235.525s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|███████████████████████████████████████████████████████████████████████| 4691/4691 [03:44<00:00, 20.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 elapsed: 225.824s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|███████████████████████████████████████████████████████████████████████| 4691/4691 [03:56<00:00, 19.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 elapsed: 238.138s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|███████████████████████████████████████████████████████████████████████| 4691/4691 [04:02<00:00, 19.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 elapsed: 243.490s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|███████████████████████████████████████████████████████████████████████| 4691/4691 [03:57<00:00, 19.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 elapsed: 238.399s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|███████████████████████████████████████████████████████████████████████| 4691/4691 [03:50<00:00, 20.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 elapsed: 232.185s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|███████████████████████████████████████████████████████████████████████| 4691/4691 [03:49<00:00, 20.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 elapsed: 230.082s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|███████████████████████████████████████████████████████████████████████| 4691/4691 [03:41<00:00, 21.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 elapsed: 222.859s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|███████████████████████████████████████████████████████████████████████| 4691/4691 [03:49<00:00, 20.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 elapsed: 231.046s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|███████████████████████████████████████████████████████████████████████| 4691/4691 [04:01<00:00, 19.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 elapsed: 242.184s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|███████████████████████████████████████████████████████████████████████| 4691/4691 [03:41<00:00, 21.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 elapsed: 222.535s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|███████████████████████████████████████████████████████████████████████| 4691/4691 [03:54<00:00, 19.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 elapsed: 235.946s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|███████████████████████████████████████████████████████████████████████| 4691/4691 [04:02<00:00, 19.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 elapsed: 243.430s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|███████████████████████████████████████████████████████████████████████| 4691/4691 [04:15<00:00, 18.33it/s]\n",
      "eval_pred:   0%|                                                                                | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 elapsed: 257.163s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pred: 100%|███████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  4.16it/s]\n",
      "eval_rec: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [01:56<00:00,  8.55it/s]\n",
      "eval_rec:   0%|                                                                       | 1/1000 [00:00<02:00,  8.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'roc_auc': 0.6810931621735752, 'precision': 0.00575, 'recall': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_rec: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [02:02<00:00,  8.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall @90 {'recall': 0.11683333333333334}\n",
      "Wall time: 1h 19min 1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# train_data, test_data = split_by_ratio_chrono(df, test_size=0.15)\n",
    "train_data, test_data = split_by_ratio(df_feat, test_size=0.2)\n",
    "\n",
    "# specify complete columns information\n",
    "# sparse_col = ['first_hotel_country', \n",
    "# #               'first_city_id','first_device_class','first_affiliate_id', \n",
    "#               'first_booker_country',\n",
    "#               'first_city_id', 'first_device_class','first_affiliate_id',\n",
    "#              \"hotel_country\"]\n",
    "# # dense_col = [] #['total_rows',\n",
    "# #              'first_checkin_quarter', \n",
    "#               #'first_checkin_month',]\n",
    "# user_col = ['first_hotel_country','first_booker_country','first_city_id', 'first_device_class','first_affiliate_id']\n",
    "\n",
    "sparse_col = sparse_columns\n",
    "dense_col = dense_columns\n",
    "item_col = [\"hotel_country\"]\n",
    "user_col = [i for i in sparse_columns+ dense_columns if i not in item_col] ## evrything except for hotel_country\n",
    "\n",
    "# train_data, test_data = split_by_ratio(df[user_col+item_col+[\"label\"]+sparse_col+dense_col], test_size=0.15)\n",
    "\n",
    "# train_data, data_info = DatasetFeat.build_trainset(\n",
    "#     train_data, user_col, item_col, sparse_col, dense_col\n",
    "# )\n",
    "train_data, data_info = DatasetFeat.build_trainset(\n",
    "    train_data, item_col=item_col, sparse_col=sparse_col,user_col=user_col,\n",
    ")\n",
    "print(\"trainset made\")\n",
    "test_data = DatasetFeat.build_testset(test_data)\n",
    "print(\"testset made\")\n",
    "\n",
    "# sample negative items for each record\n",
    "train_data.build_negative_samples(data_info,num_neg=3)\n",
    "print(\"train negs done\")\n",
    "test_data.build_negative_samples(data_info,num_neg=1)\n",
    "print(\"test negs done\")\n",
    "print(data_info)  # n_users: 9058, n_items: 456, data sparsity: 0.7836 %\n",
    "## (min 25 cities): n_users: 162904, n_items: 3667, data sparsity: 0.0977 %\n",
    "\n",
    "# reset_state(\"YouTubeRanking\")\n",
    "# reset_state(\"YouTubeMatch\") \n",
    "### The YouTubeMatch model assumes no item features.  \n",
    "\n",
    "## YouTubeRanking  ## YouTubeMatch # DeepFM\n",
    "# ytb_ranking = YouTubeRanking(task=\"ranking\", data_info=data_info,\n",
    "#                              embed_size=16, n_epochs=3, lr=1e-3,\n",
    "#                              recent_num=10,\n",
    "#                              batch_size=128, use_bn=True,\n",
    "#                              hidden_units=\"128,64\") ## hidden_units=\"128,64,32\"\n",
    "\n",
    "# ytb_ranking = DIN(task=\"ranking\", data_info=data_info,\n",
    "#                              n_epochs=3, lr=2e-3,\n",
    "#                              recent_num=8,\n",
    "# #                   use_tf_attention=True,\n",
    "# #                   num_neg=2,\n",
    "#                              batch_size=129, use_bn=True,\n",
    "#                              hidden_units=\"128,64\") ## hidden_units=\"128,64,32\"\n",
    " \n",
    "# ytb_ranking = AutoInt(task=\"ranking\", data_info=data_info,\n",
    "#                              n_epochs=15, lr=1e-3, batch_size=512,num_neg=3,num_heads=4,)\n",
    "\n",
    "ytb_ranking = DeepFM(task=\"ranking\", data_info=data_info,num_neg=3,)\n",
    "\n",
    "ytb_ranking.fit(train_data, verbose=1, shuffle=True,\n",
    "                eval_data=test_data,\n",
    "                metrics=[ \"loss\", \"roc_auc\", \"precision\", \"recall\"]) # \"loss\", \"roc_auc\", \"precision\", \"recall\"\n",
    "\n",
    "\n",
    "print(ytb_ranking.evaluate(test_data,metrics=[ \"roc_auc\", \"precision\", \"recall\"],k=4))\n",
    "print(\"recall @90\",ytb_ranking.evaluate(test_data,metrics=[ \"recall\"],k=90))\n",
    "\n",
    "# # predict preference of user 1 to item 2333\n",
    "# print(\"prediction: \", ytb_ranking.predict(user=1, item=33))\n",
    "# # recommend 7 items for user 1\n",
    "# print(\"recommendation: \", ytb_ranking.recommend_user(user=1, n_rec=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start time: \u001b[35m2021-01-06 22:16:24\u001b[0m\n",
      "total params: \u001b[33m7,161,925\u001b[0m | embedding params: \u001b[33m7,107,140\u001b[0m | network params: \u001b[33m54,785\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|███████████████████████████████████████████████████████████████████████| 9382/9382 [03:38<00:00, 42.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 elapsed: 220.480s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|███████████████████████████████████████████████████████████████████████| 9382/9382 [03:42<00:00, 42.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 elapsed: 223.824s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|███████████████████████████████████████████████████████████████████████| 9382/9382 [03:44<00:00, 41.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 elapsed: 226.317s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|███████████████████████████████████████████████████████████████████████| 9382/9382 [03:40<00:00, 42.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 elapsed: 222.670s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|███████████████████████████████████████████████████████████████████████| 9382/9382 [03:40<00:00, 42.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 elapsed: 221.711s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|███████████████████████████████████████████████████████████████████████| 9382/9382 [03:35<00:00, 43.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 elapsed: 216.292s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|███████████████████████████████████████████████████████████████████████| 9382/9382 [03:38<00:00, 43.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 elapsed: 219.579s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|███████████████████████████████████████████████████████████████████████| 9382/9382 [03:36<00:00, 43.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 elapsed: 218.272s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|███████████████████████████████████████████████████████████████████████| 9382/9382 [03:36<00:00, 43.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 elapsed: 217.858s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|███████████████████████████████████████████████████████████████████████| 9382/9382 [03:37<00:00, 43.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 elapsed: 218.409s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|███████████████████████████████████████████████████████████████████████| 9382/9382 [03:39<00:00, 42.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 elapsed: 221.279s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|███████████████████████████████████████████████████████████████████████| 9382/9382 [03:38<00:00, 43.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 elapsed: 219.440s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|███████████████████████████████████████████████████████████████████████| 9382/9382 [03:37<00:00, 43.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 elapsed: 219.386s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|███████████████████████████████████████████████████████████████████████| 9382/9382 [03:25<00:00, 45.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 elapsed: 206.842s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|███████████████████████████████████████████████████████████████████████| 9382/9382 [03:30<00:00, 44.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 elapsed: 211.382s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|███████████████████████████████████████████████████████████████████████| 9382/9382 [03:31<00:00, 44.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 elapsed: 212.857s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|███████████████████████████████████████████████████████████████████████| 9382/9382 [03:14<00:00, 48.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 elapsed: 195.858s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|███████████████████████████████████████████████████████████████████████| 9382/9382 [03:14<00:00, 48.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 elapsed: 195.596s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|███████████████████████████████████████████████████████████████████████| 9382/9382 [03:14<00:00, 48.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 elapsed: 195.527s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|███████████████████████████████████████████████████████████████████████| 9382/9382 [03:20<00:00, 46.89it/s]\n",
      "eval_pred:  12%|████████▋                                                               | 3/25 [00:00<00:00, 24.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 elapsed: 201.223s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pred: 100%|███████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 33.38it/s]\n",
      "eval_rec: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [00:10<00:00, 96.56it/s]\n",
      "eval_rec:   1%|▋                                                                     | 10/1000 [00:00<00:10, 97.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'roc_auc': 0.6412648057996093, 'precision': 0.00175, 'recall': 0.0065}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_rec: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [00:10<00:00, 95.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall @90 {'recall': 0.09833333333333334}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ytb_ranking = DeepFM(task=\"ranking\", data_info=data_info,num_neg=3,)\n",
    "\n",
    "ytb_ranking.fit(train_data, verbose=1, shuffle=True,\n",
    "                eval_data=test_data,\n",
    "                metrics=[ \"loss\", \"roc_auc\", \"precision\", \"recall\"]) # \"loss\", \"roc_auc\", \"precision\", \"recall\"\n",
    "\n",
    "\n",
    "print(ytb_ranking.evaluate(test_data,metrics=[ \"roc_auc\", \"precision\", \"recall\"],k=4))\n",
    "print(\"recall @90\",ytb_ranking.evaluate(test_data,metrics=[ \"recall\"],k=90))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### temporal split, min 25:   (results the same on random split ) \n",
    "train_data.build_negative_samples(data_info,num_neg=7)\n",
    "test_data.build_negative_samples(data_info,num_neg=1)\n",
    "\n",
    "train_data, test_data = split_by_ratio_chrono(df, test_size=0.15)\n",
    "ytb_ranking = YouTubeRanking(task=\"ranking\", data_info=data_info,\n",
    "                             embed_size=16, n_epochs=7, lr=2e-4,\n",
    "                             batch_size=64, use_bn=True,\n",
    "                             hidden_units=\"128,64\")\n",
    "============================== YouTubeRanking ==============================\n",
    "trainset made\n",
    "testset made\n",
    "random neg item sampling elapsed: 1.895s\n",
    "train negs done\n",
    "random neg item sampling elapsed: 0.080s\n",
    "test negs done\n",
    "n_users: 162904, n_items: 3667, data sparsity: 0.0977 %\n",
    "Training start time: 2021-01-06 00:43:33\n",
    "train: 100%| \n",
    "Epoch 7 elapsed: 1096.930s\n",
    "Wall time: 2h 15min 44s\n",
    "\n",
    "Eval (temporal split): \n",
    "* {'loss': 0.806, 'roc_auc': 0.982, 'precision': 0.0885, 'recall': 0.3468}\n",
    "* {recall @ 90 : 'recall': 0.771}\n",
    "\n",
    "\n",
    "On a random split with same model (might include train on train )  - results are same as with temporal split ; \n",
    "* {'loss': 0.806, 'roc_auc': 0.982, 'precision': 0.0885, 'recall': 0.3468}\n",
    "* recall @90 {'recall': 0.77}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* autoINT (min 25):\n",
    "\n",
    "{'roc_auc': 0.6810, 'precision': 0.00575, 'recall': 0.02}\n",
    "recall @90 {'recall': 0.1168}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset_state(\"YouTubeRanking\")\n",
    "# data = pd.read_csv(\"./sample_movielens_merged.csv\", sep=\",\", header=0)\n",
    "# data[\"label\"] = 1  # convert to implicit data and do negative sampling afterwards\n",
    "\n",
    "# # split into train and test data based on time\n",
    "# train_data, test_data = split_by_ratio_chrono(data, test_size=0.2)\n",
    "\n",
    "# # specify complete columns information\n",
    "# sparse_col = [\"sex\", \"occupation\", \"genre1\", \"genre2\", \"genre3\"]\n",
    "# dense_col = [\"age\"]\n",
    "# user_col = [\"sex\", \"age\", \"occupation\"]\n",
    "# item_col = [\"genre1\", \"genre2\", \"genre3\"]\n",
    "\n",
    "# train_data, data_info = DatasetFeat.build_trainset(\n",
    "#     train_data, user_col, item_col, sparse_col, dense_col\n",
    "# )\n",
    "# test_data = DatasetFeat.build_testset(test_data)\n",
    "# train_data.build_negative_samples(data_info)  # sample negative items for each record\n",
    "# test_data.build_negative_samples(data_info)\n",
    "# print(data_info)  # n_users: 5962, n_items: 3226, data sparsity: 0.4185 %\n",
    "\n",
    "# ytb_ranking = YouTubeRanking(task=\"ranking\", data_info=data_info, embed_size=16, \n",
    "#                              n_epochs=3, lr=1e-4, batch_size=512, use_bn=True, \n",
    "#                              hidden_units=\"128,64,32\")\n",
    "# ytb_ranking.fit(train_data, verbose=2, shuffle=True, eval_data=test_data,\n",
    "#                 metrics=[\"loss\", \"roc_auc\", \"precision\", \"recall\", \"map\", \"ndcg\"])\n",
    "\n",
    "# # predict preference of user 1 to item 2333\n",
    "# print(\"prediction: \", ytb_ranking.predict(user=1, item=2333))  \n",
    "# # recommend 7 items for user 1\n",
    "# print(\"recommendation(id, probability): \", ytb_ranking.recommend_user(user=1, n_rec=7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
