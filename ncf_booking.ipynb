{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Collaborative Filtering on MovieLens dataset.\n",
    "\n",
    "Neural Collaborative Filtering (NCF) is a well known recommendation algorithm that generalizes the matrix factorization problem with multi-layer perceptron. \n",
    "\n",
    "This notebook provides an example of how to utilize and evaluate NCF implementation in the `reco_utils`. We use a smaller dataset in this example to run NCF efficiently with GPU acceleration on a [Data Science Virtual Machine](https://azure.microsoft.com/en-gb/services/virtual-machines/data-science-virtual-machines/).\n",
    "\n",
    "\n",
    "* https://github.com/microsoft/recommenders/blob/6815e5663ef87da1d0b9029bc9a8a367dc3d33a7/examples/02_model_hybrid/ncf_deep_dive.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.11 (default, Nov 27 2020, 18:37:51) [MSC v.1916 64 bit (AMD64)]\n",
      "Pandas version: 0.25.3\n",
      "Tensorflow version: 1.15.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "sys.path.append(\"./recommenders-master\")\n",
    "import time\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from reco_utils.recommender.ncf.ncf_singlenode import NCF\n",
    "from reco_utils.recommender.ncf.dataset import Dataset as NCFDataset\n",
    "from reco_utils.dataset import movielens\n",
    "from reco_utils.common.notebook_utils import is_jupyter\n",
    "from reco_utils.dataset.python_splitters import python_chrono_split\n",
    "from reco_utils.evaluation.python_evaluation import (rmse, mae, rsquared, exp_var, map_at_k, ndcg_at_k, precision_at_k, \n",
    "                                                     recall_at_k, get_top_k_items)\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Pandas version: {}\".format(pd.__version__))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# top k items to recommend\n",
    "TOP_K = 4\n",
    "\n",
    "# Model parameters\n",
    "EPOCHS = 20 # 20\n",
    "BATCH_SIZE =  64#256\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "MIN_TARGET_FREQ = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166835\n",
      "867651\n",
      "Index(['timestamp', 'itemID', 'userID', 'rating'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>itemID</th>\n",
       "      <th>userID</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1061281</th>\n",
       "      <td>2016-04-09</td>\n",
       "      <td>38677</td>\n",
       "      <td>1000033_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061282</th>\n",
       "      <td>2016-04-11</td>\n",
       "      <td>52089</td>\n",
       "      <td>1000033_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061283</th>\n",
       "      <td>2016-04-12</td>\n",
       "      <td>21328</td>\n",
       "      <td>1000033_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061284</th>\n",
       "      <td>2016-04-14</td>\n",
       "      <td>27485</td>\n",
       "      <td>1000033_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061285</th>\n",
       "      <td>2016-04-16</td>\n",
       "      <td>38677</td>\n",
       "      <td>1000033_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120385</th>\n",
       "      <td>2016-04-21</td>\n",
       "      <td>24718</td>\n",
       "      <td>999855_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120386</th>\n",
       "      <td>2016-04-22</td>\n",
       "      <td>33408</td>\n",
       "      <td>999855_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120390</th>\n",
       "      <td>2016-04-27</td>\n",
       "      <td>63729</td>\n",
       "      <td>999855_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120391</th>\n",
       "      <td>2016-04-29</td>\n",
       "      <td>44489</td>\n",
       "      <td>999855_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120392</th>\n",
       "      <td>2016-05-01</td>\n",
       "      <td>20345</td>\n",
       "      <td>999855_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>867651 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         timestamp  itemID     userID  rating\n",
       "1061281 2016-04-09   38677  1000033_1       1\n",
       "1061282 2016-04-11   52089  1000033_1       1\n",
       "1061283 2016-04-12   21328  1000033_1       1\n",
       "1061284 2016-04-14   27485  1000033_1       1\n",
       "1061285 2016-04-16   38677  1000033_1       1\n",
       "...            ...     ...        ...     ...\n",
       "1120385 2016-04-21   24718   999855_1       1\n",
       "1120386 2016-04-22   33408   999855_1       1\n",
       "1120390 2016-04-27   63729   999855_1       1\n",
       "1120391 2016-04-29   44489   999855_1       1\n",
       "1120392 2016-05-01   20345   999855_1       1\n",
       "\n",
       "[867651 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USE_COLS = [\"utrip_id\",\"city_id\",\"checkin\"]\n",
    "\n",
    "df = pd.read_csv(\"booking_train_set.csv\",\n",
    "#                  nrows=423456,\n",
    "#                  index_col=[0],\n",
    "                 parse_dates=[\"checkin\"],infer_datetime_format=True,\n",
    "                usecols=USE_COLS)\n",
    "\n",
    "df.sort_values([ \"utrip_id\", \"checkin\"],inplace=True)\n",
    "\n",
    "print(df.shape[0])\n",
    "##################\n",
    "## filter min freq\n",
    "freq = df[\"city_id\"].value_counts()\n",
    "df[\"city_id_count\"] = df[\"city_id\"].map(freq)\n",
    "df = df.loc[df[\"city_id_count\"]>= MIN_TARGET_FREQ]\n",
    "df.drop([\"city_id_count\"],axis=1,inplace=True,errors=\"ignore\")\n",
    "### filter rare users\n",
    "freq = df[\"utrip_id\"].value_counts()\n",
    "df[\"utrip_id_count\"] = df[\"utrip_id\"].map(freq)\n",
    "df = df.loc[df[\"utrip_id_count\"]>= 4]\n",
    "df.drop([\"utrip_id_count\"],axis=1,inplace=True,errors=\"ignore\")\n",
    "\n",
    "freq = df[\"city_id\"].value_counts()\n",
    "df[\"city_id_count\"] = df[\"city_id\"].map(freq)\n",
    "df = df.loc[df[\"city_id_count\"]>= MIN_TARGET_FREQ]\n",
    "df.drop([\"city_id_count\"],axis=1,inplace=True,errors=\"ignore\")\n",
    "freq = df[\"utrip_id\"].value_counts()\n",
    "df[\"utrip_id_count\"] = df[\"utrip_id\"].map(freq)\n",
    "df = df.loc[df[\"utrip_id_count\"]>= 4]\n",
    "df.drop([\"utrip_id_count\"],axis=1,inplace=True,errors=\"ignore\")\n",
    "#################\n",
    "print(df.shape[0])\n",
    "\n",
    "# df.columns = [\"userID\", \"itemID\", \"timestamp\"]\n",
    "df.rename(columns={\"utrip_id\":\"userID\",\"city_id\":\"itemID\",\"checkin\":\"timestamp\"},inplace=True)\n",
    "\n",
    "df[\"rating\"] = 1\n",
    "print(df.columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 867651 entries, 1061281 to 1120392\n",
      "Data columns (total 4 columns):\n",
      "timestamp    867651 non-null datetime64[ns]\n",
      "itemID       867651 non-null int32\n",
      "userID       867651 non-null object\n",
      "rating       867651 non-null int8\n",
      "dtypes: datetime64[ns](1), int32(1), int8(1), object(1)\n",
      "memory usage: 24.0+ MB\n"
     ]
    }
   ],
   "source": [
    "## could also type userID as categorical to save memory, but that may cause errors?   and datetime to total seconds? \n",
    "df[\"itemID\"] = pd.to_numeric(df[\"itemID\"].values, errors=\"ignore\",downcast=\"integer\")\n",
    "df[\"rating\"] = pd.to_numeric(df[\"rating\"].values, errors=\"ignore\",downcast=\"integer\")\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itemID      3874\n",
      "userID    164813\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[['itemID', 'userID']].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split the data using the Spark chronological splitter provided in utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = python_chrono_split(df, 0.9, filter_by=\"user\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate an NCF dataset object from the data subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = NCFDataset(train=train, test=test, seed=SEED,n_neg_test=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the NCF model on the training data, and get the top-k recommendations for our testing data\n",
    "\n",
    "NCF accepts implicit feedback and generates prospensity of items to be recommended to users in the scale of 0 to 1. A recommended item list can then be generated based on the scores. Note that this quickstart notebook is using a smaller number of epochs to reduce time for training. As a consequence, the model performance will be slighlty deteriorated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NCF (\n",
    "    n_users=data.n_users, \n",
    "    n_items=data.n_items,\n",
    "    model_type=\"NeuMF\",\n",
    "#     n_factors=6,\n",
    "#     layer_sizes=[16,8], # layer_sizes=[16,8,4],\n",
    "    n_epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    learning_rate=1e-3,\n",
    "    verbose=1,\n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "model.fit(data)\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print(\"Took {} seconds for training.\".format(train_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the movie recommendation use case scenario, seen movies are not recommended to the users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "#### this part is very slow and compute intensive. output file is 2.5 GB in memory\n",
    "\n",
    "users, items, preds = [], [], []\n",
    "item = list(train.itemID.unique())\n",
    "for user in train.userID.unique():\n",
    "    user = [user] * len(item) \n",
    "    users.extend(user)\n",
    "    items.extend(item)\n",
    "    preds.extend(list(model.predict(user, item, is_list=True)))\n",
    "\n",
    "all_predictions = pd.DataFrame(data={\"userID\": users, \"itemID\":items, \"prediction\":preds})\n",
    "\n",
    "# ### following was in original code - I comment out, as we do want repeat predictions - dan\n",
    "# merged = pd.merge(train, all_predictions, on=[\"userID\", \"itemID\"], how=\"outer\")\n",
    "# all_predictions = merged[merged.rating.isnull()].drop('rating', axis=1)\n",
    "\n",
    "test_time = time.time() - start_time\n",
    "print(\"Took {} seconds for prediction.\".format(test_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate how well NCF performs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ranking metrics are used for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## get top 4 accuracy - per user\n",
    "all_preds = all_predictions.drop(\"timestamp\",axis=1).sort_values([\"userID\",\"prediction\"],ascending=False)\n",
    "\n",
    "all_preds = all_preds.groupby(\"userID\").head(4).drop(\"prediction\",axis=1)\n",
    "# .head(250123)\n",
    "\n",
    "test_users_total = test[\"userID\"].nunique()\n",
    "print(\"test_users_total\",test_users_total)\n",
    "\n",
    "print(\"%  match of users with top4 acc\", round(100*test.merge(all_preds,on=[\"itemID\",\"userID\"],how=\"inner\")[\"userID\"].nunique()/test_users_total,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "### very slowwww\n",
    "\n",
    "# eval_map = map_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
    "# eval_ndcg = ndcg_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
    "\n",
    "# eval_precision = precision_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
    "eval_recall = recall_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
    "\n",
    "\n",
    "print(#\"MAP:\\t%f\" % eval_map,\n",
    "      #\"NDCG:\\t%f\" % eval_ndcg,\n",
    "#       \"Precision@K:\\t%f\" % eval_precision,\n",
    "      \"Recall@K:\\t%f\" % eval_recall, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_at_k(test, all_predictions, col_prediction='prediction', k=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_jupyter():\n",
    "    # Record results with papermill for tests\n",
    "    import papermill as pm\n",
    "    import scrapbook as sb\n",
    "    sb.glue(\"map\", eval_map)\n",
    "    sb.glue(\"ndcg\", eval_ndcg)\n",
    "    sb.glue(\"precision\", eval_precision)\n",
    "    sb.glue(\"recall\", eval_recall)\n",
    "    sb.glue(\"train_time\", train_time)\n",
    "    sb.glue(\"test_time\", test_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python reco_gpu",
   "language": "python",
   "name": "reco_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
