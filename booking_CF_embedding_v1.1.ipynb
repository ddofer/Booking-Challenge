{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collab filtering/MF model V1.1\n",
    "\n",
    "* try intermediate-simple collaborative filtering/MF/implicit recc embedding model\n",
    "* create new \"id\" - semi distinct tuple of distinguishing variables\n",
    "    * Could consider adding previous city/country visited to tuple - would reduce data but greatly improve this model. For that to make sense, we would also need to do a shared embedding so that city id lag1 and city id will have the same embedding weights learned. \n",
    "    * Could also add discretized number of locations in a trip `total_rows` (1-3,4-5,6+?) as feature , +- step/stage in a trip - https://pbpython.com/natural-breaks.html\n",
    "* Start with stupid approach: ~9k most popular items. highly unbalanced. Dot product .\n",
    "    * Alt simple baseline - SVD (but that loses on embeddings per variable. Instead we'd use just the id)\n",
    "    \n",
    "    \n",
    "    \n",
    " - Possible negatives sampling code : \n",
    "     * https://curiousily.com/posts/build-a-recommender-system-using-keras-and-tensorflow2-in-python/\n",
    "     * tf.datasets - https://stackoverflow.com/questions/58520594/tf-data-dataset-on-each-epoch-only-train-with-a-sub-sample-of-the-full-datase\n",
    "     \n",
    "     \n",
    "     \n",
    " * ALT - get all combinations (itertools) - https://stackoverflow.com/questions/43800390/how-to-create-all-combinations-column-wise-for-multiple-variables-in-pandas\n",
    " \n",
    " * Get all combinations: `pd.crosstab(df[\"city_id\"],df[\"ID\"]).stack().reset_index()`\n",
    " * Keras + sparse generator - https://stackoverflow.com/questions/41538692/using-sparse-matrices-with-keras-and-tensorflow\n",
    "     * https://www.kaggle.com/luisgarcia/keras-nn-with-parallelized-batch-training\n",
    " \n",
    " ###### V1.1: Add lag feature - hotel_country\n",
    " * +- first hotel country, and lag1 hotel country ?\n",
    " \n",
    " * Would be interesting to combine with \"next country\" prediction task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://www.bookingchallenge.com/\n",
    "\n",
    "* Predict `city_id`\n",
    "        * Metric: P@4\n",
    "\n",
    "##### Dataset\n",
    "The training dataset consists of over a million of anonymized hotel reservations, based on real data, with the following features:\n",
    "*    user_id - User ID\n",
    "*    check-in - Reservation check-in date\n",
    "*    checkout - Reservation check-out date\n",
    "*    affiliate_id - An anonymized ID of affiliate channels where the booker came from (e.g. direct, some third party referrals, paid search engine, etc.)\n",
    "*    device_class - desktop/mobile\n",
    "*    booker_country - Country from which the reservation was made (anonymized)\n",
    "*    hotel_country - Country of the hotel (anonymized)\n",
    "*    city_id - city_id of the hotel’s city (anonymized)\n",
    "*    utrip_id - Unique identification of user’s trip (a group of multi-destinations bookings within the same trip)\n",
    "\n",
    "\n",
    "* Each reservation is a part of a customer’s trip (identified by utrip_id) which includes at least 4 consecutive reservations. The check-out date of a reservation is the check-in date of the following reservation in their trip.\n",
    "\n",
    "* The evaluation dataset is constructed similarly, however the city_id of the final reservation of each trip is concealed and requires a prediction.\n",
    "\n",
    " \n",
    "###### Evaluation criteria\n",
    "The goal of the challenge is to predict (and recommend) the final city (city_id) of each trip (utrip_id). We will evaluate the quality of the predictions based on the top four recommended cities for each trip by using Precision@4 metric (4 representing the four suggestion slots at Booking.com website). When the true city is one of the top 4 suggestions (regardless of the order), it is considered correct.\n",
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "* If we are given  the country in question, then this problem is maybe more of a _learning to rank_ problem. (Rather than massively multiclass). \n",
    "    * CatBoost learning to rank on ms dataset (0/1):  https://colab.research.google.com/github/catboost/tutorials/blob/master/ranking/ranking_tutorial.ipynb\n",
    "        * https://catboost.ai/docs/concepts/loss-functions-ranking.html\n",
    "        * for CB ranking,  all objects in dataset must be grouped by group_id - this would be user/trip id X country, in our case. (Still need to add negatives, within each such subgroup/group/\"query\"). \n",
    "\n",
    "    * lightFM - ranking (implicit interactions)\n",
    "        * https://github.com/qqwjq/lightFM\n",
    "\n",
    "    * lstm/w2v - next item recomendation\n",
    "    * dot product between different factors as features (recc.)\n",
    "    * xgboost ap - https://www.kaggle.com/anokas/xgboost-2\n",
    "* Relevant: Kaggle expedia hotel prediction: https://www.kaggle.com/c/expedia-hotel-recommendations/discussion  \n",
    "\n",
    "* ALSO: `implicit interaction` - reccommendation problem (We have only positive feedback, no ranked/negative explicit feedback)'\n",
    "\n",
    "\n",
    "* __BASELINE__ to beat: 4 most popular by country ; 4 most popular by affiliate_id X booker_country X hotel_country (X month?)\n",
    "    * Ignore/auto answer the 4 most popular for countries with less than 4 unique cities in data\n",
    " \n",
    " \n",
    "* Likely approach : build a model (and targets/negatives) per country.\n",
    "\n",
    "-----------\n",
    "#### Data notes:\n",
    "* Long tail of cities and countries\n",
    "* Some (31%) countries have 4 or less unique cities - for those return fixed answer/prediction ?  -\n",
    "    * CAN'T! In test set, we will not have the country ID :(\n",
    "    \n",
    "    \n",
    "----------------------\n",
    "MF - embedding model\n",
    "\n",
    "* https://blog.tensorflow.org/2020/09/introducing-tensorflow-recommenders.html\n",
    "* Implicit recommendations - needs negs\n",
    "    * example of explicit (simple): https://petamind.com/build-a-simple-recommender-system-with-matrix-factorization/\n",
    "* sample negatives - how ? TFRS requires tf.dataset overhead (And confuses me with what user id should be )\n",
    "    * https://www.kaggle.com/skihikingkevin/some-recommender-system-implementations\n",
    "    \n",
    "    \n",
    "Simple keras example of multiple inputs : \n",
    "* keras topologies\n",
    "* https://stackoverflow.com/questions/61722973/why-keras-embedding-not-learning-for-recommendation-system\n",
    "\n",
    "\n",
    "*Tensorflow ranking (seems in beta) : https://colab.research.google.com/github/tensorflow/ranking/blob/master/tensorflow_ranking/examples/handling_sparse_features.ipynb#scrollTo=HfDMGnZY9eVO\n",
    "\n",
    "\n",
    "Negative pairs training with generator - https://towardsdatascience.com/building-a-recommendation-system-using-neural-network-embeddings-1ef92e5c80c9\n",
    "* See code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* example for implicit recomender (naive) - mainly for negatives data gen ?\n",
    "* https://www.kaggle.com/skihikingkevin/some-recommender-system-implementations\n",
    "\n",
    "\n",
    "* Could use **lightFM** - implicit recommender? \n",
    "    * https://github.com/lyst/lightfm/tree/master/examples/dataset\n",
    "    * https://github.com/lyst/lightfm/blob/master/examples/stackexchange/hybrid_crossvalidated.ipynb\n",
    "    * Note use of sparse matrices. Supports metadata\n",
    "* We could use tuple of features for \"user id\" for purposes of recommenders? \n",
    "  \n",
    "* user-item sparse OHE creation - https://github.com/piyushpathak03/Recommendation-systems/blob/master/Recomendation%20system%20end%20to%20end/4)%20Feature%20Creation.ipynb\n",
    "* lightfm -\n",
    "    * https://github.com/piyushpathak03/Recommendation-systems/tree/master/Recomendation%20system%20end%20to%20end  - building the sparse interactions matrix for implecit recc\n",
    "    * https://making.lyst.com/lightfm/docs/examples/dataset.html#building-the-interactions-matrix\n",
    "* https://making.lyst.com/lightfm/docs/examples/hybrid_crossvalidated.html - example of metadata features for lightfm\n",
    "  \n",
    "* https://github.com/zhangruiskyline/DeepLearning/blob/master/doc/Recommendation.md#ranking  - includes negatives sampling! \n",
    "\n",
    "**Spotlight**\n",
    "*  https://maciejkula.github.io/spotlight/interactions.html\n",
    "    * Also has sequence support easily\n",
    "    * Example of loading custom dataset for implicit recc - https://github.com/maciejkula/spotlight/issues/30\n",
    "\n",
    "**SVD/ALS**\n",
    "    * https://stats.stackexchange.com/questions/354355/what-is-the-relation-between-svd-and-als\n",
    "    * https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.svds.html\n",
    "    \n",
    "    \n",
    "RankFM (implicit package) - don't know if adds anything vs lightfm?\n",
    "* https://github.com/etlundquist/rankfm\n",
    "* Does seem easier to \"productionize\"\n",
    "\n",
    "```\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from scipy import sparse\n",
    "\n",
    "def matrix_decomposition(matrix, k, i):\n",
    "    matrix = sparse.csr_matrix(matrix.T)\n",
    "    model = AlternatingLeastSquares(factors=k, iterations=i)\n",
    "    model.fit(matrix)\n",
    "    user_latent = model.user_factors\n",
    "    item_latent = model.item_factors\n",
    "\n",
    "    return user_latent, item_latent\n",
    "```\n",
    "\n",
    "Neural Collaborative Filtering\n",
    "* https://calvinfeng.gitbook.io/machine-learning-notebook/supervised-learning/recommender/neural_collaborative_filtering\n",
    "     * Using the model from here: https://nanx.me/blog/post/recsys-binary-implicit-feedback-r-keras/ and https://github.com/hexiangnan/neural_collaborative_filtering/blob/master/GMF.py\n",
    "\n",
    "```\n",
    "\\# Create the Training Set\n",
    "APPROX_NEGATIVE_SAMPLE_SIZE = int(len(train)*1.2)\n",
    "n_users = c_user.categories.shape[0]\n",
    "n_tracks = c_track.categories.shape[0]\n",
    "\\# Create Training Set\n",
    "train_users = train['username'].cat.codes.values\n",
    "train_tracks = train['track_id'].cat.codes.values\n",
    "train_labels = np.ones(len(train_users))\n",
    "\\# insert negative samples\n",
    "u = np.random.randint(n_users, size=APPROX_NEGATIVE_SAMPLE_SIZE)\n",
    "i = np.random.randint(n_tracks, size=APPROX_NEGATIVE_SAMPLE_SIZE)\n",
    "non_neg_idx = np.where(train_data[u,i] == 0)\n",
    "train_users = np.concatenate([train_users, u[non_neg_idx[1]]])\n",
    "train_tracks = np.concatenate([train_tracks, i[non_neg_idx[1]]])\n",
    "train_labels = np.concatenate([train_labels, np.zeros(u[non_neg_idx[1]].shape[0])])\n",
    "print((train_users.shape, train_tracks.shape, train_labels.shape))\n",
    "\n",
    "\\# random shuffle the data (because Keras takes last 10% as validation split)\n",
    "X = np.stack([train_users, train_tracks, train_labels], axis=1)\n",
    "np.random.shuffle(X)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "* https://vitobellini.github.io/posts/2018/01/03/how-to-build-a-recommender-system-in-tensorflow.html  - easily turn df into matrix (need to add \"as_sparse) - autoencoder approach: \n",
    "    ```\n",
    "    # Convert DataFrame in user-item matrix\n",
    "    matrix = df.pivot(index='user', columns='item', values='rating')\n",
    "    matrix.fillna(0, inplace=True)\n",
    "    ...\n",
    "    # Users and items ordered as they are in matrix\n",
    "\n",
    "    users = matrix.index.tolist()\n",
    "    items = matrix.columns.tolist()\n",
    "\n",
    "    matrix = matrix.as_matrix()\n",
    "    ```\n",
    "    \n",
    "Triplets/siamese + triplet mining - \n",
    "* https://github.com/maciejkula/triplet_recommendations_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommenders embedding - fit generator\n",
    "# https://towardsdatascience.com/building-a-recommendation-system-using-neural-network-embeddings-1ef92e5c80c9\n",
    "# Also has code for generator to generate positive, negative pairs per batch - good for siamese/triplets/metric! \n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "def generate_batch(pairs, n_positive = 50, negative_ratio = 1.0):\n",
    "    \"\"\"Generate batches of samples for training. \n",
    "       Random select positive samples\n",
    "       from pairs and randomly select negatives.\"\"\"\n",
    "    \n",
    "    # Create empty array to hold batch\n",
    "    batch_size = n_positive * (1 + negative_ratio)\n",
    "    batch = np.zeros((batch_size, 3))\n",
    "    \n",
    "    # Continue to yield samples\n",
    "    while True:\n",
    "        # Randomly choose positive examples\n",
    "        for idx, (book_id, link_id) in enumerate(random.sample(pairs, n_positive)):\n",
    "            batch[idx, :] = (book_id, link_id, 1)\n",
    "        idx += 1\n",
    "        \n",
    "        # Add negative examples until reach batch size\n",
    "        while idx < batch_size:\n",
    "            \n",
    "            # Random selection\n",
    "            random_book = random.randrange(len(books))\n",
    "            random_link = random.randrange(len(links))\n",
    "            \n",
    "            # Check to make sure this is not a positive example\n",
    "            if (random_book, random_link) not in pairs_set:\n",
    "                \n",
    "                # Add to batch and increment index\n",
    "                batch[idx, :] = (random_book, random_link, neg_label)\n",
    "                idx += 1\n",
    "                \n",
    "        # Make sure to shuffle order\n",
    "        np.random.shuffle(batch)\n",
    "        yield {'book': batch[:, 0], 'link': batch[:, 1]}, batch[:, 2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible approahc + negatives - https://github.com/zhangruiskyline/DeepLearning/blob/master/doc/Recommendation.md#ranking \n",
    "\n",
    "\n",
    "* Negative sampling from the sparse user-item cooccurrence matrix\n",
    "    * https://stackoverflow.com/questions/49971318/how-to-generate-negative-samples-in-tensorflow\n",
    "    ```\n",
    "    def subsampler(data, num_pos=10, num_neg=10):\n",
    "    \"\"\" Obtain random batch size made up of positive and negative samples\n",
    "    Returns\n",
    "    -------\n",
    "    positive_row : np.array\n",
    "       Row ids of the positive samples\n",
    "    positive_col : np.array\n",
    "       Column ids of the positive samples\n",
    "    positive_data : np.array\n",
    "       Data values in the positive samples\n",
    "    negative_row : np.array\n",
    "       Row ids of the negative samples\n",
    "    negative_col : np.array\n",
    "       Column ids of the negative samples\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    We are not return negative data, since the negative values\n",
    "    are always zero.\n",
    "    \"\"\"\n",
    "    N, D = data.shape\n",
    "    y_data = data.data\n",
    "    y_row = data.row\n",
    "    y_col = data.col\n",
    "\n",
    "    \\# store all of the positive (i, j) coords\n",
    "    idx = np.vstack((y_row, y_col)).T\n",
    "    idx = set(map(tuple, idx.tolist()))\n",
    "    while True:\n",
    "        \\# get positive sample\n",
    "        positive_idx = np.random.choice(len(y_data), num_pos)\n",
    "        positive_row = y_row[positive_idx].astype(np.int32)\n",
    "        positive_col = y_col[positive_idx].astype(np.int32)\n",
    "        positive_data = y_data[positive_idx].astype(np.float32)\n",
    "\n",
    "        \\# get negative sample\n",
    "        negative_row = np.zeros(num_neg, dtype=np.int32)\n",
    "        negative_col = np.zeros(num_neg, dtype=np.int32)\n",
    "        for k in range(num_neg):\n",
    "            i, j = np.random.randint(N), np.random.randint(D)\n",
    "            while (i, j) in idx:\n",
    "                i, j = np.random.randint(N), np.random.randint(D)\n",
    "                negative_row[k] = i\n",
    "                negative_col[k] = j\n",
    "\n",
    "        yield (positive_row, positive_col, positive_data,\n",
    "               negative_row, negative_col)\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, GroupShuffleSplit\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.metrics import TopKCategoricalAccuracy, Precision, SparseTopKCategoricalAccuracy # @4\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## https://www.tensorflow.org/guide/mixed_precision ## TF mixed precision - pytorch requires other setup\n",
    "# from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "\n",
    "# policy = mixed_precision.Policy('mixed_float16')\n",
    "# mixed_precision.set_policy(policy)\n",
    "# ## will need to correct in places, e.g.: \n",
    "# ## outputs = layers.Activation('softmax', dtype='float32', name='predictions')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features to add:\n",
    "* Lag \n",
    "* Rank (popularity) of city, country (in general, +- given booker country)\n",
    "* Count of hotel; user, trip size ? (may be leaky )\n",
    "* Seasonal features - Holidays? , datetime\n",
    "\n",
    "Aggregate feats:\n",
    "* user changed country? last booking (lag 1) country change? \n",
    "* max/min/avg popularity rank of previous locations visited\n",
    "\n",
    "\n",
    "\n",
    "We should create a dictionary of the rank, count, city/country etc' feats, so we can easily merge them when making more \"negative\" samples/feats for ranking.\n",
    "\n",
    "\n",
    "* Consider using a df2 of df without dates + drop_duplicates, +- without user/trip id (After calcing that) .\n",
    "\n",
    "\n",
    "Leaky or potentially leaky (Dependso n test set): \n",
    "* Target freq features - frequency of target city, given source county +- affiliate +- month of year +- given country (and interactions of target freq). \n",
    "    * Risk of leaks - depends of test data has temporal split or not. \n",
    "    * cartboost can do target encode, but this lets us do it for interactions, e.g. target city freq given the 2 countries and affiliate.\n",
    "    * beware overfitting! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_TARGET_FREQ = 30 # drop target/city_id values that appear less than this many times, as final step's target \n",
    "KEEP_TOP_K_TARGETS = 0 # keep K most frequent city ID targets (redundnat with the above, )\n",
    "\n",
    "## (some) categorical variables that appear less than this many times will be replaced with a placeholder value!\n",
    "## Includes CITY id (but done after target filtering, to avoid creating a \"rare class\" target:\n",
    "LOW_COUNT_THRESH = 10\n",
    "\n",
    "RUN_TABNET = False\n",
    "max_epochs = 3\n",
    "\n",
    "GET_COUNT_AGG_FEATS = False ## disable getting count, rank etc' groupby features , for speedup\n",
    "\n",
    "## for matrix factorization/CF:\n",
    "### morte possible ID_cols :  # last (last step in trip) - would double data per user incorrectly\n",
    "### hotel_country_lag1 , city_id_lag1  (very relevant - needs shared embeddingm and would increase cardinality a lot.. ) \n",
    "ID_COLS = ['device_class','affiliate_id', 'booker_country',\n",
    "           'checkin_quarter',\"last\",\n",
    "          \"first_hotel_country\"] \n",
    "MF_KEEP_COLS = [\"ID\"]+ID_COLS+['city_id',\"hotel_country\"]\n",
    "\n",
    "SAVE_TO_DISK = False\n",
    "\n",
    "TARGET_COL = 'hotel_country' # 'city_id'  , \"hotel_country\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most basic categorical columns , without 'user_id', , 'utrip_id' ordevice_class - used for count encoding/filtering\n",
    "BASE_CAT_COLS = ['city_id',  'affiliate_id', 'booker_country', 'hotel_country']\n",
    "\n",
    "# ### features to get lags for. Not very robust. May want different feats for lags before -1\n",
    "# LAG_FEAT_COLS = ['city_id', 'device_class',\n",
    "#        'affiliate_id', 'booker_country', 'hotel_country', \n",
    "#        'duration', 'same_country', 'checkin_day', 'checkin_weekday',\n",
    "#        'checkin_week',\n",
    "#         'checkout_weekday','checkout_week',\n",
    "#        'city_id_count', 'affiliate_id_count',\n",
    "#        'booker_country_count', 'hotel_country_count', \n",
    "#        'checkin_month_count', 'checkin_week_count', 'city_id_nunique',\n",
    "#        'affiliate_id_nunique', 'booker_country_nunique',\n",
    "#        'hotel_country_nunique', 'city_id_rank_by_hotel_country',\n",
    "#        'city_id_rank_by_booker_country', 'city_id_rank_by_affiliate',\n",
    "#        'affiliate_id_rank_by_hotel_country',\n",
    "#        'affiliate_id_rank_by_booker_country', 'affiliate_id_rank_by_affiliate',\n",
    "#        'booker_country_rank_by_hotel_country',\n",
    "#        'booker_country_rank_by_booker_country',\n",
    "#        'booker_country_rank_by_affiliate',\n",
    "#        'hotel_country_rank_by_hotel_country',\n",
    "#        'hotel_country_rank_by_booker_country',\n",
    "#        'hotel_country_rank_by_affiliate',\n",
    "#        'checkin_month_rank_by_hotel_country',\n",
    "#        'checkin_month_rank_by_booker_country',\n",
    "#        'checkin_month_rank_by_affiliate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/33907537/groupby-and-lag-all-columns-of-a-dataframe\n",
    "# https://stackoverflow.com/questions/62924987/lag-multiple-variables-grouped-by-columns\n",
    "## lag features with groupby over many columns: \n",
    "def groupbyLagFeatures(df:pd.DataFrame,lag:[]=[1,2],group=\"utrip_id\",lag_feature_cols=[]):\n",
    "    \"\"\"\n",
    "    lag features with groupby over many columns.\n",
    "    Assumes sorted data!\n",
    "    https://stackoverflow.com/questions/62924987/lag-multiple-variables-grouped-by-columns\"\"\"\n",
    "    if len(lag_feature_cols)>0:\n",
    "        df=pd.concat([df]+[df.groupby(group)[lag_feature_cols].shift(x).add_prefix('lag'+str(x)+\"_\") for x in lag],axis=1)\n",
    "    else:\n",
    "         df=pd.concat([df]+[df.groupby(group).shift(x).add_prefix('lag'+str(x)+\"_\") for x in lag],axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def groupbyFirstLagFeatures(df:pd.DataFrame,group=\"user_id\",lag_feature_cols=[]):\n",
    "    \"\"\"\n",
    "    Get  first/head value lag-like of features with groupby over columns. Assumes sorted data!\n",
    "    \"\"\"\n",
    "    if len(lag_feature_cols)>0:\n",
    "        df=pd.concat([df]+[df.groupby(group)[lag_feature_cols].transform(\"first\").add_prefix(\"first_\")],axis=1)\n",
    "    else:\n",
    "#          df=pd.concat([df]+[df.groupby(group).first().add_prefix(\"first_\")],axis=1)\n",
    "        df=pd.concat([df]+[df.groupby(group).transform(\"first\").add_prefix(\"first_\")],axis=1)\n",
    "    return df\n",
    "\n",
    "######## Get n most popular items, per group\n",
    "def most_popular(group, n_max=4):\n",
    "    \"\"\"Find most popular hotel clusters by destination\n",
    "    Define a function to get most popular hotels for a destination group.\n",
    "\n",
    "    Previous version used nlargest() Series method to get indices of largest elements. But the method is rather slow.\n",
    "    Source: https://www.kaggle.com/dvasyukova/predict-hotel-type-with-pandas\n",
    "    \"\"\"\n",
    "    relevance = group['relevance'].values\n",
    "    hotel_cluster = group['hotel_cluster'].values\n",
    "    most_popular = hotel_cluster[np.argsort(relevance)[::-1]][:n_max]\n",
    "    return np.array_str(most_popular)[1:-1] # remove square brackets\n",
    "\n",
    "\n",
    "## https://codereview.stackexchange.com/questions/149306/select-the-n-most-frequent-items-from-a-pandas-groupby-dataframe\n",
    "# https://stackoverflow.com/questions/52073054/group-by-a-column-to-find-the-most-frequent-value-in-another-column\n",
    "## can get modes (sorted)\n",
    "# https://stackoverflow.com/questions/50592762/finding-most-common-values-with-pandas-groupby-and-value-counts\n",
    "## df.groupby('tag')['category'].agg(lambda x: x.value_counts().index[0])\n",
    "# https://stackoverflow.com/questions/15222754/groupby-pandas-dataframe-and-select-most-common-value\n",
    "# source2.groupby(['Country','City'])['Short name'].agg(pd.Series.mode)\n",
    "\n",
    "\n",
    "\n",
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=64,target_col=\"target\"):\n",
    "    \"\"\"\n",
    "    Wrap dataframes with tf.data. \n",
    "    This will enable us to use feature columns as a bridge to map from the columns in a dataframe to features used to train the model.\n",
    "    https://www.tensorflow.org/tutorials/structured_data/feature_columns#create_an_input_pipeline_using_tfdata\n",
    "    \"\"\"\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop(target_col)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id           58667\n",
      "checkin             425\n",
      "checkout            425\n",
      "city_id           24922\n",
      "device_class          3\n",
      "affiliate_id       1918\n",
      "booker_country        5\n",
      "hotel_country       172\n",
      "utrip_id          60332\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>checkin</th>\n",
       "      <th>checkout</th>\n",
       "      <th>city_id</th>\n",
       "      <th>device_class</th>\n",
       "      <th>affiliate_id</th>\n",
       "      <th>booker_country</th>\n",
       "      <th>hotel_country</th>\n",
       "      <th>utrip_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7504</th>\n",
       "      <td>2595109</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>27404</td>\n",
       "      <td>mobile</td>\n",
       "      <td>359</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>2595109_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121894</th>\n",
       "      <td>5757202</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>15284</td>\n",
       "      <td>mobile</td>\n",
       "      <td>9452</td>\n",
       "      <td>Tcherkistan</td>\n",
       "      <td>Tcherkistan</td>\n",
       "      <td>5757202_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200488</th>\n",
       "      <td>2451893</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>37689</td>\n",
       "      <td>mobile</td>\n",
       "      <td>384</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>2451893_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210285</th>\n",
       "      <td>6179022</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>11115</td>\n",
       "      <td>mobile</td>\n",
       "      <td>359</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Aslerfan</td>\n",
       "      <td>6179022_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26034</th>\n",
       "      <td>3554942</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>9161</td>\n",
       "      <td>mobile</td>\n",
       "      <td>359</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Bozatta</td>\n",
       "      <td>3554942_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272651</th>\n",
       "      <td>4145326</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>43038</td>\n",
       "      <td>desktop</td>\n",
       "      <td>7974</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Yudonia</td>\n",
       "      <td>4145326_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232502</th>\n",
       "      <td>5274588</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>52356</td>\n",
       "      <td>desktop</td>\n",
       "      <td>384</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>5274588_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135142</th>\n",
       "      <td>1358731</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>31088</td>\n",
       "      <td>desktop</td>\n",
       "      <td>4541</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Fook Island</td>\n",
       "      <td>1358731_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175371</th>\n",
       "      <td>890798</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>43524</td>\n",
       "      <td>desktop</td>\n",
       "      <td>9924</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Rolisica</td>\n",
       "      <td>890798_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302077</th>\n",
       "      <td>5184195</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>27914</td>\n",
       "      <td>desktop</td>\n",
       "      <td>9924</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>Eurasia</td>\n",
       "      <td>5184195_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>323456 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id    checkin   checkout  city_id device_class  affiliate_id  \\\n",
       "7504    2595109 2015-12-31 2016-01-01    27404       mobile           359   \n",
       "121894  5757202 2016-01-01 2016-01-02    15284       mobile          9452   \n",
       "200488  2451893 2016-01-01 2016-01-04    37689       mobile           384   \n",
       "210285  6179022 2016-01-01 2016-01-02    11115       mobile           359   \n",
       "26034   3554942 2016-01-01 2016-01-02     9161       mobile           359   \n",
       "...         ...        ...        ...      ...          ...           ...   \n",
       "272651  4145326 2017-02-27 2017-02-28    43038      desktop          7974   \n",
       "232502  5274588 2017-02-27 2017-02-28    52356      desktop           384   \n",
       "135142  1358731 2017-02-27 2017-02-28    31088      desktop          4541   \n",
       "175371   890798 2017-02-27 2017-02-28    43524      desktop          9924   \n",
       "302077  5184195 2017-02-27 2017-02-28    27914      desktop          9924   \n",
       "\n",
       "              booker_country hotel_country   utrip_id  \n",
       "7504    The Devilfire Empire  Cobra Island  2595109_1  \n",
       "121894           Tcherkistan   Tcherkistan  5757202_1  \n",
       "200488                Gondal        Gondal  2451893_1  \n",
       "210285                Gondal      Aslerfan  6179022_1  \n",
       "26034   The Devilfire Empire       Bozatta  3554942_1  \n",
       "...                      ...           ...        ...  \n",
       "272651  The Devilfire Empire       Yudonia  4145326_1  \n",
       "232502                Gondal  Cobra Island  5274588_3  \n",
       "135142                Gondal   Fook Island  1358731_1  \n",
       "175371                Gondal      Rolisica   890798_1  \n",
       "302077               Elbonia       Eurasia  5184195_1  \n",
       "\n",
       "[323456 rows x 9 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"booking_train_set.csv\",\n",
    "                 nrows=323456,\n",
    "                 index_col=[0],\n",
    "                 parse_dates=[\"checkin\",\"checkout\"],infer_datetime_format=True)\n",
    "\n",
    "df.sort_values([#\"user_id\",\n",
    "                \"checkin\"],inplace=True)\n",
    "print(df.nunique())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### i disabled most of thefeature eztraction here for simplicity\n",
    "\n",
    "# df[\"duration\"] = (df[\"checkout\"] - df[\"checkin\"]).dt.days\n",
    "# df[\"same_country\"] = (df[\"booker_country\"]==df[\"hotel_country\"]).astype(int)\n",
    "\n",
    "# df[\"checkin_day\"] = df[\"checkin\"].dt.day\n",
    "# df[\"checkin_weekday\"] = df[\"checkin\"].dt.weekday\n",
    "df[\"checkin_week\"] = df[\"checkin\"].dt.isocalendar().week.astype(int) ## week of year\n",
    "df[\"checkin_month\"] = df[\"checkin\"].dt.month\n",
    "# df[\"checkin_year\"] = df[\"checkin\"].dt.year-2016\n",
    "\n",
    "df[\"checkin_quarter\"] = df[\"checkin\"].dt.quarter # relatively redundant but may be used for \"id\"\n",
    "\n",
    "# df[\"checkin_quarter\"] = df[\"checkin_quarter\"]/4 # scale. could also do cos, sin extraction. makesi t a float instead of int/embedding\n",
    "\n",
    "\n",
    "# df[\"checkout_weekday\"] = df[\"checkout\"].dt.weekday\n",
    "# df[\"checkout_week\"] = df[\"checkout\"].dt.isocalendar().week.astype(int) ## week of year\n",
    "# df[\"checkout_day\"] = df[\"checkout\"].dt.day ## day of month\n",
    "\n",
    "## cyclical datetime embeddings\n",
    "## drop originakl variables? \n",
    "## TODO:L add for other variables, +- those that we'll embed (week?)\n",
    "\n",
    "# df['checkin_weekday_sin'] = np.sin(df[\"checkin_weekday\"]*(2.*np.pi/7))\n",
    "# df['checkin_weekday_cos'] = np.cos(df[\"checkin_weekday\"]*(2.*np.pi/7))\n",
    "df['checkin_month_sin'] = np.sin((df[\"checkin_month\"]-1)*(2.*np.pi/12))\n",
    "df['checkin_month_cos'] = np.cos((df[\"checkin_month\"]-1)*(2.*np.pi/12))\n",
    "\n",
    "# #############\n",
    "# # last number in utrip id - probably which trip number it is:\n",
    "# df[\"utrip_number\"] = df[\"utrip_id\"].str.split(\"_\",expand=True)[1].astype(int)\n",
    "\n",
    "### encode string columns - must be consistent with test data \n",
    "### IF we can concat test with train, we can just do a single transformation  for the NON TARGET cols\n",
    "# obj_cols_list = df.select_dtypes(\"O\").columns.values\n",
    "obj_cols_list = ['device_class','booker_country','hotel_country',\n",
    "#                 \"city_id\"\n",
    "                ] # we could also define when loading data, dtype\n",
    "\n",
    "for c in obj_cols_list:\n",
    "    df[c] = df[c].astype(\"category\")\n",
    "    df[c] = df[c].cat.codes.astype(int)\n",
    "#     print(\"min\",df[c].min()) min is 0 - which is what the embedding (indices) will expect\n",
    "\n",
    "## view steps of a trip per user & trip, in order. ## last step == 1.\n",
    "## count #/pct step in a trip (utrip_id) per user. Useful to get the \"final\" step per trip - for prediction\n",
    "## note that the order is ascending, so we would need to select by \"last\" . (i.e \"1\" is the first step, 2 the second, etc') , or we could use pct .rank(ascending=True,pct=True)\n",
    "#### this feature overlaps with the count of each trip id (for the final row)\n",
    "##  = df.sort_values([\"checkin\",\"checkout\"])... - df already sorted above\n",
    "df[\"utrip_steps_from_end\"] = df.groupby(\"utrip_id\")[\"checkin\"].rank(ascending=True,pct=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             row_num     total_rows\n",
      "count  323456.000000  323456.000000\n",
      "mean        3.557621       6.115243\n",
      "std         2.372447       2.790419\n",
      "min         1.000000       1.000000\n",
      "25%         2.000000       4.000000\n",
      "50%         3.000000       5.000000\n",
      "75%         5.000000       7.000000\n",
      "max        48.000000      48.000000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### add features to be consistent with test set of row in trip, and total trips in trip\n",
    "\n",
    "df[\"row_num\"] = df.groupby(\"utrip_id\")[\"checkin\"].rank(ascending=True,pct=False).astype(int)\n",
    "utrip_counts = df[\"utrip_id\"].value_counts()\n",
    "df[\"total_rows\"] = df[\"utrip_id\"].map(utrip_counts)\n",
    "\n",
    "### last step in trip\n",
    "df[\"last\"] = (df[\"total_rows\"]==df[\"row_num\"]).astype(int)\n",
    "print(df[[\"row_num\",\"total_rows\"]].describe())\n",
    "\n",
    "df[\"total_rows\"].hist(); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### replace rare categorical variable(s) - affiliates\n",
    "* replace rare variables (under 2 occurrences) with \"-1\" dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: 9924    76761\n",
      "359     47588\n",
      "384     24703\n",
      "9452    23994\n",
      "4541    11587\n",
      "        ...  \n",
      "3390        1\n",
      "5031        1\n",
      "4260        1\n",
      "168         1\n",
      "4094        1\n",
      "Name: affiliate_id, Length: 1918, dtype: int64\n",
      "uniques 1918\n",
      "after\n",
      " 9924     76761\n",
      "359      47588\n",
      "384      24703\n",
      "9452     23994\n",
      "4541     11587\n",
      "         ...  \n",
      "3903         3\n",
      "10554        3\n",
      "1989         3\n",
      "1896         3\n",
      "6845         3\n",
      "Name: affiliate_id, Length: 1217, dtype: int64\n",
      "uniques 1217\n"
     ]
    }
   ],
   "source": [
    "### replace rare variables (under 2 occurrences) with \"-1\" dummy\n",
    "\n",
    "affiliates_counts = df[\"affiliate_id\"].value_counts()\n",
    "print(\"before:\", affiliates_counts)\n",
    "print(\"uniques\",df[\"affiliate_id\"].nunique())\n",
    "affiliates_counts = affiliates_counts.to_dict()\n",
    "# df[\"affiliate_id\"] = df[\"affiliate_id\"].where(df[\"affiliate_id\"].apply(lambda x: x.map(x.value_counts()))>=3, -1)\n",
    "df[\"affiliate_id\"] = df[\"affiliate_id\"].where(df[\"affiliate_id\"].map(affiliates_counts)>=3, -2)\n",
    "df[\"affiliate_id\"] = df[\"affiliate_id\"].astype(int)\n",
    "\n",
    "print(\"after\\n\",df[\"affiliate_id\"].value_counts())\n",
    "print(\"uniques\",df[\"affiliate_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"total_rows\"].map(pd.cut(df[\"total_rows\"],bins=3,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add LAG feature(s)  + DROP First visited location rows\n",
    "* \"first\" hotel country (vs most recent country visited)\n",
    "    * `groupbyFirstLagFeatures(df,group=\"user_id\",lag_feature_cols=[\"hotel_country\",\"city_id\"])`\n",
    "* Can consider: lag1 hotel_country, hotel_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['device_class', 'affiliate_id', 'booker_country', 'checkin_quarter', 'last', 'first_hotel_country']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ID_COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>checkin</th>\n",
       "      <th>checkout</th>\n",
       "      <th>city_id</th>\n",
       "      <th>device_class</th>\n",
       "      <th>affiliate_id</th>\n",
       "      <th>booker_country</th>\n",
       "      <th>hotel_country</th>\n",
       "      <th>utrip_id</th>\n",
       "      <th>checkin_week</th>\n",
       "      <th>checkin_month</th>\n",
       "      <th>checkin_quarter</th>\n",
       "      <th>checkin_month_sin</th>\n",
       "      <th>checkin_month_cos</th>\n",
       "      <th>utrip_steps_from_end</th>\n",
       "      <th>row_num</th>\n",
       "      <th>total_rows</th>\n",
       "      <th>last</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7504</th>\n",
       "      <td>2595109</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>27404</td>\n",
       "      <td>1</td>\n",
       "      <td>359</td>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>2595109_1</td>\n",
       "      <td>53</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121894</th>\n",
       "      <td>5757202</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>15284</td>\n",
       "      <td>1</td>\n",
       "      <td>9452</td>\n",
       "      <td>3</td>\n",
       "      <td>153</td>\n",
       "      <td>5757202_1</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200488</th>\n",
       "      <td>2451893</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>37689</td>\n",
       "      <td>1</td>\n",
       "      <td>384</td>\n",
       "      <td>2</td>\n",
       "      <td>59</td>\n",
       "      <td>2451893_1</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210285</th>\n",
       "      <td>6179022</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>11115</td>\n",
       "      <td>1</td>\n",
       "      <td>359</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>6179022_1</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26034</th>\n",
       "      <td>3554942</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>9161</td>\n",
       "      <td>1</td>\n",
       "      <td>359</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>3554942_1</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272651</th>\n",
       "      <td>4145326</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>43038</td>\n",
       "      <td>0</td>\n",
       "      <td>7974</td>\n",
       "      <td>4</td>\n",
       "      <td>169</td>\n",
       "      <td>4145326_1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232502</th>\n",
       "      <td>5274588</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>52356</td>\n",
       "      <td>0</td>\n",
       "      <td>384</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>5274588_3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135142</th>\n",
       "      <td>1358731</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>31088</td>\n",
       "      <td>0</td>\n",
       "      <td>4541</td>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>1358731_1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175371</th>\n",
       "      <td>890798</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>43524</td>\n",
       "      <td>0</td>\n",
       "      <td>9924</td>\n",
       "      <td>2</td>\n",
       "      <td>126</td>\n",
       "      <td>890798_1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302077</th>\n",
       "      <td>5184195</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>27914</td>\n",
       "      <td>0</td>\n",
       "      <td>9924</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5184195_1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>323456 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id    checkin   checkout  city_id  device_class  affiliate_id  \\\n",
       "7504    2595109 2015-12-31 2016-01-01    27404             1           359   \n",
       "121894  5757202 2016-01-01 2016-01-02    15284             1          9452   \n",
       "200488  2451893 2016-01-01 2016-01-04    37689             1           384   \n",
       "210285  6179022 2016-01-01 2016-01-02    11115             1           359   \n",
       "26034   3554942 2016-01-01 2016-01-02     9161             1           359   \n",
       "...         ...        ...        ...      ...           ...           ...   \n",
       "272651  4145326 2017-02-27 2017-02-28    43038             0          7974   \n",
       "232502  5274588 2017-02-27 2017-02-28    52356             0           384   \n",
       "135142  1358731 2017-02-27 2017-02-28    31088             0          4541   \n",
       "175371   890798 2017-02-27 2017-02-28    43524             0          9924   \n",
       "302077  5184195 2017-02-27 2017-02-28    27914             0          9924   \n",
       "\n",
       "        booker_country  hotel_country   utrip_id  checkin_week  checkin_month  \\\n",
       "7504                 4             35  2595109_1            53             12   \n",
       "121894               3            153  5757202_1            53              1   \n",
       "200488               2             59  2451893_1            53              1   \n",
       "210285               2              7  6179022_1            53              1   \n",
       "26034                4             23  3554942_1            53              1   \n",
       "...                ...            ...        ...           ...            ...   \n",
       "272651               4            169  4145326_1             9              2   \n",
       "232502               2             35  5274588_3             9              2   \n",
       "135142               2             51  1358731_1             9              2   \n",
       "175371               2            126   890798_1             9              2   \n",
       "302077               1             47  5184195_1             9              2   \n",
       "\n",
       "        checkin_quarter  checkin_month_sin  checkin_month_cos  \\\n",
       "7504                  4               -0.5           0.866025   \n",
       "121894                1                0.0           1.000000   \n",
       "200488                1                0.0           1.000000   \n",
       "210285                1                0.0           1.000000   \n",
       "26034                 1                0.0           1.000000   \n",
       "...                 ...                ...                ...   \n",
       "272651                1                0.5           0.866025   \n",
       "232502                1                0.5           0.866025   \n",
       "135142                1                0.5           0.866025   \n",
       "175371                1                0.5           0.866025   \n",
       "302077                1                0.5           0.866025   \n",
       "\n",
       "        utrip_steps_from_end  row_num  total_rows  last  \n",
       "7504                0.250000        1           4     0  \n",
       "121894              0.250000        1           4     0  \n",
       "200488              0.142857        1           7     0  \n",
       "210285              0.166667        1           6     0  \n",
       "26034               0.250000        1           4     0  \n",
       "...                      ...      ...         ...   ...  \n",
       "272651              1.000000        4           4     1  \n",
       "232502              1.000000        4           4     1  \n",
       "135142              1.000000        4           4     1  \n",
       "175371              1.000000        4           4     1  \n",
       "302077              1.000000        4           4     1  \n",
       "\n",
       "[323456 rows x 18 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_hotel_country      120\n",
      "hotel_country            169\n",
      "city_id                22976\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>checkin</th>\n",
       "      <th>checkout</th>\n",
       "      <th>city_id</th>\n",
       "      <th>device_class</th>\n",
       "      <th>affiliate_id</th>\n",
       "      <th>booker_country</th>\n",
       "      <th>hotel_country</th>\n",
       "      <th>utrip_id</th>\n",
       "      <th>checkin_week</th>\n",
       "      <th>checkin_month</th>\n",
       "      <th>checkin_quarter</th>\n",
       "      <th>checkin_month_sin</th>\n",
       "      <th>checkin_month_cos</th>\n",
       "      <th>utrip_steps_from_end</th>\n",
       "      <th>row_num</th>\n",
       "      <th>total_rows</th>\n",
       "      <th>last</th>\n",
       "      <th>first_hotel_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52885</th>\n",
       "      <td>110418</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>2016-01-03</td>\n",
       "      <td>3763</td>\n",
       "      <td>0</td>\n",
       "      <td>9924</td>\n",
       "      <td>4</td>\n",
       "      <td>58</td>\n",
       "      <td>110418_1</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26035</th>\n",
       "      <td>3554942</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>2016-01-03</td>\n",
       "      <td>42359</td>\n",
       "      <td>1</td>\n",
       "      <td>359</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>3554942_1</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7505</th>\n",
       "      <td>2595109</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>27404</td>\n",
       "      <td>1</td>\n",
       "      <td>359</td>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>2595109_1</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187788</th>\n",
       "      <td>1589600</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>2016-01-03</td>\n",
       "      <td>39850</td>\n",
       "      <td>1</td>\n",
       "      <td>10332</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1589600_1</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138821</th>\n",
       "      <td>4997413</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>2016-01-03</td>\n",
       "      <td>20497</td>\n",
       "      <td>1</td>\n",
       "      <td>359</td>\n",
       "      <td>4</td>\n",
       "      <td>75</td>\n",
       "      <td>4997413_1</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272651</th>\n",
       "      <td>4145326</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>43038</td>\n",
       "      <td>0</td>\n",
       "      <td>7974</td>\n",
       "      <td>4</td>\n",
       "      <td>169</td>\n",
       "      <td>4145326_1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232502</th>\n",
       "      <td>5274588</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>52356</td>\n",
       "      <td>0</td>\n",
       "      <td>384</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>5274588_3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135142</th>\n",
       "      <td>1358731</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>31088</td>\n",
       "      <td>0</td>\n",
       "      <td>4541</td>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>1358731_1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175371</th>\n",
       "      <td>890798</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>43524</td>\n",
       "      <td>0</td>\n",
       "      <td>9924</td>\n",
       "      <td>2</td>\n",
       "      <td>126</td>\n",
       "      <td>890798_1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302077</th>\n",
       "      <td>5184195</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>27914</td>\n",
       "      <td>0</td>\n",
       "      <td>9924</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5184195_1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>263124 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id    checkin   checkout  city_id  device_class  affiliate_id  \\\n",
       "52885    110418 2016-01-02 2016-01-03     3763             0          9924   \n",
       "26035   3554942 2016-01-02 2016-01-03    42359             1           359   \n",
       "7505    2595109 2016-01-02 2016-01-04    27404             1           359   \n",
       "187788  1589600 2016-01-02 2016-01-03    39850             1         10332   \n",
       "138821  4997413 2016-01-02 2016-01-03    20497             1           359   \n",
       "...         ...        ...        ...      ...           ...           ...   \n",
       "272651  4145326 2017-02-27 2017-02-28    43038             0          7974   \n",
       "232502  5274588 2017-02-27 2017-02-28    52356             0           384   \n",
       "135142  1358731 2017-02-27 2017-02-28    31088             0          4541   \n",
       "175371   890798 2017-02-27 2017-02-28    43524             0          9924   \n",
       "302077  5184195 2017-02-27 2017-02-28    27914             0          9924   \n",
       "\n",
       "        booker_country  hotel_country   utrip_id  checkin_week  checkin_month  \\\n",
       "52885                4             58   110418_1            53              1   \n",
       "26035                4             23  3554942_1            53              1   \n",
       "7505                 4             35  2595109_1            53              1   \n",
       "187788               1             35  1589600_1            53              1   \n",
       "138821               4             75  4997413_1            53              1   \n",
       "...                ...            ...        ...           ...            ...   \n",
       "272651               4            169  4145326_1             9              2   \n",
       "232502               2             35  5274588_3             9              2   \n",
       "135142               2             51  1358731_1             9              2   \n",
       "175371               2            126   890798_1             9              2   \n",
       "302077               1             47  5184195_1             9              2   \n",
       "\n",
       "        checkin_quarter  checkin_month_sin  checkin_month_cos  \\\n",
       "52885                 1                0.0           1.000000   \n",
       "26035                 1                0.0           1.000000   \n",
       "7505                  1                0.0           1.000000   \n",
       "187788                1                0.0           1.000000   \n",
       "138821                1                0.0           1.000000   \n",
       "...                 ...                ...                ...   \n",
       "272651                1                0.5           0.866025   \n",
       "232502                1                0.5           0.866025   \n",
       "135142                1                0.5           0.866025   \n",
       "175371                1                0.5           0.866025   \n",
       "302077                1                0.5           0.866025   \n",
       "\n",
       "        utrip_steps_from_end  row_num  total_rows  last  first_hotel_country  \n",
       "52885                    0.2        2          10     0                   58  \n",
       "26035                    0.5        2           4     0                   23  \n",
       "7505                     0.5        2           4     0                   23  \n",
       "187788                   0.5        2           4     0                   35  \n",
       "138821                   0.5        2           4     0                   23  \n",
       "...                      ...      ...         ...   ...                  ...  \n",
       "272651                   1.0        4           4     1                  107  \n",
       "232502                   1.0        4           4     1                   69  \n",
       "135142                   1.0        4           4     1                  126  \n",
       "175371                   1.0        4           4     1                   75  \n",
       "302077                   1.0        4           4     1                   31  \n",
       "\n",
       "[263124 rows x 19 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## add the \"first\" place visited/values\n",
    "### nopte - will need to drop first row in trip, or impute nans when using this feature \n",
    "\n",
    "### first by user results in too much sparsity/rareness for our IDs purposes\n",
    "# df = groupbyFirstLagFeatures(df,group=\"user_id\",lag_feature_cols=[\"hotel_country\"]) # [\"hotel_country\",\"city_id\"]\n",
    "\n",
    "## alt - messy, but maybe good enough : \n",
    "df = groupbyFirstLagFeatures(df,group=['device_class', 'affiliate_id',\n",
    "                                       'booker_country','checkin_month',\"last\"],lag_feature_cols=[\"hotel_country\"])\n",
    "\n",
    "df = df.loc[df[\"row_num\"]>1]\n",
    "print(df[[\"first_hotel_country\",\"hotel_country\",\"city_id\"]].nunique())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id                 58638\n",
       "checkin                   423\n",
       "checkout                  423\n",
       "city_id                 22976\n",
       "device_class                3\n",
       "affiliate_id             1217\n",
       "booker_country              5\n",
       "hotel_country             169\n",
       "utrip_id                60302\n",
       "checkin_week               53\n",
       "checkin_month              12\n",
       "checkin_quarter             4\n",
       "checkin_month_sin          12\n",
       "checkin_month_cos          12\n",
       "utrip_steps_from_end      298\n",
       "row_num                    47\n",
       "total_rows                 30\n",
       "last                        2\n",
       "first_hotel_country       120\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create pseudo ID for use with CF - composed of semi-distinct tuple of variables\n",
    "* should have moderate uniqueness. Main purpose is to get good embeddings for those variables \n",
    "\n",
    "* `'device_class','affiliate_id', 'booker_country','checkin_quarter'` - 14K \"uniques\"\n",
    "* `'device_class','affiliate_id', 'booker_country'` - 7.5 K \"uniques\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_class              3\n",
      "affiliate_id           1217\n",
      "booker_country            5\n",
      "checkin_quarter           4\n",
      "last                      2\n",
      "first_hotel_country     120\n",
      "checkin_month            12\n",
      "total_rows               30\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device_class  affiliate_id  booker_country  checkin_quarter  last  first_hotel_country\n",
       "0             -2            0               2                0     35                     2\n",
       "                                            3                0     35                     4\n",
       "                                                                   40                     4\n",
       "                                                                   51                     3\n",
       "                                                             1     17                     2\n",
       "                                                                                         ..\n",
       "2              10615        1               3                0     149                    2\n",
       "                                                                   168                    6\n",
       "                                                             1     137                    1\n",
       "               10646        2               4                1     59                     1\n",
       "               10668        2               1                0     132                    1\n",
       "Length: 14568, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### for possible \"user id\" embedding/ID : How many unique values are there for these source tuple? :\n",
    "### Could also maybe add previous location/lag1 country/city ? \n",
    "## 'device_class','affiliate_id', 'booker_country' - 7.5 K \"uniques\"\n",
    "## 'device_class','affiliate_id', 'booker_country','checkin_month' - 24 K \"uniques\"\n",
    "## 'device_class','affiliate_id', 'booker_country','checkin_quarter' 14K \"uniques\"\n",
    "## ,\"total_rows\" \n",
    "\n",
    "print(df[ID_COLS + ['checkin_month',\"total_rows\"]].nunique(axis=0))\n",
    "df.groupby(ID_COLS).size()\n",
    "\n",
    "# id2 = [item for item in ID_COLS if item != \"last\"]\n",
    "# df.groupby(id2).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    14568.000000\n",
      "mean        18.061779\n",
      "std        108.762212\n",
      "min          1.000000\n",
      "25%          1.000000\n",
      "50%          2.000000\n",
      "75%          5.000000\n",
      "max       5992.000000\n",
      "Name: ID, dtype: float64\n",
      "135142    04541211126\n",
      "175371     0992421175\n",
      "302077     0992411131\n",
      "Name: ID, dtype: object\n",
      "14568\n"
     ]
    }
   ],
   "source": [
    "# df[\"ID\"] = df['device_class'].astype(str)+\"_\"+df['affiliate_id'].astype(str)\\\n",
    "# +\"_\"+df['booker_country'].astype(str)+\"_\"+df['checkin_quarter'].astype(str)\n",
    "\n",
    "df[\"ID\"] = df[ID_COLS].astype(str).sum(1)#.astype(\"category\")\n",
    "\n",
    "# df[\"ID\"] = df[\"ID\"].astype(\"category\")\n",
    "print(df[\"ID\"].value_counts().describe())\n",
    "print(df[\"ID\"].tail(3))\n",
    "print(df[\"ID\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keep seperate DF of IDs and component features - will make merging id feats simpler later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>device_class</th>\n",
       "      <th>affiliate_id</th>\n",
       "      <th>booker_country</th>\n",
       "      <th>checkin_quarter</th>\n",
       "      <th>last</th>\n",
       "      <th>first_hotel_country</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0992441058</th>\n",
       "      <td>0</td>\n",
       "      <td>9924</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135941023</th>\n",
       "      <td>1</td>\n",
       "      <td>359</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11033211035</th>\n",
       "      <td>1</td>\n",
       "      <td>10332</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13592107</th>\n",
       "      <td>1</td>\n",
       "      <td>359</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19452310153</th>\n",
       "      <td>1</td>\n",
       "      <td>9452</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0881211151</th>\n",
       "      <td>0</td>\n",
       "      <td>8812</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0249141164</th>\n",
       "      <td>0</td>\n",
       "      <td>2491</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>03724211126</th>\n",
       "      <td>0</td>\n",
       "      <td>3724</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0551311167</th>\n",
       "      <td>0</td>\n",
       "      <td>5513</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28065111115</th>\n",
       "      <td>2</td>\n",
       "      <td>8065</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14568 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             device_class  affiliate_id  booker_country  checkin_quarter  \\\n",
       "ID                                                                         \n",
       "0992441058              0          9924               4                1   \n",
       "135941023               1           359               4                1   \n",
       "11033211035             1         10332               1                1   \n",
       "13592107                1           359               2                1   \n",
       "19452310153             1          9452               3                1   \n",
       "...                   ...           ...             ...              ...   \n",
       "0881211151              0          8812               1                1   \n",
       "0249141164              0          2491               4                1   \n",
       "03724211126             0          3724               2                1   \n",
       "0551311167              0          5513               1                1   \n",
       "28065111115             2          8065               1                1   \n",
       "\n",
       "             last  first_hotel_country  \n",
       "ID                                      \n",
       "0992441058      0                   58  \n",
       "135941023       0                   23  \n",
       "11033211035     0                   35  \n",
       "13592107        0                    7  \n",
       "19452310153     0                  153  \n",
       "...           ...                  ...  \n",
       "0881211151      1                   51  \n",
       "0249141164      1                   64  \n",
       "03724211126     1                  126  \n",
       "0551311167      1                   67  \n",
       "28065111115     1                  115  \n",
       "\n",
       "[14568 rows x 6 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ID = df[[\"ID\"]+ID_COLS].drop_duplicates().set_index(\"ID\")\n",
    "df_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    263124.000000\n",
      "mean        343.295085\n",
      "std         517.379581\n",
      "min           1.000000\n",
      "25%          20.000000\n",
      "50%         114.000000\n",
      "75%         417.000000\n",
      "max        2522.000000\n",
      "Name: city_id_count, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>checkin</th>\n",
       "      <th>checkout</th>\n",
       "      <th>city_id</th>\n",
       "      <th>device_class</th>\n",
       "      <th>affiliate_id</th>\n",
       "      <th>booker_country</th>\n",
       "      <th>hotel_country</th>\n",
       "      <th>utrip_id</th>\n",
       "      <th>checkin_week</th>\n",
       "      <th>checkin_month</th>\n",
       "      <th>checkin_quarter</th>\n",
       "      <th>checkin_month_sin</th>\n",
       "      <th>checkin_month_cos</th>\n",
       "      <th>utrip_steps_from_end</th>\n",
       "      <th>row_num</th>\n",
       "      <th>total_rows</th>\n",
       "      <th>last</th>\n",
       "      <th>first_hotel_country</th>\n",
       "      <th>ID</th>\n",
       "      <th>city_id_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52885</th>\n",
       "      <td>110418</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>2016-01-03</td>\n",
       "      <td>3763</td>\n",
       "      <td>0</td>\n",
       "      <td>9924</td>\n",
       "      <td>4</td>\n",
       "      <td>58</td>\n",
       "      <td>110418_1</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>0992441058</td>\n",
       "      <td>1138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26035</th>\n",
       "      <td>3554942</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>2016-01-03</td>\n",
       "      <td>42359</td>\n",
       "      <td>1</td>\n",
       "      <td>359</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>3554942_1</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>135941023</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7505</th>\n",
       "      <td>2595109</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>27404</td>\n",
       "      <td>1</td>\n",
       "      <td>359</td>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>2595109_1</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>135941023</td>\n",
       "      <td>609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187788</th>\n",
       "      <td>1589600</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>2016-01-03</td>\n",
       "      <td>39850</td>\n",
       "      <td>1</td>\n",
       "      <td>10332</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1589600_1</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>11033211035</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138821</th>\n",
       "      <td>4997413</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>2016-01-03</td>\n",
       "      <td>20497</td>\n",
       "      <td>1</td>\n",
       "      <td>359</td>\n",
       "      <td>4</td>\n",
       "      <td>75</td>\n",
       "      <td>4997413_1</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>135941023</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272651</th>\n",
       "      <td>4145326</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>43038</td>\n",
       "      <td>0</td>\n",
       "      <td>7974</td>\n",
       "      <td>4</td>\n",
       "      <td>169</td>\n",
       "      <td>4145326_1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>107</td>\n",
       "      <td>07974411107</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232502</th>\n",
       "      <td>5274588</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>52356</td>\n",
       "      <td>0</td>\n",
       "      <td>384</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>5274588_3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>038421169</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135142</th>\n",
       "      <td>1358731</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>31088</td>\n",
       "      <td>0</td>\n",
       "      <td>4541</td>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>1358731_1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>04541211126</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175371</th>\n",
       "      <td>890798</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>43524</td>\n",
       "      <td>0</td>\n",
       "      <td>9924</td>\n",
       "      <td>2</td>\n",
       "      <td>126</td>\n",
       "      <td>890798_1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>0992421175</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302077</th>\n",
       "      <td>5184195</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>27914</td>\n",
       "      <td>0</td>\n",
       "      <td>9924</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5184195_1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>0992411131</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>263124 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id    checkin   checkout  city_id  device_class  affiliate_id  \\\n",
       "52885    110418 2016-01-02 2016-01-03     3763             0          9924   \n",
       "26035   3554942 2016-01-02 2016-01-03    42359             1           359   \n",
       "7505    2595109 2016-01-02 2016-01-04    27404             1           359   \n",
       "187788  1589600 2016-01-02 2016-01-03    39850             1         10332   \n",
       "138821  4997413 2016-01-02 2016-01-03    20497             1           359   \n",
       "...         ...        ...        ...      ...           ...           ...   \n",
       "272651  4145326 2017-02-27 2017-02-28    43038             0          7974   \n",
       "232502  5274588 2017-02-27 2017-02-28    52356             0           384   \n",
       "135142  1358731 2017-02-27 2017-02-28    31088             0          4541   \n",
       "175371   890798 2017-02-27 2017-02-28    43524             0          9924   \n",
       "302077  5184195 2017-02-27 2017-02-28    27914             0          9924   \n",
       "\n",
       "        booker_country  hotel_country   utrip_id  checkin_week  checkin_month  \\\n",
       "52885                4             58   110418_1            53              1   \n",
       "26035                4             23  3554942_1            53              1   \n",
       "7505                 4             35  2595109_1            53              1   \n",
       "187788               1             35  1589600_1            53              1   \n",
       "138821               4             75  4997413_1            53              1   \n",
       "...                ...            ...        ...           ...            ...   \n",
       "272651               4            169  4145326_1             9              2   \n",
       "232502               2             35  5274588_3             9              2   \n",
       "135142               2             51  1358731_1             9              2   \n",
       "175371               2            126   890798_1             9              2   \n",
       "302077               1             47  5184195_1             9              2   \n",
       "\n",
       "        checkin_quarter  checkin_month_sin  checkin_month_cos  \\\n",
       "52885                 1                0.0           1.000000   \n",
       "26035                 1                0.0           1.000000   \n",
       "7505                  1                0.0           1.000000   \n",
       "187788                1                0.0           1.000000   \n",
       "138821                1                0.0           1.000000   \n",
       "...                 ...                ...                ...   \n",
       "272651                1                0.5           0.866025   \n",
       "232502                1                0.5           0.866025   \n",
       "135142                1                0.5           0.866025   \n",
       "175371                1                0.5           0.866025   \n",
       "302077                1                0.5           0.866025   \n",
       "\n",
       "        utrip_steps_from_end  row_num  total_rows  last  first_hotel_country  \\\n",
       "52885                    0.2        2          10     0                   58   \n",
       "26035                    0.5        2           4     0                   23   \n",
       "7505                     0.5        2           4     0                   23   \n",
       "187788                   0.5        2           4     0                   35   \n",
       "138821                   0.5        2           4     0                   23   \n",
       "...                      ...      ...         ...   ...                  ...   \n",
       "272651                   1.0        4           4     1                  107   \n",
       "232502                   1.0        4           4     1                   69   \n",
       "135142                   1.0        4           4     1                  126   \n",
       "175371                   1.0        4           4     1                   75   \n",
       "302077                   1.0        4           4     1                   31   \n",
       "\n",
       "                 ID  city_id_count  \n",
       "52885    0992441058           1138  \n",
       "26035     135941023             63  \n",
       "7505      135941023            609  \n",
       "187788  11033211035             62  \n",
       "138821    135941023             28  \n",
       "...             ...            ...  \n",
       "272651  07974411107             46  \n",
       "232502    038421169             38  \n",
       "135142  04541211126            103  \n",
       "175371   0992421175            128  \n",
       "302077   0992411131             30  \n",
       "\n",
       "[263124 rows x 21 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Following aggregation features - would be best to use time window (sort data) to generate, otherwise they will LEAK! (e.g. nunique countries visited)\n",
    "\n",
    "### count features (can also later add rank inside groups).\n",
    "### Some may be leaks (# visits in a trip should use time window?) , and do users repeat? \n",
    "### can add more counts of group X time period (e.g. affiliate X month of year)\n",
    "\n",
    "## alt way to get counts/freq :\n",
    "\n",
    "if GET_COUNT_AGG_FEATS:\n",
    "    count_cols = [ 'city_id','affiliate_id', 'booker_country', 'hotel_country', \n",
    "    #               'utrip_id','user_id', \n",
    "     \"checkin_month\",\"checkin_week\"]\n",
    "    for c in count_cols:\n",
    "        df[f\"{c}_count\"] = df.groupby([c])[\"duration\"].transform(\"size\")\n",
    "\n",
    "    ########################################################\n",
    "    ## nunique per trip\n",
    "    ### https://stackoverflow.com/questions/46470743/how-to-efficiently-compute-a-rolling-unique-count-in-a-pandas-time-series\n",
    "\n",
    "    nunique_cols = [ 'city_id','affiliate_id', 'booker_country', 'hotel_country']\n",
    "    # df[\"nunique_booker_countries\"] = df.groupby(\"utrip_id\")[\"booker_country\"].nunique()\n",
    "    # df[\"nunique_hotel_country\"] = df.groupby(\"utrip_id\")[\"hotel_country\"].nunique()\n",
    "    for c in nunique_cols:\n",
    "        df[f\"{c}_nunique\"] = df.groupby([\"utrip_id\"])[c].transform(\"nunique\")\n",
    "    print(df.nunique())\n",
    "\n",
    "    ########################################################\n",
    "    ## get frequency/count feature's rank within a group - e.g. within a country (or affiliate) \n",
    "    ## add \"_count\" to column name to get count col name, then add rank col \n",
    "\n",
    "    ### ALT/ duplicate feat - add percent rank (instead or in addition)\n",
    "\n",
    "    rank_cols = ['city_id','affiliate_id', 'booker_country','hotel_country',\n",
    "     \"checkin_month\"]\n",
    "    ### what is meaning of groupby and rank of smae variable by same var? Surely should be 1 / unary? \n",
    "    for c in rank_cols:\n",
    "        df[f\"{c}_rank_by_hotel_country\"] = df.groupby(['hotel_country'])[f\"{c}_count\"].transform(\"rank\")\n",
    "        df[f\"{c}_rank_by_booker_country\"] = df.groupby(['booker_country'])[f\"{c}_count\"].transform(\"rank\")\n",
    "        df[f\"{c}_rank_by_affiliate\"] = df.groupby(['affiliate_id'])[f\"{c}_count\"].transform(\"rank\")     \n",
    "else:\n",
    "    freq = df[\"city_id\"].value_counts()\n",
    "    df[\"city_id_count\"] = df[\"city_id\"].map(freq)\n",
    "    print(df[\"city_id_count\"].describe())\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4485"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df[\"city_id_count\"]>=7][\"city_id\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df.isna().sum().max() ==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240247\n",
      "df2 nunique (cities without duplicate user visits) 22976\n",
      "city counts\n",
      "23921    2017\n",
      "55128    1844\n",
      "47499    1516\n",
      "29319    1461\n",
      "64876    1387\n",
      "         ... \n",
      "53544       1\n",
      "47397       1\n",
      "24766       1\n",
      "49012       1\n",
      "2049        1\n",
      "Name: city_id, Length: 22976, dtype: int64\n",
      "count    22976.000000\n",
      "mean        10.456433\n",
      "std         52.498044\n",
      "min          1.000000\n",
      "25%          1.000000\n",
      "50%          2.000000\n",
      "75%          5.000000\n",
      "max       2017.000000\n",
      "Name: city_id, dtype: float64\n",
      "cities with at least 3: 8987\n",
      "cities with at least 7: 4323\n",
      "cities with at least 15: 2280\n",
      "cities with at least 30: 1306\n",
      "cities with at least 100: 430\n",
      "cities with at least 300: 103\n",
      "top 4 sum coverage (normalized):  0.028\n",
      "top 50 sum coverage (normalized):  0.18\n",
      "top 100 sum coverage (normalized):  0.26\n",
      "top 400 sum coverage (normalized):  0.478\n",
      "top 1,000 sum coverage (normalized):  0.64\n",
      "top 5,000 sum coverage (normalized):  0.865\n",
      "top 8,000 sum coverage (normalized):  0.914\n"
     ]
    }
   ],
   "source": [
    "# df2 = df[[\"user_id\",\"city_id\"]].drop_duplicates().copy()\n",
    "df2 = df.drop_duplicates(subset=[\"user_id\",\"city_id\"])[\"city_id\"].copy()\n",
    "print(df2.shape[0])\n",
    "print(\"df2 nunique (cities without duplicate user visits)\",df2.nunique())\n",
    "\n",
    "# c2_counts = df2[\"city_id\"].value_counts()\n",
    "c2_counts = df2.value_counts()\n",
    "# df2[\"new_counts\"] = df2[\"city_id\"].map(c2_counts)\n",
    "# df2[\"new_counts\"] = df2.map(c2_counts)\n",
    "print(\"city counts\")\n",
    "print(c2_counts)\n",
    "print(c2_counts.describe())\n",
    "print(\"cities with at least 3:\",(c2_counts>=3).sum())\n",
    "print(\"cities with at least 7:\",(c2_counts>=7).sum())\n",
    "print(\"cities with at least 15:\",(c2_counts>=15).sum())\n",
    "print(\"cities with at least 30:\",(c2_counts>=30).sum())\n",
    "print(\"cities with at least 100:\",(c2_counts>=100).sum())\n",
    "print(\"cities with at least 300:\",(c2_counts>=300).sum())\n",
    "\n",
    "c2_freq = df2.value_counts(normalize=True)\n",
    "print(\"top 4 sum coverage (normalized): \",c2_freq[0:4].sum().round(3))\n",
    "print(\"top 50 sum coverage (normalized): \",c2_freq[0:50].sum().round(3))\n",
    "print(\"top 100 sum coverage (normalized): \",c2_freq[0:100].sum().round(3))\n",
    "print(\"top 400 sum coverage (normalized): \",c2_freq[0:400].sum().round(3))\n",
    "print(\"top 1,000 sum coverage (normalized): \",c2_freq[0:1000].sum().round(3))\n",
    "print(\"top 5,000 sum coverage (normalized): \",c2_freq[0:5000].sum().round(3))\n",
    "print(\"top 8,000 sum coverage (normalized): \",c2_freq[0:8000].sum().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent city target List + City count encoding\n",
    "* Get the K most frequent target city IDs - selected based on frequency as final destination (not just overall)\n",
    "* +- Also after this, replace rare city IDs categorical features with count encoding to reduce dimensionality\n",
    "    * Keep them as count, or aggregate all of them as \"under_K\"?\n",
    "\n",
    "##### Output  : `TOP_TARGETS` - filter data by this *after* creation of lag features ! \n",
    "\n",
    "* Drop duplicates by the same user (reduce possible bias of frequent users? Only relevant if test is seperater from \"frequent travellers\") \n",
    "    * results in 216,633 , vs 217,686 without dropping duplicates by users\n",
    "    * ~19.9k unique cities\n",
    "    \n",
    "* Could do other encodings - https://contrib.scikit-learn.org/category_encoders/count.html\n",
    "\n",
    "* Note that all this is after we've added rank, count features beforehand, so that information won't be lost for these variables, despite these transforms\n",
    "\n",
    "\n",
    "\n",
    "* **NOTE** he most frequent final destinations are NOT the same as the most popular overall destinations +- first location ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if KEEP_TOP_K_TARGETS > 0 :\n",
    "    df_end = df.loc[df[\"utrip_steps_from_end\"]==1].drop_duplicates(subset=[\"city_id\",\"hotel_country\",\"user_id\"])[[\"city_id\",\"hotel_country\"]].copy()\n",
    "    print(df_end.shape[0])\n",
    "    end_city_counts = df_end.city_id.value_counts()\n",
    "    print(end_city_counts)\n",
    "    \n",
    "    TOP_TARGETS = end_city_counts.head(KEEP_TOP_K_TARGETS).index.values\n",
    "    print(f\"top {KEEP_TOP_K_TARGETS} targets \\n\",TOP_TARGETS)\n",
    "    \n",
    "#     assert df.loc[df[\"city_id\"].isin(TOP_TARGETS)][\"city_id\"].nunique() == KEEP_TOP_K_TARGETS\n",
    "\n",
    "####        \n",
    "# replace low frequency categoircal features    \n",
    "\n",
    "# ##replace with count encoding if have at least k, group rarest as \"-1\":# df[BASE_CAT_COLS] = df[BASE_CAT_COLS].where(df[BASE_CAT_COLS].apply(lambda x: x.map(x.value_counts()))>=LOW_COUNT_THRESH, -1)   \n",
    "# ## replace/group only the rare variables : \n",
    "# df[BASE_CAT_COLS] = df[BASE_CAT_COLS].where(df[BASE_CAT_COLS].apply(lambda x: x.map(x.value_counts()))>=LOW_COUNT_THRESH, -1)\n",
    "# df[BASE_CAT_COLS].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Long tail of targets warning!\n",
    "* 75% of cities appear less than 4 times in the data (as a final destination!) \n",
    "    * Dropping them will mean a maximum accuracy of 25% at best!!\n",
    "    * training on intermediates may help overcome improve this. \n",
    "* Using ~2d step+ , still leaves us with 75% appearing less than 7 times\n",
    "\n",
    "* Top 4,000 cities (just for those as final trip destination) - offers 89% coverage - \n",
    "\n",
    "* Unsure how to handle this - too amny targets to learn, and no auxiliary data to help learn it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df_end.city_id.value_counts(normalize=True)[0:4000].sum().round(3)# 4k: 89% coverage  (note, this is just for the end count cities, not all cities overall)\n",
    "\n",
    "# df_end.city_id.value_counts(normalize=True)[0:2000].sum().round(3) #7k: 97% coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter data by city id frequency\n",
    "#### df2 - smaller df (may not ben ecessary to make\n",
    "\n",
    "* drop rows if it's city id appears less than X times - this is prior to CF\n",
    "* We could also add inclusion/exclusion based on target appearing/frequency as target in final stage of rtip - optional\n",
    "* maybe also drop (end exclude in freq counting) thefirst point in a trip ? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropping users with less than 4 trips\n",
      "abnormal users dropped 372\n",
      "dropping cities with less than 30 occurences:\n",
      "185173\n",
      "nunique cities after freq filt 1354\n",
      "nunique city_id per hotel_country:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count     86.000000\n",
       "mean      15.744186\n",
       "std       27.907488\n",
       "min        1.000000\n",
       "25%        2.000000\n",
       "50%        5.000000\n",
       "75%       14.000000\n",
       "max      171.000000\n",
       "Name: city_id, dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### unsure about this filtering - depends if data points are real or mistake\n",
    "print(\"dropping users with less than 4 trips\")\n",
    "df2 = df.loc[df[\"total_rows\"]>=4].copy()\n",
    "print(\"abnormal users dropped\",df.shape[0]-df2.shape[0])\n",
    "\n",
    "print(f\"dropping cities with less than {MIN_TARGET_FREQ} occurences:\")\n",
    "df2 = df2.loc[df2.groupby([\"city_id\"])[\"hotel_country\"].transform(\"count\")>=MIN_TARGET_FREQ] ## update count\n",
    "# df2 = df2.loc[df2[\"city_id_count\"]>=MIN_TARGET_FREQ]\n",
    "\n",
    "print(df2.shape[0])\n",
    "\n",
    "print(\"nunique cities after freq filt\",df2[\"city_id\"].nunique())\n",
    "print(\"nunique city_id per hotel_country:\")\n",
    "df2.groupby([\"hotel_country\"])[\"city_id\"].nunique().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1354"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[\"city_id\"].nunique()#.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    54063.000000\n",
       "mean         3.059856\n",
       "std          1.794779\n",
       "min          1.000000\n",
       "25%          2.000000\n",
       "50%          3.000000\n",
       "75%          4.000000\n",
       "max         30.000000\n",
       "Name: city_id, dtype: float64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.groupby([\"user_id\"])[\"city_id\"].nunique().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAG_FEAT_COLS = ['city_id', 'device_class',\n",
    "#        'affiliate_id', 'booker_country', 'hotel_country', \n",
    "#        'duration', 'same_country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ### lag features - last n visits\n",
    "# groupbyLagFeatures(df=df2.head(20), # .set_index([\"checkin\",\"checkout\",\"user_id\"])\n",
    "#                    lag=[1,2],group=\"utrip_id\",lag_feature_cols=LAG_FEAT_COLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get a DF of all cities per country\n",
    "* +- get from original DF, +- remove cities that appear less than 4? times , and countries with less than 4 hotels? (Or keep - to avoid messing up training?)\n",
    "* Weighted Sample from it, for negatives, +- most freq by country/affiliate/etc\n",
    "* Don't drop duplicates by user, keep orig freq? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city_id          347\n",
      "hotel_country     55\n",
      "city_id_count    153\n",
      "dtype: int64\n",
      "347\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_id</th>\n",
       "      <th>hotel_country</th>\n",
       "      <th>city_id_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70913</th>\n",
       "      <td>23921</td>\n",
       "      <td>31</td>\n",
       "      <td>962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22192</th>\n",
       "      <td>47499</td>\n",
       "      <td>67</td>\n",
       "      <td>787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57051</th>\n",
       "      <td>55128</td>\n",
       "      <td>46</td>\n",
       "      <td>753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81585</th>\n",
       "      <td>36063</td>\n",
       "      <td>52</td>\n",
       "      <td>729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60950</th>\n",
       "      <td>29319</td>\n",
       "      <td>31</td>\n",
       "      <td>696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89796</th>\n",
       "      <td>11052</td>\n",
       "      <td>28</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108400</th>\n",
       "      <td>41111</td>\n",
       "      <td>123</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72610</th>\n",
       "      <td>20820</td>\n",
       "      <td>8</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123301</th>\n",
       "      <td>18063</td>\n",
       "      <td>31</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72004</th>\n",
       "      <td>17429</td>\n",
       "      <td>31</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>347 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        city_id  hotel_country  city_id_count\n",
       "70913     23921             31            962\n",
       "22192     47499             67            787\n",
       "57051     55128             46            753\n",
       "81585     36063             52            729\n",
       "60950     29319             31            696\n",
       "...         ...            ...            ...\n",
       "89796     11052             28             51\n",
       "108400    41111            123             50\n",
       "72610     20820              8             50\n",
       "123301    18063             31             50\n",
       "72004     17429             31             50\n",
       "\n",
       "[347 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_cities = df[[\"city_id\",\"hotel_country\",\"city_id_count\"]] ## +- drop duplicates by tripid? \n",
    "df_cities = df2[[\"city_id\",\"hotel_country\",\"city_id_count\"]].drop_duplicates().sort_values([\"city_id_count\",\"city_id\"],ascending=False).head(9123)\n",
    "# print(df_cities.nunique())\n",
    "# df_cities = df_cities.loc[df_cities.groupby(\"hotel_country\")[\"city_id\"].transform(\"nunique\")>4]\n",
    "# df_cities = df_cities.loc[df_cities[\"city_id_count\"]>=10].sort_values(\"city_id_count\",ascending=False)\n",
    "print(df_cities.nunique())\n",
    "print(df_cities.shape[0])\n",
    "df_cities\n",
    "\n",
    "# ### 5 most frequent overall\n",
    "# df_city_samples = df_cities.drop_duplicates().sort_values(\"city_id_count\",ascending=False).groupby(\"city_id\").head(5) \n",
    "# df_city_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    347.000000\n",
       "mean     136.985591\n",
       "std      131.295930\n",
       "min       50.000000\n",
       "25%       65.000000\n",
       "50%       91.000000\n",
       "75%      142.000000\n",
       "max      962.000000\n",
       "Name: city_id_count, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cities.city_id_count.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add lag features + Train/test/data split\n",
    "* Lag feats (remember for categorical)\n",
    "* Drop leak features (target values - country, city)\n",
    "\n",
    "* drop instances  that lack history (e.g. at least 3d step and onwards) - by dropna in lag feat\n",
    "* fill nans\n",
    "* Split train/test by `user id` / split could maybe be by `utrip ID` ? ? \n",
    "    * Test - only last trip\n",
    "    *  stratified train/test split by class - then drop any train rows with overlap with tests' IDs.  \n",
    "        * Could also stratify by users, but risks some classes being non present in test\n",
    "        \n",
    "###### Big possible improvement to lag features: Have \"first location\" (starting point) \"lag\" feature\n",
    "* `groupbyFirstLagFeatures(df,group=\"user_id\",lag_feature_cols=[\"hotel_country\",\"city_id\"])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### features to drop - not usable, or leaks (e.g. aggregations on target)\n",
    "\n",
    "\n",
    "# DROP_FEATS = ['user_id',\n",
    "#     'checkin', 'checkout',\n",
    "#               'hotel_country','city_id_count','same_country',\n",
    "#               'utrip_id',\n",
    "# #               'utrip_steps_from_end',\n",
    "#              'city_id_count','hotel_country_count',\n",
    "#               'city_id_nunique', 'hotel_country_nunique',\n",
    "#               'city_id_rank_by_hotel_country','city_id_rank_by_booker_country', 'city_id_rank_by_affiliate',\n",
    "#               'affiliate_id_rank_by_hotel_country','affiliate_id_rank_by_booker_country', 'affiliate_id_rank_by_affiliate',\n",
    "#               'hotel_country_rank_by_hotel_country',\n",
    "#        'hotel_country_rank_by_booker_country','hotel_country_rank_by_affiliate',\n",
    "#               'booker_country_rank_by_hotel_country','booker_country_rank_by_booker_country',\n",
    "#               'checkin_month_rank_by_hotel_country',\n",
    "#              ]\n",
    "\n",
    "# df2.drop(DROP_FEATS,axis=1).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "* we will want later to split also by user/`utrip_Id`...  - add that ot variables\n",
    "\n",
    "* We may want to keep the count of the targets - for explicit reccomenders (i.e frequncy of an id visiting some location) - could use to augment feature matrix, e.g. with svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID', 'device_class', 'affiliate_id', 'booker_country', 'checkin_quarter', 'last', 'first_hotel_country', 'city_id', 'hotel_country']\n",
      "(47468, 21)\n",
      "IDs: 5777\n",
      "drop duplicates after subset of variables shape: (34196, 9)\n",
      "unique cities: 347\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>device_class</th>\n",
       "      <th>affiliate_id</th>\n",
       "      <th>booker_country</th>\n",
       "      <th>checkin_quarter</th>\n",
       "      <th>last</th>\n",
       "      <th>first_hotel_country</th>\n",
       "      <th>city_id</th>\n",
       "      <th>hotel_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52885</th>\n",
       "      <td>0992441051</td>\n",
       "      <td>0</td>\n",
       "      <td>9924</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>3763</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121895</th>\n",
       "      <td>19452310137</td>\n",
       "      <td>1</td>\n",
       "      <td>9452</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>15284</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7505</th>\n",
       "      <td>135941021</td>\n",
       "      <td>1</td>\n",
       "      <td>359</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>27404</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70913</th>\n",
       "      <td>135911031</td>\n",
       "      <td>1</td>\n",
       "      <td>359</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>23921</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8435</th>\n",
       "      <td>19452310137</td>\n",
       "      <td>1</td>\n",
       "      <td>9452</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>60274</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31105</th>\n",
       "      <td>1359411104</td>\n",
       "      <td>1</td>\n",
       "      <td>359</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>104</td>\n",
       "      <td>8766</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19718</th>\n",
       "      <td>0992401141</td>\n",
       "      <td>0</td>\n",
       "      <td>9924</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>3763</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51826</th>\n",
       "      <td>2992421193</td>\n",
       "      <td>2</td>\n",
       "      <td>9924</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>25025</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44465</th>\n",
       "      <td>0146921131</td>\n",
       "      <td>0</td>\n",
       "      <td>1469</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>48968</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79672</th>\n",
       "      <td>28065111104</td>\n",
       "      <td>2</td>\n",
       "      <td>8065</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>104</td>\n",
       "      <td>36073</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34196 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID  device_class  affiliate_id  booker_country  \\\n",
       "52885    0992441051             0          9924               4   \n",
       "121895  19452310137             1          9452               3   \n",
       "7505      135941021             1           359               4   \n",
       "70913     135911031             1           359               1   \n",
       "8435    19452310137             1          9452               3   \n",
       "...             ...           ...           ...             ...   \n",
       "31105    1359411104             1           359               4   \n",
       "19718    0992401141             0          9924               0   \n",
       "51826    2992421193             2          9924               2   \n",
       "44465    0146921131             0          1469               2   \n",
       "79672   28065111104             2          8065               1   \n",
       "\n",
       "        checkin_quarter  last  first_hotel_country  city_id  hotel_country  \n",
       "52885                 1     0                   51     3763             51  \n",
       "121895                1     0                  137    15284            137  \n",
       "7505                  1     0                   21    27404             31  \n",
       "70913                 1     0                   31    23921             31  \n",
       "8435                  1     0                  137    60274            126  \n",
       "...                 ...   ...                  ...      ...            ...  \n",
       "31105                 1     1                  104     8766            104  \n",
       "19718                 1     1                   41     3763             51  \n",
       "51826                 1     1                   93    25025             94  \n",
       "44465                 1     1                   31    48968             31  \n",
       "79672                 1     1                  104    36073            104  \n",
       "\n",
       "[34196 rows x 9 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## n ote - we will want later to split also by user/utrip Id... \n",
    "print(MF_KEEP_COLS)\n",
    "print(df2.shape)\n",
    "df2 = df2[MF_KEEP_COLS]\n",
    "print(\"IDs:\",df2[\"ID\"].nunique())\n",
    "## for now - drop duplicates/frequency info. (could keeo as count ?)\n",
    "df2 = df2.drop_duplicates()\n",
    "print(\"drop duplicates after subset of variables shape:\",df2.shape)\n",
    "print(\"unique cities:\",df2[\"city_id\"].nunique())\n",
    "# print(\"positives targets/cities per id\\n\",df2.groupby(\"ID\"))\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5777.000000\n",
       "mean        5.919335\n",
       "std        14.993434\n",
       "min         1.000000\n",
       "25%         1.000000\n",
       "50%         2.000000\n",
       "75%         4.000000\n",
       "max       206.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.groupby(\"ID\",observed=True).size().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cities_per_id\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    5777.000000\n",
       "mean        5.919335\n",
       "std        14.993434\n",
       "min         1.000000\n",
       "25%         1.000000\n",
       "50%         2.000000\n",
       "75%         4.000000\n",
       "max       206.000000\n",
       "Name: city_id, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities_per_id = df2.groupby(\"ID\",observed=True)[\"city_id\"].nunique() ## if we don't user observed = True, we'd get all combinations counted #.transform(\"nunique\")\n",
    "print(\"cities_per_id\")\n",
    "cities_per_id.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join DF with negatives of all possible, filtered city/county combinations\n",
    "* Note that this will yield a **LOT** of rows - with lots of sparsity (vast majority are 0)\n",
    "* Ideally - we would sample from a sparse user-item cooccurrence, or sample from scratch with every epoch. But that's hard. \n",
    "    * If sampling - we should be sure to keep the most frequent/popular overall. e.g. sample 1k + 10 most popular ? \n",
    "    \n",
    "    \n",
    "* Cartesian product ? https://stackoverflow.com/questions/13269890/cartesian-product-in-pandas\n",
    "    * https://stackoverflow.com/questions/34161978/pandas-two-dataframe-cross-join\n",
    "    \n",
    "* This would be **MUCH** more effecient with a sparse matrix (\"user-item cooccurrence\"), and we could even apply SVD directly to a spars ematrix - but it is harder to use TF/keras on such a matrix, without a special generator. \n",
    "\n",
    "* I will downsample the result randomly, keeping all positives, and downsampling negatives per \"ID\", to a given ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Any input passed containing Categorical data will have all of its categories included in the cross-tabulation, even if the actual data does not contain any instances of a particular category.\"\n",
    "* https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.crosstab.html\n",
    "\n",
    "\n",
    "USe crosstab, stack/unstack, with categorical - \n",
    "https://stackoverflow.com/questions/57385009/pandas-groupby-observed-parameter-with-multiple-categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf2.groupby([\"ID\",\"city_id\"],observed=False,dropna=False)[\"target\"].count().fillna(0).astype(int).reset_index().describe()\\n\\n# df2.groupby([\"ID\",\"city_id\"],observed=True,dropna=False)[\"target\"].count().reset_index().describe()\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "### (on subset) 2 million rows, mean target: 1.7%\n",
    "\"\"\"\n",
    "df2.groupby([\"ID\",\"city_id\"],observed=False,dropna=False)[\"target\"].count().fillna(0).astype(int).reset_index().describe()\n",
    "\n",
    "# df2.groupby([\"ID\",\"city_id\"],observed=True,dropna=False)[\"target\"].count().reset_index().describe()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       hotel_country        target\n",
      "count   1.062014e+06  1.062014e+06\n",
      "mean    8.116279e+01  1.743602e-01\n",
      "std     4.874589e+01  2.499654e+00\n",
      "min     2.000000e+00  0.000000e+00\n",
      "25%     3.500000e+01  0.000000e+00\n",
      "50%     8.150000e+01  0.000000e+00\n",
      "75%     1.220000e+02  0.000000e+00\n",
      "max     1.690000e+02  6.260000e+02\n",
      "Wall time: 531 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>hotel_country</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-203035</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0-203035</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0-203035</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0-203035</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0-203035</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062009</th>\n",
       "      <td>29970441138</td>\n",
       "      <td>157</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062010</th>\n",
       "      <td>29970441138</td>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062011</th>\n",
       "      <td>29970441138</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062012</th>\n",
       "      <td>29970441138</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062013</th>\n",
       "      <td>29970441138</td>\n",
       "      <td>169</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1062014 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ID  hotel_country  target\n",
       "0           0-203035              2       0\n",
       "1           0-203035              4       0\n",
       "2           0-203035              5       0\n",
       "3           0-203035              7       0\n",
       "4           0-203035              8       0\n",
       "...              ...            ...     ...\n",
       "1062009  29970441138            157       0\n",
       "1062010  29970441138            161       0\n",
       "1062011  29970441138            162       0\n",
       "1062012  29970441138            168       0\n",
       "1062013  29970441138            169       0\n",
       "\n",
       "[1062014 rows x 3 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "### all combinations - slow, outputs all combinations, 3d column [\"0\"] is sum/count \n",
    "## solution from : https://stackoverflow.com/questions/57385009/pandas-groupby-observed-parameter-with-multiple-categoricals\n",
    "\n",
    "df_interactions = pd.crosstab(df2[\"ID\"],df2[TARGET_COL]).stack().reset_index()\n",
    "\n",
    "df_interactions.rename(columns={0:\"target\"},inplace=True)\n",
    "\n",
    "print(df_interactions.describe())\n",
    "# df_interactions.loc[df_interactions.iloc[:,2]>0].describe()\n",
    "\n",
    "### we may want to leave the count unchanged, for purposes of a recomender! \n",
    "df_interactions[\"target\"] = df_interactions[\"target\"].clip(upper=1)\n",
    "df_interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save intermediate results to disk + add id feats\n",
    "* all interactions (after the filtering, befoer class wise subsampling)\n",
    "* also join with categoircal feats\n",
    "\n",
    "* jion with subset of most popular cities + sample from other cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_interactions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-9861b98bffa3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_interactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_interactions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_ID\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ID\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_interactions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#save to disk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mSAVE_TO_DISK\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_interactions' is not defined"
     ]
    }
   ],
   "source": [
    "df_interactions = df_interactions.join(df_ID,on=\"ID\")\n",
    "df_interactions\n",
    "\n",
    "#save to disk\n",
    "if SAVE_TO_DISK:\n",
    "    df_interactions.to_csv(\"interactions_filtered_raw.csv.gz\",index=False,compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cities_top = df_cities.head(99).drop([\"city_id_count\"],axis=1) ## top K most popular cities\n",
    "df_cities_bottom = df_cities[99:].drop([\"city_id_count\"],axis=1) ## all remainign cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Subsample training data by class and group\n",
    "* Afterwards, join with individual variables (per id, and also city to country)\n",
    "\n",
    "* Note that we're taking negatives only from our target subset here, and aren't addressing frequency either...\n",
    "    * Approach would be best if we resampled many times (per epoch?)   to improve coverage \n",
    "\n",
    "* NOTE: Currently missing any split by user, or final stage in trip level! \n",
    "* May leak to train like this.. !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(100*df_interactions[\"target\"].mean(),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interactions.loc[df_interactions[\"target\"]==1].groupby('ID',observed=True).size().describe() #.sample(frac=.3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interactions.loc[df_interactions[\"target\"]==0].groupby('ID',observed=True).size().describe() #.sample(frac=.3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interactions.loc[df_interactions[\"target\"]==0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert df_interactions.loc[df_interactions[\"target\"]==1].isna().sum().max() ==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "NEGATIVES_DOWNSAMPLE_FRAC = 0.1\n",
    "if TARGET_COL ==\"city_id\":\n",
    "    ### guarantee K most popular destinations are included if in negatives: \n",
    "    df_neg1 = df_interactions.loc[df_interactions[\"target\"]==0].merge(df_cities_top,on=\"city_id\",how=\"inner\")\n",
    "\n",
    "    ### sample randomly k or % interactions per \"ID\": \n",
    "    df_neg2 = df_interactions.loc[df_interactions[\"target\"]==0].merge(df_cities_bottom,on=\"city_id\",how=\"inner\")\n",
    "    df_neg2 = df_neg2.groupby(\"ID\",observed=True).sample(frac=NEGATIVES_DOWNSAMPLE_FRAC)\n",
    "    # df_neg2\n",
    "\n",
    "    ### I do not htink there is a need to add a downsample specifically for \"last\"==1 (since we keep all the positives anyway)\n",
    "    \n",
    "    df_feat = pd.concat([df_interactions.loc[df_interactions[\"target\"]==1].merge(df_cities[[\"city_id\",\"hotel_country\"]],on=\"city_id\",how=\"inner\"),\n",
    "                        df_neg1,df_neg2],ignore_index=True).drop_duplicates().sample(frac=1)\n",
    "    del df_neg1,df_neg2\n",
    "\n",
    "else: ## using country as target\n",
    "    df_feat = df_interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"target mean %:\",round(100*df_feat[\"target\"].mean(),3))\n",
    "assert df_feat.isna().sum().max()==0\n",
    "if TARGET_COL ==\"city_id\":assert df_feat.drop_duplicates([\"ID\",\"city_id\"]).shape[0] == df_feat.shape[0]\n",
    "\n",
    "## for some reason, affilaite id gets turned into a float, fix this: \n",
    "assert df_feat[\"affiliate_id\"].nunique() == df_feat[\"affiliate_id\"].astype(int).nunique()\n",
    "df[\"affiliate_id\"] = df[\"affiliate_id\"].astype(int)\n",
    "\n",
    "## if doing quarter as embedding instead of continous :\n",
    "# assert df_feat[\"checkin_quarter\"].nunique() == df_feat[\"checkin_quarter\"].astype(int).nunique()\n",
    "# df[\"checkin_quarter\"] = df[\"checkin_quarter\"].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save intermediate result to disk\n",
    "if SAVE_TO_DISK:\n",
    "    df_feat.to_csv(\"interactions_filtered_train.csv.gz\",index=False,compression=\"gzip\")\n",
    "\n",
    "df_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"dropping ID\")\n",
    "df_feat.drop(\"ID\",axis=1,inplace=True,errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### city ID encoder & other features ordinal encoder\n",
    "* Encode city id into integer range with label_encoder\n",
    "* This will make embedding more effecient \n",
    "* IMPORTANT: We will need to map back using this encoder, in order to get original IDs. \n",
    "\n",
    "* We do it at this step, since we've dropped so many targets beforehand. We could do this as first step in pipeline as well ! \n",
    "\n",
    "* I do this also for the other categorical variables. For them we don't save the transformer since we don't care about reversing ? \n",
    "     * **WARNING**: Check this doesn't mess things when predicting on new data !!!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAT_COLS_LIST = ['device_class', 'affiliate_id', 'booker_country']\n",
    "\n",
    "# print(df_feat[\"city_id\"].min())\n",
    "# df_feat[\"city_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TARGET_COL ==\"city_id\": \n",
    "    city_encoder = LabelEncoder().fit(df_feat[\"city_id\"])\n",
    "country_encoder = LabelEncoder().fit(df_feat[\"hotel_country\"]) \n",
    "\n",
    "df_feat[\"city_id\"] = city_encoder.transform(df_feat[\"city_id\"])\n",
    "df_feat[\"hotel_country\"] = country_encoder.transform(df_feat[\"hotel_country\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_feat[CAT_COLS_LIST].describe())\n",
    "categories_ordinal_encoder = OrdinalEncoder(dtype=np.int64)\n",
    "categories_ordinal_encoder.fit(df_feat[CAT_COLS_LIST])\n",
    "df_feat[CAT_COLS_LIST] = categories_ordinal_encoder.transform(df_feat[CAT_COLS_LIST])\n",
    "print(df_feat[CAT_COLS_LIST].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Artifical train/test split grouping \n",
    "\n",
    "* Make additional grouping for train/test split.\n",
    "WARNING: This is NOT as good/reliable as splitting by USER_ID or utripID, but we do this for now, since we don't have that variable here\n",
    "* This splitting would result in some variables not being learned ? \n",
    "\n",
    "* Ignore some of the ID columns for this groups creation\n",
    "    * warning - manua/fragile/may break if columns used change\n",
    "    \n",
    "This splitting is **not reliable**!! It is just to try to approximate something like splitting across trips\n",
    "\n",
    "* Mainly - groups should include rows with last 0/1 for same trip\n",
    "    * How bad is it that we group/exclude by city_id (for a specific grouping)  ?? \n",
    "    * Alt - could do group also with lag1_city_id ? (If using that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat[\"Group_id\"] = df_feat[[ 'device_class', 'affiliate_id', 'booker_country',\n",
    "                                f\"{TARGET_COL}\", #\"hotel_country\",  # 'city_id',\n",
    "                               'checkin_quarter']].astype(str).sum(1)\n",
    "print(df_feat[\"Group_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inds, test_inds = next(GroupShuffleSplit(test_size=.25, n_splits=2, random_state = 7).split(df_feat, groups=df_feat['Group_id']))\n",
    "\n",
    "train = df_feat.iloc[train_inds].drop(\"Group_id\",axis=1)\n",
    "test = df_feat.iloc[test_inds]\n",
    "\n",
    "## split test into validation and final test (only last stage of trip). We could also spliut by group here as well\n",
    "\n",
    "valid_inds, test_inds  = next(GroupShuffleSplit(test_size=.6, n_splits=2, random_state = 7).split(test, groups=test['Group_id']))\n",
    "valid = test.iloc[valid_inds].drop(\"Group_id\",axis=1)\n",
    "test = test.iloc[test_inds].drop(\"Group_id\",axis=1)\n",
    "test = test.loc[test[\"last\"]==1]\n",
    "\n",
    "print(\"train\",train.shape[0],\"valid\",valid.shape[0],\"test\",test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df2.shape)\n",
    "# # ### lag features - last n visits\n",
    "# df_feat = groupbyLagFeatures(df=df2.copy(), \n",
    "#                    lag=[1],group=\"utrip_id\",lag_feature_cols=LAG_FEAT_COLS)\n",
    "# df_feat = df_feat.dropna(subset=[\"lag3_city_id\"]).sample(frac=1)\n",
    "\n",
    "### filter for only trip targets that are among the K most popular :\n",
    "\n",
    "# df_feat = df_feat.drop(DROP_FEATS,axis=1,errors=\"ignore\")\n",
    "# print(df_feat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### filter for most frequent targets\n",
    "\n",
    "if KEEP_TOP_K_TARGETS > 0 :\n",
    "    print(df_feat.shape[0])\n",
    "    df_feat = df_feat.loc[df_feat[\"city_id\"].isin(TOP_TARGETS)]\n",
    "    print(df_feat.shape[0])    \n",
    "    assert df_feat[\"city_id\"].nunique() == KEEP_TOP_K_TARGETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########################\n",
    "# ## stratified train/test split by class - then drop any train rows with overlap wit htest IDs.  Could also stratify by users, but risks some classes being non present in test\n",
    "# ### split could maybe be by utrip ID ? \n",
    "# ### orig - split by group : \n",
    "\n",
    "# # train_inds, test_inds = next(GroupShuffleSplit(test_size=.2, n_splits=2, random_state = 7).split(df_feat, groups=df_feat['user_id']))\n",
    "# # X_train = df_feat.iloc[train_inds].drop(DROP_FEATS,axis=1,errors=\"ignore\")\n",
    "# # X_test = df_feat.iloc[test_inds].drop(DROP_FEATS,axis=1,errors=\"ignore\")\n",
    "# # assert (set(X_train[TARGET_COL].unique()) == set(X_test[TARGET_COL].unique()))\n",
    "# #################\n",
    "# ## alt: split by class. May be leaky! \n",
    "# X_train, X_test = train_test_split(df_feat,stratify=df_feat[TARGET_COL])\n",
    "\n",
    "# ########################\n",
    "# print(\"X_train\",X_train.shape[0])\n",
    "# ## get last row in trip only in test/eval set: \n",
    "# print(\"X_test\",X_test.shape[0])\n",
    "\n",
    "# ### following is for splitting by group - can't do so currebntly as group/user id col is missing\n",
    "# # X_test = X_test.loc[X_test[\"utrip_steps_from_end\"]==1] # last row per trip\n",
    "# # print(\"X_test after filtering for last instance per trip\",X_test.shape[0])\n",
    "\n",
    "# # y_train = X_train.pop(TARGET_COL)\n",
    "# # y_test = X_test.pop(TARGET_COL)\n",
    "\n",
    "# # print(\"# classes\",y_train.nunique())\n",
    "\n",
    "# # # ## check that same classes in train and test - \n",
    "# # # assert (set(y_train.unique()) == set(y_test.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "* For now - simple multiclass model (Tabnet? LSTM?) ; +- subsample - only most frequent classes/cities\n",
    "\n",
    "    * Tabnet: `pip install pytorch-tabnet`\n",
    "        * https://github.com/dreamquark-ai/tabnet/blob/develop/forest_example.ipynb\n",
    "    * TensorFlow Tabmet: https://github.com/ostamand/tensorflow-tabnet/blob/master/examples/train_mnist.py\n",
    "\n",
    "* split train/test by groups\n",
    "    * Test - only last trip. \n",
    "\n",
    "\n",
    "* Preprocess for TF/keras + dot product, emebddings etc'\n",
    "\n",
    "* categoircal + embedding example : \n",
    "    * https://www.tensorflow.org/tutorials/structured_data/feature_columns\n",
    "    * https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_identity\n",
    "    \n",
    "    \n",
    "* Shared embeddings (for use of lag1 city, country id : https://www.tensorflow.org/api_docs/python/tf/feature_column/shared_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add tf feature columns\n",
    "* https://www.tensorflow.org/tutorials/structured_data/feature_columns#choose_which_columns_to_use\n",
    "* Assumes use of a tf.data pipeline  (`df_to_dataset`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = df_to_dataset(train, shuffle=False, batch_size=256)\n",
    "val_ds = df_to_dataset(valid, shuffle=False, batch_size=512)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=512)\n",
    "val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAT_COLS_LIST + [\"city_id\",\"hotel_country\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = []\n",
    "\n",
    "# numeric cols\n",
    "for header in ['checkin_quarter',\"last\"]:\n",
    "    feature_columns.append(feature_column.numeric_column(header))\n",
    "    \n",
    "# Categorical cols (\"generic\") & embedding (We could use strings here instead of numbers - https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_list\n",
    "# embedding columns\n",
    "for col_name in CAT_COLS_LIST + [\"city_id\",\"hotel_country\"]:\n",
    "    print(col_name)\n",
    "    # breed1 = feature_column.categorical_column_with_vocabulary_list(\n",
    "    #       'Breed1', dataframe.Breed1.unique())\n",
    "    c_uniques = df_feat[col_name].nunique()\n",
    "    categorical_column = feature_column.categorical_column_with_identity(\n",
    "        col_name, c_uniques)\n",
    "    col_embedding = feature_column.embedding_column(categorical_column, dimension=min(c_uniques//2,50))\n",
    "    feature_columns.append(col_embedding)\n",
    "    \n",
    "\n",
    "# # indicator_columns - do one hot encoding - maybe instead of embedding ?? \n",
    "indicator_column_names = [\"device_class\"] ## \"booker_country\",\n",
    "for col_name in indicator_column_names:\n",
    "#     categorical_column = feature_column.categorical_column_with_vocabulary_list(\n",
    "#         col_name, dataframe[col_name].unique())\n",
    "    categorical_column = feature_column.categorical_column_with_identity(\n",
    "        col_name, df_feat[col_name].nunique())\n",
    "    \n",
    "    indicator_column = feature_column.indicator_column(categorical_column)\n",
    "    feature_columns.append(indicator_column)\n",
    "\n",
    "    \n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Todo : how to get dot product(s) between the inputs? \n",
    "\n",
    "\n",
    "https://stackoverflow.com/questions/54375298/how-to-use-tensorflow-feature-columns-as-input-to-a-keras-model\n",
    "\n",
    "https://towardsdatascience.com/how-to-build-a-wide-and-deep-model-using-keras-in-tensorflow-2-0-2f7a236b5a4b\n",
    "\n",
    "* https://www.tensorflow.org/tutorials/load_data/csv#mixed_data_types\n",
    "\n",
    "* https://www.kaggle.com/blaskowitz100/dnn-keras-and-categorical-feature-embedding  - without using `tf.feature_column`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set output bias for better initial guesses / class imbalance : \n",
    "## https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#optional_set_the_correct_initial_bias\n",
    "output_bias = tf.keras.initializers.Constant(np.log([train[\"target\"].mean()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FC model, without the dot product\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "#   feature_layer,\n",
    "# #   layers.Dense(128, activation='elu'),\n",
    "# #     layers.Dropout(.2),\n",
    "#   layers.Dense(64, activation='relu'),\n",
    "#   layers.Dropout(.1),\n",
    "#   layers.Dense(1, activation='sigmoid',bias_initializer=output_bias)\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss=\"binary_crossentropy\", #tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#               metrics=['accuracy',\"AUC\",\"Recall\",\"Precision\"]) # 'accuracy', ,tf.keras.metrics.TopKCategoricalAccuracy(4) - not used in ranking\n",
    "\n",
    "\n",
    "# model.fit(train_ds,\n",
    "#           validation_data=val_ds,\n",
    "#           epochs=max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### https://towardsdatascience.com/how-to-build-a-wide-and-deep-model-using-keras-in-tensorflow-2-0-2f7a236b5a4b\n",
    "\n",
    "# feature_layer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/blaskowitz100/dnn-keras-and-categorical-feature-embedding\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def get_keras_dataset(df):\n",
    "    X = {str(col) : np.array(df[col]) for col in df.columns}\n",
    "    return X\n",
    "\n",
    "\n",
    "get_keras_dataset(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_feat.nunique()+1)//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## based on:  https://www.kaggle.com/blaskowitz100/dnn-keras-and-categorical-feature-embedding\n",
    "cat_inputs = []\n",
    "num_inputs = []\n",
    "embeddings = []\n",
    "embedding_layer_names = []\n",
    "# emb_n = 57\n",
    "CITY_EMB_DIM = 57 # dimensions for hotel_country embedding - limited by user level variables\n",
    "HOTEL_COUNTRY_EMB_DIM = 7 # dimensions for hotel_country embedding - limited by subset of user level variables\n",
    "\n",
    "numeric_cols = [\"last\"] ## \"checkin_quarter\"\n",
    "\n",
    "categorical_cols_user = ['device_class', 'booker_country',\"checkin_quarter\", 'affiliate_id'] # , 'affiliate_id'\n",
    "categorical_cols_item = ['city_id', 'hotel_country']\n",
    "categorical_cols = categorical_cols_item + categorical_cols_user \n",
    "\n",
    "\n",
    "## Embedding for categorical, \"X\" features (vs \"targets\") - do items first for easy selection. Do twice, to control embed sizes for dot product\n",
    "for col in categorical_cols_item[0:1]: # city only\n",
    "    _input = layers.Input(shape=[1], name=col)    \n",
    "    c_uniques = df_feat[col].nunique()\n",
    "    _embed = layers.Embedding(df_feat[col].max() + 1, CITY_EMB_DIM, name=col+'_emb')(_input)\n",
    "    cat_inputs.append(_input)\n",
    "    embeddings.append(_embed) \n",
    "    embedding_layer_names.append(col+'_emb')\n",
    "for col in categorical_cols_item[1:2]: # hotel_country only only\n",
    "    _input = layers.Input(shape=[1], name=col)    \n",
    "    c_uniques = df_feat[col].nunique()\n",
    "    _embed = layers.Embedding(df_feat[col].max() + 1, HOTEL_COUNTRY_EMB_DIM, name=col+'_emb')(_input)\n",
    "    cat_inputs.append(_input)\n",
    "    embeddings.append(_embed) \n",
    "    embedding_layer_names.append(col+'_emb')\n",
    "    \n",
    "    \n",
    "for col in categorical_cols_user:\n",
    "    _input = layers.Input(shape=[1], name=col)    \n",
    "    c_uniques = df_feat[col].nunique()\n",
    "    _embed = layers.Embedding(df_feat[col].max() + 1, min((c_uniques+1)//2,50), name=col+'_emb')(_input)\n",
    "    cat_inputs.append(_input)\n",
    "    embeddings.append(_embed) \n",
    "    embedding_layer_names.append(col+'_emb')## doesn't seem to be used? \n",
    "    \n",
    "# Simple inputs for the numeric features\n",
    "for col in numeric_cols:\n",
    "    numeric_input = layers.Input(shape=(1,), name=col)\n",
    "    num_inputs.append(numeric_input)\n",
    "   \n",
    "    \n",
    "# # Merge the numeric inputs\n",
    "# merged_num_inputs = layers.concatenate(num_inputs)\n",
    "\n",
    "#numeric_dense = layers.Dense(20, activation='relu')(merged_num_inputs)\n",
    "\n",
    "### try to get just user, and just item embeddings cincated seperately. Based on input order\n",
    "\n",
    "# target_inputs =  layers.concatenate(embeddings[0:2])\n",
    "\n",
    "### TODO - concat with numerics also\n",
    "user_inputs =  layers.concatenate(embeddings[2:])\n",
    "# user_inputs =  layers.concatenate([embeddings[2:],num_inputs]) ### I currently couldn't get the concat working!! \n",
    "\n",
    "# dot_score = layers.Dot(axes=(2))([target_inputs,user_inputs]) ## meantto be used on all users X all item (city+country)\n",
    "\n",
    "dot_score_city1 = layers.Dot(axes=(2),normalize=False)([embeddings[0],user_inputs])\n",
    "dot_score_country1 = layers.Dot(axes=(2),normalize=False)([embeddings[1],layers.concatenate(embeddings[2:-1])])\n",
    "\n",
    "# # Merge embedding and use a Droput to prevent overfittting\n",
    "merged_inputs = layers.concatenate(embeddings)\n",
    "# # spatial_dropout = layers.SpatialDropout1D(0.2)(merged_inputs)\n",
    "# flat_embed = layers.Flatten()(merged_inputs)#(spatial_dropout)\n",
    "\n",
    "# # Merge embedding and numeric features\n",
    "# all_features = layers.concatenate([flat_embed, merged_num_inputs])\n",
    "\n",
    "# all_features = layers.concatenate([dot_score, merged_num_inputs])\n",
    "\n",
    "# x = layers.concatenate([dot_score_city1,dot_score_country1]) # for just dot product model\n",
    "x = layers.concatenate([merged_inputs,dot_score_city1,dot_score_country1])\n",
    "\n",
    "# MLP for classification\n",
    "# x = layers.Dropout(0.2)(layers.Dense(128, activation='relu')(all_features))\n",
    "x = layers.Dropout(0.1)(layers.Dense(128, activation='relu')(x))\n",
    "x = layers.Dropout(0.1)(layers.Dense(64, activation='relu')(x))\n",
    "\n",
    "# Final model\n",
    "output = layers.Dense(1, activation='sigmoid',bias_initializer=output_bias)(x)\n",
    "model = models.Model(inputs=cat_inputs + num_inputs, outputs=output)\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss=\"binary_crossentropy\", \n",
    "              metrics=['accuracy',\"AUC\"]) #,\"Precision\"\"Recall\", 'accuracy', ,tf.keras.metrics.TopKCategoricalAccuracy(4) - not used in ranking\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(train_ds,\n",
    "          validation_data=val_ds,\n",
    "          epochs=max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After 4 epochs (1.23k sampled data - only dot product model + embeddings (no FC), 4 epochs\n",
    "    * loss: 0.1380 - accuracy: 0.9503 - auc: 0.9185 - val_loss: 0.1426 - val_accuracy: 0.9492 - val_auc: 0.9104\n",
    "    \n",
    "* Above + sigmoid bias, + 128 elu FC last layer, 20% dropout: \n",
    "    * 12s 3ms/step - loss: 0.1255 - accuracy: 0.9539 - auc: 0.9362 - val_loss: 0.1421 - val_accuracy: 0.9519 - val_auc: 0.9084\n",
    "* Above + sigmoid bias, + 64 + 128 elu FC last layer, 10% dropout:     \n",
    "    * 13s 4ms/step - loss: 0.1150 - accuracy: 0.9586 - auc: 0.9456 - val_loss: 0.1409 - val_accuracy: 0.9540 - val_auc: 0.9078\n",
    "* (directly) above, + relu (instead of elu):\n",
    "    * 14s 4ms/step - loss: 0.1100 - accuracy: 0.9601 - auc: 0.9515 - val_loss: 0.1367 - val_accuracy: 0.9568 - val_auc: 0.9152\n",
    "    \n",
    "* (directly) above,(relu) + normalize Dot product:\n",
    "    * 13s 4ms/step - loss: 0.1060 - accuracy: 0.9605 - auc: 0.9564 - val_loss: 0.1372 - val_accuracy: 0.9564 - val_auc: 0.9119\n",
    "    * Normalizing seems to be the same or overfit by an iota? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation: \n",
    "* don't enable shuffle on the tf_ds, or results will be scrambled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_val_preds_proba = model.predict(val_ds)\n",
    "# print(roc_auc_score(y_true = valid[\"target\"],y_score=y_val_preds_proba[:,0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_preds_proba = model.predict(test_ds)\n",
    "print(round(100*roc_auc_score(y_true = test[\"target\"],y_score=y_test_preds_proba[:,0,0]),2))\n",
    "\n",
    "# classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAT_FEAT_NAMES = [\"booker_country\", \"device_class\",\"affiliate_id\",\n",
    "# #                   \"user_id\", ## ? could use lower dim - depends on train/test overlap\n",
    "#                   \"checkin_week\",\"checkout_week\",\n",
    "#                     \"checkin_weekday\",\n",
    "#     \"lag1_city_id\",\"lag1_booker_country\",\"lag1_hotel_country\",\"lag1_affiliate_id\", \"lag1_device_class\",\n",
    "#      \"lag2_city_id\",\"lag2_booker_country\",\"lag2_hotel_country\",\"lag2_affiliate_id\",\"lag2_device_class\",\n",
    "#       \"lag3_city_id\",\"lag3_booker_country\",\"lag3_hotel_country\",\"lag3_affiliate_id\",\"lag3_device_class\",\n",
    "#                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NUMERIC_COLS = [item for item in list(df_feat.columns.drop(TARGET_COL))  if item not in CAT_FEAT_NAMES]\n",
    "# print(len(NUMERIC_COLS))\n",
    "# print(\"numeric cols\",NUMERIC_COLS)\n",
    "\n",
    "# for c in NUMERIC_COLS:\n",
    "#     l_enc =   StandardScaler() # MinMaxScaler()#\n",
    "#     l_enc.fit(df_feat[c].values.reshape(-1,1))\n",
    "#     X_train[c] = l_enc.transform(X_train[c].values.reshape(-1,1))\n",
    "#     X_test[c] = l_enc.transform(X_test[c].values.reshape(-1,1))\n",
    "    \n",
    "# for c in CAT_FEAT_NAMES:\n",
    "#     l_enc = LabelEncoder().fit(df_feat[c])\n",
    "#     X_train[c] = l_enc.transform(X_train[c])\n",
    "#     X_test[c] = l_enc.transform(X_test[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(\"sum top4 total percentage:\",y_train.value_counts(normalize=True)[0:4].sum().round(3))\n",
    "# y_train.value_counts(normalize=True).round(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature importance & evaluation\n",
    "* Look for leaks!\n",
    "* May be bug with ordering of results - evaluation doesn't make sense. Note that diff # outputs/classes, likely culprit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"y_test nunique classes\",y_test.nunique())\n",
    "# y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
