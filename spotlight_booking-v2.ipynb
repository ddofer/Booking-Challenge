{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* hparams search:\n",
    "\n",
    "https://github.com/maciejkula/spotlight/blob/master/examples/movielens_sequence/movielens_sequence.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install pandas numpy scipy ipython -y\n",
    "# !conda install -c maciejkula -c pytorch spotlight -y\n",
    "# !pip install spotlight\n",
    "\n",
    "# !conda install pytorch torchvision  cudatoolkit=10.2 -c pytorch --force-reinstall -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append(\"../../\")\n",
    "import os\n",
    "# import papermill as pm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from reco_utils.evaluation.python_evaluation import map_at_k, ndcg_at_k, precision_at_k, recall_at_k, get_top_k_items, auc\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit, GridSearchCV, GroupKFold\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from spotlight.cross_validation import random_train_test_split, user_based_train_test_split\n",
    "from spotlight.datasets.movielens import get_movielens_dataset\n",
    "from spotlight.evaluation import mrr_score\n",
    "from spotlight.factorization.implicit import ImplicitFactorizationModel\n",
    "from spotlight.cross_validation import user_based_train_test_split\n",
    "from spotlight.datasets.synthetic import generate_sequential\n",
    "from spotlight.evaluation import sequence_mrr_score\n",
    "from spotlight.sequence.implicit import ImplicitSequenceModel\n",
    "from spotlight.interactions import Interactions\n",
    "import torch\n",
    "from spotlight.evaluation import sequence_mrr_score,precision_recall_score\n",
    "# from spotlight.evaluation import sequence_precision_recall_score # https://maciejkula.github.io/spotlight/evaluation.html#spotlight.evaluation.sequence_precision_recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spotlight_hyperparams_movielens_sequence import *\n",
    "\n",
    "# NUM_SAMPLES = 50\n",
    "\n",
    "# LEARNING_RATES = [1e-3, 1e-2, 5 * 1e-2, 1e-1]\n",
    "# LOSSES = ['bpr', 'hinge', 'adaptive_hinge', 'pointwise']\n",
    "# BATCH_SIZE = [16,64, 128, 512]\n",
    "# EMBEDDING_DIM = [32, 64, 128, 256]\n",
    "# # N_ITER = list(range(5, 20))\n",
    "# N_ITER = [10,25]\n",
    "# L2 =[1e-6, 1e-4,  5e-3,1e-2,1e-1, 0.0]  ### [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.0] \n",
    "\n",
    "random_state = np.random.RandomState(100)\n",
    "\n",
    "# CUDA = (os.environ.get('CUDA') is not None or\n",
    "#         shutil.which('nvidia-smi') is not None)\n",
    "# print(CUDA)\n",
    "CUDA = False ## pytorch issues\n",
    "\n",
    "max_sequence_length = 12\n",
    "min_sequence_length = 3\n",
    "random_state = np.random.RandomState(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy_at_4(submission,ground_truth):\n",
    "    '''checks if the true city is within the four recommended cities'''\n",
    "    data_to_eval = submission.join(ground_truth,on='utrip_id')\n",
    "    hits = data_to_eval.apply(\n",
    "        lambda row: row['city_id'] in (row[['city_id_1', 'city_id_2', 'city_id_3', 'city_id_4']].values),\n",
    "            axis = 1)\n",
    "    return hits.mean()    \n",
    "\n",
    "\n",
    "def evaluate_lstm_model(hyperparameters, train, test,random_state):\n",
    "\n",
    "    h = hyperparameters\n",
    "\n",
    "    model = ImplicitSequenceModel(loss=h['loss'],\n",
    "                                  representation='lstm',\n",
    "                                  batch_size=h['batch_size'],\n",
    "                                  learning_rate=h['learning_rate'],\n",
    "                                  l2=h['l2'],\n",
    "                                  n_iter=h['n_iter'],\n",
    "                                  use_cuda=CUDA,\n",
    "                                  random_state=random_state)\n",
    "\n",
    "    model.fit(train, verbose=True)\n",
    "\n",
    "    test_mrr = sequence_mrr_score(model, test)\n",
    "    print(\"test mrr\",round(test_mrr,4))\n",
    "    #val_mrr = sequence_mrr_score(model, validation)\n",
    "    test_mrr = sequence_precision_recall_score(model, test, k=1, exclude_preceding=False )[0]\n",
    "    # test_rec_prec = sequence_precision_recall_score(model, test, k=1, exclude_preceding=True )[0]\n",
    "    # val_rec_prec = sequence_precision_recall_score(model, validation, k=4, exclude_preceding=True )[0]\n",
    "\n",
    "    return test_mrr, model  \n",
    "\n",
    "def evaluate_pooling_model(hyperparameters, train, test, random_state):\n",
    "\n",
    "    h = hyperparameters\n",
    "\n",
    "    model = ImplicitSequenceModel(loss=h['loss'],\n",
    "                                  representation='pooling',\n",
    "                                  batch_size=h['batch_size'],\n",
    "                                  learning_rate=h['learning_rate'],\n",
    "                                  l2=h['l2'],\n",
    "                                  n_iter=h['n_iter'],\n",
    "                                  use_cuda=CUDA,\n",
    "                                  random_state=random_state)\n",
    "\n",
    "    model.fit(train, verbose=True)\n",
    "\n",
    "    test_mrr = sequence_mrr_score(model, test)\n",
    "    #val_mrr = sequence_mrr_score(model, validation)\n",
    "    #test_rec_prec = sequence_precision_recall_score(model, test)[0]\n",
    "    #val_rec_prec = sequence_precision_recall_score(model, validation)[0]\n",
    "    \n",
    "    return test_mrr, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8.0'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_precision_recall_score(model, test, k=1, exclude_preceding=False):\n",
    "    \"\"\"\n",
    "    https://maciejkula.github.io/spotlight/_modules/spotlight/evaluation.html#sequence_precision_recall_score\n",
    "    \n",
    "    Compute sequence precision and recall scores. Each sequence\n",
    "    in test is split into two parts: the first part, containing\n",
    "    all but the last k elements, is used to predict the last k\n",
    "    elements.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    model: fitted instance of a recommender model\n",
    "        The model to evaluate.\n",
    "    test: :class:`spotlight.interactions.SequenceInteractions`\n",
    "        Test interactions.\n",
    "    exclude_preceding: boolean, optional\n",
    "        When true, items already present in the sequence will\n",
    "        be excluded from evaluation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    mrr scores: numpy array of shape (num_users,)\n",
    "        Array of MRR scores for each sequence in test.\n",
    "    \"\"\"\n",
    "    sequences = test.sequences[:, :-k]\n",
    "    targets = test.sequences[:, -k:]\n",
    "    precision_recalls = []\n",
    "    for i in range(len(sequences)):\n",
    "        predictions = -model.predict(sequences[i])\n",
    "        if exclude_preceding:\n",
    "            predictions[sequences[i]] = FLOAT_MAX\n",
    "\n",
    "        predictions = predictions.argsort()[:k]\n",
    "        precision_recall = _get_precision_recall(predictions, targets[i], k)\n",
    "        precision_recalls.append(precision_recall)\n",
    "\n",
    "    precision = np.array(precision_recalls)[:, 0]\n",
    "    recall = np.array(precision_recalls)[:, 1]\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def _get_precision_recall(predictions, targets, k=4):\n",
    "\n",
    "    predictions = predictions[:k]\n",
    "    num_hit = len(set(predictions).intersection(set(targets)))\n",
    "\n",
    "    return float(num_hit) / len(predictions), float(num_hit) / len(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top k items to recommend\n",
    "TOP_K = 4\n",
    "\n",
    "# # Model parameters\n",
    "# EPOCHS = 10\n",
    "# BATCH_SIZE =  256#1024\n",
    "\n",
    "# SEED = 0  # Set None for non-deterministic results\n",
    "# random_state = np.random.RandomState(100)\n",
    "# user_file = \"../../tests/resources/deeprec/lightgcn/user_embeddings.csv\"\n",
    "# item_file = \"../../tests/resources/deeprec/lightgcn/item_embeddings.csv\"\n",
    "\n",
    "TRAIN_FILE_PATH = os.path.normpath(r\"C:\\Users\\Dan Ofer\\Desktop\\Stuff\\booking_wisdom\\booking_train_set.csv\") #\"/content/drive/MyDrive/booking_wisdom/booking_train_set.csv\" #\"booking_train_set.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spotlight relevant examples:\n",
    "\n",
    "https://github.com/maciejkula/spotlight/blob/master/examples/movielens_sequence/movielens_sequence.py\n",
    "\n",
    "https://github.com/maciejkula/spotlight/issues/172\n",
    "* Dataset creation/loading + get predictions (sequence)\n",
    "\n",
    "Spotlight hyperparameter tuning:\n",
    "https://github.com/maciejkula/spotlight/blob/master/examples/movielens_sequence/movielens_sequence.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = movielens.load_pandas_df(size=MOVIELENS_DATA_SIZE) ## orig \n",
    "# print(df.dtypes)\n",
    "# print(df.shape)\n",
    "# ## expected columns: ['userID', 'itemID', 'rating', 'timestamp'] (last as float), others as number\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max rows count    1.164595e+06\n",
      "mean     6.123632e+00\n",
      "std      2.795051e+00\n",
      "min      4.000000e+00\n",
      "25%      4.000000e+00\n",
      "50%      5.000000e+00\n",
      "75%      7.000000e+00\n",
      "max      4.800000e+01\n",
      "Name: total_rows, dtype: float64\n",
      "nunique\n",
      " userID       195341\n",
      "itemID        39560\n",
      "timestamp       425\n",
      "last              2\n",
      "dtype: int64\n",
      "(1042324, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>last</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>542546</th>\n",
       "      <td>0</td>\n",
       "      <td>8183</td>\n",
       "      <td>2016-08-13</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542547</th>\n",
       "      <td>0</td>\n",
       "      <td>15626</td>\n",
       "      <td>2016-08-14</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542548</th>\n",
       "      <td>0</td>\n",
       "      <td>60902</td>\n",
       "      <td>2016-08-16</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542549</th>\n",
       "      <td>0</td>\n",
       "      <td>30628</td>\n",
       "      <td>2016-08-18</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061281</th>\n",
       "      <td>1</td>\n",
       "      <td>38677</td>\n",
       "      <td>2016-04-09</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         userID  itemID  timestamp   last\n",
       "542546        0    8183 2016-08-13  False\n",
       "542547        0   15626 2016-08-14  False\n",
       "542548        0   60902 2016-08-16  False\n",
       "542549        0   30628 2016-08-18   True\n",
       "1061281       1   38677 2016-04-09  False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = movielens.load_pandas_df(size=MOVIELENS_DATA_SIZE) ## orig \n",
    "df = pd.read_csv(TRAIN_FILE_PATH,\n",
    "#                      nrows=230_500,\n",
    "                     parse_dates=[\"checkin\"],infer_datetime_format=True,usecols=[\"utrip_id\",\"city_id\",\"checkin\"])\n",
    "\n",
    "user_encoder = LabelEncoder()\n",
    "\n",
    "## drop consecutive elements (need to do within group, and to order, and what about updating checkout date?)\n",
    "## 0 cases if we also use checkin or checkout. \n",
    "## The \"last\"  criteria does drop rows! Do we want or not want it???\n",
    "## We also +- need to merge with total duration data\n",
    "\n",
    "df[\"utrip_id\"] = user_encoder.fit_transform(df[\"utrip_id\"])\n",
    "df = df.groupby(\"utrip_id\").filter(lambda x: len(x) >= 4) # keep only trips with at least 4\n",
    "\n",
    "#####\n",
    "df[\"row_num\"] = df.groupby(\"utrip_id\")[\"checkin\"].rank(ascending=True,pct=False).astype(int)\n",
    "utrip_counts = df[\"utrip_id\"].value_counts()\n",
    "df[\"total_rows\"] = df[\"utrip_id\"].map(utrip_counts)\n",
    "print(\"max rows\", df[\"total_rows\"].describe())\n",
    "df[\"last\"] = (df[\"row_num\"] ==df[\"total_rows\"])#.astype(int)\n",
    "###\n",
    "df = df.loc[(df[[\"city_id\",\"utrip_id\",\"last\"]].shift() != df[[\"city_id\",\"utrip_id\",\"last\"]]).max(axis=1)]\n",
    "\n",
    "# ##########\n",
    "# ### temp, drop longer trips, for easier evaluation rule of thumb\n",
    "# print(df.shape[0])\n",
    "# df = df.loc[df[\"total_rows\"]<=max_sequence_length+1]\n",
    "# print(df.shape[0])\n",
    "##########\n",
    "df = df.groupby(\"utrip_id\").filter(lambda x: len(x) >= 4) # keep only trips with at least 4\n",
    "\n",
    "\n",
    "df = df[[\"utrip_id\",\"city_id\",\"checkin\",\"last\"]]\n",
    "              \n",
    "df.columns = ['userID', 'itemID',  'timestamp',\"last\"] ## no rating col\n",
    "\n",
    "df.sort_values([\"userID\",\"timestamp\"],ascending=True,inplace=True) ## reinforce sort order for within groups\n",
    "\n",
    "df = df[['userID', 'itemID',  'timestamp',\"last\"]]\n",
    "print(\"nunique\\n\",df.nunique())\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique items (city_id)\n",
      "39560\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    39560.000000\n",
       "mean        26.347927\n",
       "std        184.627494\n",
       "min          1.000000\n",
       "25%          1.000000\n",
       "50%          3.000000\n",
       "75%          9.000000\n",
       "max       9060.000000\n",
       "Name: itemID, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"unique items (city_id)\")\n",
    "print(df['itemID'].nunique())\n",
    "df['itemID'].value_counts().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before uniques 39560\n",
      "max 21218\n",
      "unique items (city_id)\n",
      "21218\n",
      "count    21218.000000\n",
      "mean        49.124517\n",
      "std        301.280348\n",
      "min          3.000000\n",
      "25%          4.000000\n",
      "50%          8.000000\n",
      "75%         20.000000\n",
      "max      24519.000000\n",
      "Name: itemID, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ### replace rare variables (under 2 occurrences) with \"-1\" dummy OR with their hotel country! (as negative mapped number)\n",
    "city_ids_counts = df[\"itemID\"].value_counts()\n",
    "print(\"before uniques\",df[\"itemID\"].nunique())\n",
    "city_ids_counts = city_ids_counts.to_dict()\n",
    "df[\"itemID\"] = df[\"itemID\"].where(df[\"itemID\"].map(city_ids_counts)>2, -1)\n",
    "\n",
    "## run encoder on cities (due to negative and to ensure ok ordering for spotlight)\n",
    "le_itm = LabelEncoder()\n",
    "df[\"itemID\"] = le_itm.fit_transform(df[\"itemID\"])\n",
    "\n",
    "df[\"itemID\"] = df[\"itemID\"]+1 ## add 1 due to 0 being reserved for 0 padding\n",
    "print(\"max\",df['itemID'].max())\n",
    "print(\"unique items (city_id)\")\n",
    "print(df['itemID'].nunique())\n",
    "print(df['itemID'].value_counts().describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inds, test_inds = next(GroupShuffleSplit(test_size=.25, n_splits=2,random_state = 0).split(df,groups=df['userID']))\n",
    "\n",
    "train= df.iloc[train_inds]\n",
    "test = df.iloc[test_inds]\n",
    "\n",
    "# train = pd.concat([df.iloc[train_inds],\n",
    "#                   X_test.loc[X_test[\"last\"]==False]]).drop([\"last\"],axis=1) # ORIG\n",
    "train = df.copy() # new dan = \"train + test\"\n",
    "\n",
    "num_users_all=int(df['userID'].max()+1)\n",
    "num_items_all=int(df['itemID'].max()+1)\n",
    "\n",
    "last_city = test.loc[test[\"last\"]==True][\"itemID\"] ##pd.concat([X_test.loc[X_test[\"last\"]==True]]).drop([\"last\"],axis=1)\n",
    "\n",
    "test_eval = test.loc[test[\"last\"]!=True] ## test without last row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### spotlight drops sequences that are too long instead of truncating them ??? \n",
    "## So we truncate ourselves! \n",
    "\n",
    "### Possible improvement: keep head (1) + tail  (i.e leaves info on first place visited\n",
    "\n",
    "train = train.groupby(\"userID\").tail(max_sequence_length)\n",
    "# test = test.groupby(\"userID\").tail(max_sequence_length)\n",
    "test_eval= test_eval.groupby(\"userID\").tail(max_sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert test_eval[\"userID\"].nunique() == test[\"userID\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create spotlight Interactions dataset\n",
    "    * https://github.com/maciejkula/spotlight/issues/172\n",
    "    * Could provide WEIGHTS (more weight to final row in sequence / last) \n",
    "    \n",
    "    * Getting predictions: \n",
    "    ```\n",
    "    predictions = model.predict(ids)\n",
    "\n",
    "    item_ids= (-predictions).argsort()[:10] # last 10 items\n",
    "    print(item_ids)\n",
    "    print(predictions[item_ids])\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max user 217685\n",
      "max itemID 21218\n"
     ]
    }
   ],
   "source": [
    "print(\"max user\",train['userID'].max())\n",
    "print(\"max itemID\",train['itemID'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Interactions dataset (217686 users x 21219 items x 1035934 interactions)>\n",
      "<Interactions dataset (217686 users x 21219 items x 260975 interactions)>\n",
      "<Interactions dataset (217686 users x 21219 items x 210953 interactions)>\n"
     ]
    }
   ],
   "source": [
    "train = Interactions(user_ids=train['userID'].values,\n",
    "                           item_ids=train['itemID'].values,\n",
    "                           timestamps=train['timestamp'].values\n",
    "                         ,num_users=num_users_all,num_items=num_items_all)\n",
    "print(train)\n",
    "test = Interactions(user_ids=test['userID'].values,\n",
    "                           item_ids=test['itemID'].values,\n",
    "                           timestamps=test['timestamp'].values\n",
    "                    ,num_users=num_users_all,num_items=num_items_all)\n",
    "print(test)\n",
    "\n",
    "test_eval = Interactions(user_ids=test_eval['userID'].values,\n",
    "                           item_ids=test_eval['itemID'].values,\n",
    "                           timestamps=test_eval['timestamp'].values\n",
    "                         ,num_users=num_users_all,num_items=num_items_all)\n",
    "\n",
    "\n",
    "print(test_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_sequential <Sequence interactions dataset (195341 sequences x 12 sequence length)>\n",
      "test_sequential <Sequence interactions dataset (49208 sequences x 12 sequence length)>\n",
      "test_eval_sequential <Sequence interactions dataset (48836 sequences x 12 sequence length)>\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "train_sequential = train.to_sequence(max_sequence_length, min_sequence_length=3)\n",
    "print(\"train_sequential\",train_sequential)\n",
    "test_sequential = test.to_sequence(max_sequence_length, min_sequence_length=2)\n",
    "print(\"test_sequential\",test_sequential)\n",
    "\n",
    "test_eval_sequential = test_eval.to_sequence(max_sequence_length, min_sequence_length=3)\n",
    "print(\"test_eval_sequential\",test_eval_sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = {'n_iter': 18, 'loss': 'hinge', 'learning_rate': 0.01, 'l2': 0.0, 'embedding_dim': 128, 'batch_size': 64}\n",
    "# Best lstm result: {'test_mrr': 0.2304, 'validation_mrr': 0.256,\n",
    "# hp_lstm = {'n_iter': 22, 'loss': 'adaptive_hinge', 'learning_rate': 0.008, 'l2': 0.0000001, 'embedding_dim': 128, 'batch_size': 128}\n",
    "\n",
    "hp_lstm =  {'n_iter': 40, 'loss': 'adaptive_hinge', 'learning_rate': 0.002, 'l2': 1e-09, 'embedding_dim': 128, 'batch_size': 128}\n",
    "#  in hparam search  : Test MRR 0.2826 val MRR 0.2798 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss 0.4961106244769843\n",
      "Epoch 1: loss 0.33012199064212916\n",
      "Epoch 2: loss 0.22770608632722242\n",
      "Epoch 3: loss 0.17974120025238183\n",
      "Epoch 4: loss 0.15748407724895896\n",
      "Epoch 5: loss 0.14457095095298614\n",
      "Epoch 6: loss 0.13647255513234005\n",
      "Epoch 7: loss 0.13086477649528058\n",
      "Epoch 8: loss 0.12750605344479585\n",
      "Epoch 9: loss 0.1236967424317443\n",
      "Epoch 10: loss 0.12174199121932852\n",
      "Epoch 11: loss 0.12013898980172538\n",
      "Epoch 12: loss 0.11789007339023966\n",
      "Epoch 13: loss 0.1165304221578806\n",
      "Epoch 14: loss 0.11529011144189953\n",
      "Epoch 15: loss 0.11383789671876497\n",
      "Epoch 16: loss 0.11306551845345375\n",
      "Epoch 17: loss 0.11249355958151365\n",
      "Epoch 18: loss 0.11152296460623357\n",
      "Epoch 19: loss 0.11066758837343511\n",
      "Epoch 20: loss 0.10978671324335053\n",
      "Epoch 21: loss 0.10913099765231114\n",
      "Epoch 22: loss 0.10804773529487727\n",
      "Epoch 23: loss 0.10804745734967511\n",
      "Epoch 24: loss 0.10748752131911767\n",
      "Epoch 25: loss 0.10699583461706984\n",
      "Epoch 26: loss 0.10653344118515104\n",
      "Epoch 27: loss 0.10634881131394908\n",
      "Epoch 28: loss 0.10662320020078597\n",
      "Epoch 29: loss 0.10546379301699456\n",
      "Epoch 30: loss 0.10496348236792995\n",
      "Epoch 31: loss 0.10467267999841484\n",
      "Epoch 32: loss 0.10452803004028052\n",
      "Epoch 33: loss 0.10408895642306966\n",
      "Epoch 34: loss 0.10391999891402288\n",
      "Epoch 35: loss 0.10370281033252061\n",
      "Epoch 36: loss 0.10311332009074027\n",
      "Epoch 37: loss 0.10324403789984922\n",
      "Epoch 38: loss 0.10279558879306837\n",
      "Epoch 39: loss 0.10273740095889639\n",
      "0.2717458582912555\n",
      "Wall time: 50min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_mrr, model = evaluate_lstm_model(hp_lstm, train_sequential, test_sequential,random_state)\n",
    "print(test_mrr.mean())\n",
    "\n",
    "# pr, rec = sequence_precision_recall_score(model, test_sequential, k=1, exclude_preceding=False)\n",
    "# print(\"prec\",pr.mean())\n",
    "# print(\"recall\",rec.mean())\n",
    "\n",
    "# Epoch 21: loss 0.14362004007497642\n",
    "# 0.250 MRR\n",
    "\n",
    "\n",
    "# ### new params, 40 epochs;\n",
    "## hp_lstm =  {'n_iter': 40, 'loss': 'adaptive_hinge', 'learning_rate': 0.002, 'l2': 1e-09, 'embedding_dim': 128, 'batch_size': 128}\n",
    "# Epoch 38: loss 0.10279\n",
    "# Epoch 39: loss 0.10273\n",
    "# 0.2717458582912555\n",
    "# # Wall time: 50min 22s\n",
    "\n",
    "# Epoch 0: loss 0.4961106244769843\n",
    "# Epoch 1: loss 0.33012199064212916\n",
    "# Epoch 2: loss 0.22770608632722242\n",
    "# Epoch 3: loss 0.17974120025238183\n",
    "# Epoch 4: loss 0.15748407724895896\n",
    "# Epoch 5: loss 0.14457095095298614\n",
    "# Epoch 6: loss 0.13647255513234005\n",
    "# Epoch 7: loss 0.13086477649528058\n",
    "# Epoch 8: loss 0.12750605344479585\n",
    "# Epoch 9: loss 0.1236967424317443\n",
    "# Epoch 10: loss 0.12174199121932852\n",
    "# Epoch 11: loss 0.12013898980172538\n",
    "# Epoch 12: loss 0.11789007339023966\n",
    "# Epoch 13: loss 0.1165304221578806\n",
    "# Epoch 14: loss 0.11529011144189953\n",
    "# Epoch 15: loss 0.11383789671876497\n",
    "# Epoch 16: loss 0.11306551845345375\n",
    "# Epoch 17: loss 0.11249355958151365\n",
    "# Epoch 18: loss 0.11152296460623357\n",
    "# Epoch 19: loss 0.11066758837343511\n",
    "# Epoch 20: loss 0.10978671324335053\n",
    "# Epoch 21: loss 0.10913099765231114\n",
    "# Epoch 22: loss 0.10804773529487727\n",
    "# Epoch 23: loss 0.10804745734967511\n",
    "# Epoch 24: loss 0.10748752131911767\n",
    "# Epoch 25: loss 0.10699583461706984\n",
    "# Epoch 26: loss 0.10653344118515104\n",
    "# Epoch 27: loss 0.10634881131394908\n",
    "# Epoch 28: loss 0.10662320020078597\n",
    "# Epoch 29: loss 0.10546379301699456\n",
    "# Epoch 30: loss 0.10496348236792995\n",
    "# Epoch 31: loss 0.10467267999841484\n",
    "# Epoch 32: loss 0.10452803004028052\n",
    "# Epoch 33: loss 0.10408895642306966\n",
    "# Epoch 34: loss 0.10391999891402288\n",
    "# Epoch 35: loss 0.10370281033252061\n",
    "# Epoch 36: loss 0.10311332009074027\n",
    "# Epoch 37: loss 0.10324403789984922\n",
    "# Epoch 38: loss 0.10279558879306837\n",
    "# Epoch 39: loss 0.10273740095889639\n",
    "# 0.2717458582912555\n",
    "# Wall time: 50min 22s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# test_mrr_pool, model_pool = evaluate_pooling_model(hp, train_sequential, test_sequential, random_state)\n",
    "# print(test_mrr_pool.mean())\n",
    "\n",
    "# pr, rec = sequence_precision_recall_score(model_pool, test_sequential, k=1, exclude_preceding=False)\n",
    "# print(\"prec\",pr.mean())\n",
    "# print(\"recall\",rec.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48836\n",
      "       city_id                 city_id_preds\n",
      "0        42503   [6788, 24910, 40250, 52177]\n",
      "1        28319  [30688, 17775, 10060, 18508]\n",
      "2        52815  [52815, 21578, 28051, 18033]\n",
      "3        64824  [12308, 64824, 27624, 51135]\n",
      "4         4932   [4932, 17013, 60222, 50957]\n",
      "...        ...                           ...\n",
      "48831    29770  [46258, 29770, 55196, 41781]\n",
      "48832    52571  [45454, 61320, 52571, 40069]\n",
      "48833     6582  [56775, 12708, 34342, 56184]\n",
      "48834    47700  [55763, 25946, 61320, 20967]\n",
      "48835    20345  [62541, 28829, 44489, 58819]\n",
      "\n",
      "[48836 rows x 2 columns]\n",
      "top k accuracy @ 4\n",
      "0.3612498976165124\n",
      "Wall time: 5min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "preds = []\n",
    "for i in range(len(test_eval_sequential.sequences)): \n",
    "    if (i //50) == int: print (i)\n",
    "    predictions = model.predict(test_eval_sequential.sequences[i])\n",
    "    item_ids= (-predictions).argsort()[:TOP_K+1]  ## k+1 (in case we have a negative/UNK to drop)\n",
    "    item_ids = list(le_itm.inverse_transform(item_ids-1))\n",
    "    ### drop predictions that are \"unknown cities\" (-1), and kee\n",
    "    item_ids = [i for i in item_ids if i>= 0] ## drop negatives/ -1s\n",
    "    item_ids = item_ids[:TOP_K]\n",
    "    \n",
    "    preds.append(item_ids)\n",
    "print(len(preds))\n",
    "\n",
    "df_preds = pd.DataFrame(data={\"city_id\": le_itm.inverse_transform(last_city-1),\n",
    "                             \"city_id_preds\":preds})\n",
    "\n",
    "print(df_preds)\n",
    "\n",
    "print(\"top k accuracy @\",TOP_K)\n",
    "print(df_preds.apply(\n",
    "        lambda row: row['city_id'] in (row[\"city_id_preds\"]),\n",
    "            axis = 1).mean())\n",
    "\n",
    "\n",
    "# top k accuracy @ 4 : 0.361\n",
    "# top k accuracy @ 50 0.779"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec 0.14276353509705955\n",
      "recall 0.14276353509705955\n",
      "Wall time: 5min 15s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# pr, rec = sequence_precision_recall_score(model, test_sequential, k=1, exclude_preceding=False)\n",
    "# print(\"prec\",pr.mean()) ## 0.1427\n",
    "# print(\"recall\",rec.mean()) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48836\n",
      "       city_id                                      city_id_preds\n",
      "0        42503  [6788, 24910, 40250, 52177, 17127, 61187, 1419...\n",
      "1        28319  [30688, 17775, 10060, 18508, 38382, 1745, 4790...\n",
      "2        52815  [52815, 21578, 28051, 18033, 7456, 62399, 6988...\n",
      "3        64824  [12308, 64824, 27624, 51135, 343, 60282, 29562...\n",
      "4         4932  [4932, 17013, 60222, 50957, 38912, 26007, 9680...\n",
      "...        ...                                                ...\n",
      "48831    29770  [46258, 29770, 55196, 41781, 53363, 2201, 3760...\n",
      "48832    52571  [45454, 61320, 52571, 40069, 27558, 834, 22696...\n",
      "48833     6582  [56775, 12708, 34342, 56184, 6582, 1528, 53793...\n",
      "48834    47700  [55763, 25946, 61320, 20967, 36063, 18850, 969...\n",
      "48835    20345  [62541, 28829, 44489, 58819, 22490, 42482, 203...\n",
      "\n",
      "[48836 rows x 2 columns]\n",
      "top k accuracy @ 4\n",
      "0.7797731181914981\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "for i in range(len(test_eval_sequential.sequences)): \n",
    "    predictions = model.predict(test_eval_sequential.sequences[i])\n",
    "    item_ids= (-predictions).argsort()[:50+1]  ## k+1 (in case we have a negative/UNK to drop)\n",
    "    item_ids = list(le_itm.inverse_transform(item_ids-1))\n",
    "    ### drop predictions that are \"unknown cities\" (-1), and kee\n",
    "    item_ids = [i for i in item_ids if i>= 0] ## drop negatives/ -1s\n",
    "    item_ids = item_ids[:50]\n",
    "    \n",
    "    preds.append(item_ids)\n",
    "print(len(preds))\n",
    "\n",
    "df_preds = pd.DataFrame(data={\"city_id\": le_itm.inverse_transform(last_city-1),\n",
    "                             \"city_id_preds\":preds})\n",
    "\n",
    "print(df_preds)\n",
    "\n",
    "print(\"top k accuracy 50\")\n",
    "print(df_preds.apply(\n",
    "        lambda row: row['city_id'] in (row[\"city_id_preds\"]),\n",
    "            axis = 1).mean())\n",
    "## @50: 0.779"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "#### pool model : top k accuracy @ 4 :  0.246   (on 20% data)\n",
    "# preds = []\n",
    "# for i in range(len(test_eval_sequential.sequences)): \n",
    "#     predictions = model_pool.predict(test_eval_sequential.sequences[i])\n",
    "#     item_ids= (-predictions).argsort()[:TOP_K+1]  ## k+1 (in case we have a negative/UNK to drop)\n",
    "#     item_ids = list(le_itm.inverse_transform(item_ids-1))\n",
    "#     ### drop predictions that are \"unknown cities\" (-1), and kee\n",
    "#     item_ids = [i for i in item_ids if i>= 0] ## drop negatives/ -1s\n",
    "#     item_ids = item_ids[:TOP_K]\n",
    "    \n",
    "#     preds.append(item_ids)\n",
    "# print(len(preds))\n",
    "\n",
    "# df_preds = pd.DataFrame(data={\"city_id\": le_itm.inverse_transform(last_city-1),\n",
    "#                              \"city_id_preds\":preds})\n",
    "\n",
    "# print(df_preds)\n",
    "\n",
    "# print(\"top k accuracy @\",TOP_K)\n",
    "# print(df_preds.apply(\n",
    "#         lambda row: row['city_id'] in (row[\"city_id_preds\"]),\n",
    "#             axis = 1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# run(train, test, validation, random_state, \"lstm\")\n",
    "\n",
    "# Best lstm result: {'test_mrr': 0.2304, 'validation_mrr': 0.256,\n",
    "# 'n_iter': 5, 'loss': 'adaptive_hinge', 'learning_rate': 0.05, 'l2': 0.0001, 'embedding_dim': 64, 'batch_size': 512}\n",
    "\n",
    "\n",
    "# Evaluating {'n_iter': 20, 'loss': 'adaptive_hinge', 'learning_rate': 0.01, 'l2': 0.0001, 'embedding_dim': 128, 'batch_size': 128}\n",
    "# Test MRR 0.269 val MRR 0.2738"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# run(train, test, validation, random_state, model_type=\"pooling\")\n",
    "\n",
    "# # Best pooling result: {'test_mrr': 0.28493, 'validation_mrr': 0.222,\n",
    "# # 'n_iter': 15, 'loss': 'hinge', 'learning_rate': 0.01, 'l2': 0.0, 'embedding_dim': 64, 'batch_size': 16}\n",
    "\n",
    "# Best pooling result: {'test_mrr': 0.2849, 'validation_mrr': 0.22,\n",
    "#                       'n_iter': 15, 'loss': 'hinge', 'learning_rate': 0.01, 'l2': 0.0, 'embedding_dim': 64, 'batch_size': 16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision_recall_score(implicit_sequence_model, test, k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## https://maciejkula.github.io/spotlight/evaluation.html#spotlight.evaluation.sequence_precision_recall_score\n",
    "# ## listed in spotlight docs, not found on import ? \n",
    "# # Compute sequence precision and recall scores. \n",
    "# # Each sequence in test is split into two parts: the first part, containing all but the last k elements, is used to predict the last k elements.\n",
    "\n",
    "\n",
    "# ### returns score per user\n",
    "# seq_pr_score_array = sequence_precision_recall_score(implicit_sequence_model, test_sequential, k=1, exclude_preceding=False) \n",
    "\n",
    "# print(\"pr_score_array[0].mean\",seq_pr_score_array[0].mean())\n",
    "# print(\"pr_score_array[1].mean\",seq_pr_score_array[1].mean()) ## the 2 arrays appear identical  ? \n",
    "# seq_pr_score_array"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
