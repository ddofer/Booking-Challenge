{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collab filtering/MF model V1\n",
    "\n",
    "* try intermediate-simple collaborative filtering/MF/implicit recc embedding model\n",
    "* create new \"id\" - semi distinct tuple of distinguishing variables\n",
    "    * Could consider adding previous city/country visited to tuple - would reduce data but greatly improve this model. For that to make sense, we would also need to do a shared embedding so that city id lag1 and city id will have the same embedding weights learned. \n",
    "    * Could also add discretized number of locations in a trip `total_rows` (1-3,4-5,6+?) as feature , +- step/stage in a trip - https://pbpython.com/natural-breaks.html\n",
    "* Start with stupid approach: ~9k most popular items. highly unbalanced. Dot product .\n",
    "    * Alt simple baseline - SVD (but that loses on embeddings per variable. Instead we'd use just the id)\n",
    "    \n",
    "    \n",
    "    \n",
    " - Possible negatives sampling code : \n",
    "     * https://curiousily.com/posts/build-a-recommender-system-using-keras-and-tensorflow2-in-python/\n",
    "     * tf.datasets - https://stackoverflow.com/questions/58520594/tf-data-dataset-on-each-epoch-only-train-with-a-sub-sample-of-the-full-datase\n",
    "     \n",
    "     \n",
    "     \n",
    " * ALT - get all combinations (itertools) - https://stackoverflow.com/questions/43800390/how-to-create-all-combinations-column-wise-for-multiple-variables-in-pandas\n",
    " \n",
    " * Get all combinations: `pd.crosstab(df[\"city_id\"],df[\"ID\"]).stack().reset_index()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://www.bookingchallenge.com/\n",
    "\n",
    "* Predict `city_id`\n",
    "        * Metric: P@4\n",
    "\n",
    "##### Dataset\n",
    "The training dataset consists of over a million of anonymized hotel reservations, based on real data, with the following features:\n",
    "*    user_id - User ID\n",
    "*    check-in - Reservation check-in date\n",
    "*    checkout - Reservation check-out date\n",
    "*    affiliate_id - An anonymized ID of affiliate channels where the booker came from (e.g. direct, some third party referrals, paid search engine, etc.)\n",
    "*    device_class - desktop/mobile\n",
    "*    booker_country - Country from which the reservation was made (anonymized)\n",
    "*    hotel_country - Country of the hotel (anonymized)\n",
    "*    city_id - city_id of the hotel’s city (anonymized)\n",
    "*    utrip_id - Unique identification of user’s trip (a group of multi-destinations bookings within the same trip)\n",
    "\n",
    "\n",
    "* Each reservation is a part of a customer’s trip (identified by utrip_id) which includes at least 4 consecutive reservations. The check-out date of a reservation is the check-in date of the following reservation in their trip.\n",
    "\n",
    "* The evaluation dataset is constructed similarly, however the city_id of the final reservation of each trip is concealed and requires a prediction.\n",
    "\n",
    " \n",
    "###### Evaluation criteria\n",
    "The goal of the challenge is to predict (and recommend) the final city (city_id) of each trip (utrip_id). We will evaluate the quality of the predictions based on the top four recommended cities for each trip by using Precision@4 metric (4 representing the four suggestion slots at Booking.com website). When the true city is one of the top 4 suggestions (regardless of the order), it is considered correct.\n",
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "* If we are given  the country in question, then this problem is maybe more of a _learning to rank_ problem. (Rather than massively multiclass). \n",
    "    * CatBoost learning to rank on ms dataset (0/1):  https://colab.research.google.com/github/catboost/tutorials/blob/master/ranking/ranking_tutorial.ipynb\n",
    "        * https://catboost.ai/docs/concepts/loss-functions-ranking.html\n",
    "        * for CB ranking,  all objects in dataset must be grouped by group_id - this would be user/trip id X country, in our case. (Still need to add negatives, within each such subgroup/group/\"query\"). \n",
    "\n",
    "    * lightFM - ranking (implicit interactions)\n",
    "        * https://github.com/qqwjq/lightFM\n",
    "\n",
    "    * lstm/w2v - next item recomendation\n",
    "    * dot product between different factors as features (recc.)\n",
    "    * xgboost ap - https://www.kaggle.com/anokas/xgboost-2\n",
    "* Relevant: Kaggle expedia hotel prediction: https://www.kaggle.com/c/expedia-hotel-recommendations/discussion  \n",
    "\n",
    "* ALSO: `implicit interaction` - reccommendation problem (We have only positive feedback, no ranked/negative explicit feedback)'\n",
    "\n",
    "\n",
    "* __BASELINE__ to beat: 4 most popular by country ; 4 most popular by affiliate_id X booker_country X hotel_country (X month?)\n",
    "    * Ignore/auto answer the 4 most popular for countries with less than 4 unique cities in data\n",
    " \n",
    " \n",
    "* Likely approach : build a model (and targets/negatives) per country.\n",
    "\n",
    "-----------\n",
    "#### Data notes:\n",
    "* Long tail of cities and countries\n",
    "* Some (31%) countries have 4 or less unique cities - for those return fixed answer/prediction ?  -\n",
    "    * CAN'T! In test set, we will not have the country ID :(\n",
    "    \n",
    "    \n",
    "----------------------\n",
    "MF - embedding model\n",
    "\n",
    "* https://blog.tensorflow.org/2020/09/introducing-tensorflow-recommenders.html\n",
    "* Implicit recommendations - needs negs\n",
    "    * example of explicit (simple): https://petamind.com/build-a-simple-recommender-system-with-matrix-factorization/\n",
    "* sample negatives - how ? TFRS requires tf.dataset overhead (And confuses me with what user id should be )\n",
    "    * https://www.kaggle.com/skihikingkevin/some-recommender-system-implementations\n",
    "    \n",
    "    \n",
    "Simple keras example of multiple inputs : \n",
    "* keras topologies\n",
    "* https://stackoverflow.com/questions/61722973/why-keras-embedding-not-learning-for-recommendation-system\n",
    "\n",
    "\n",
    "*Tensorflow ranking (seems in beta) : https://colab.research.google.com/github/tensorflow/ranking/blob/master/tensorflow_ranking/examples/handling_sparse_features.ipynb#scrollTo=HfDMGnZY9eVO\n",
    "\n",
    "\n",
    "Negative pairs training with generator - https://towardsdatascience.com/building-a-recommendation-system-using-neural-network-embeddings-1ef92e5c80c9\n",
    "* See code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* example for implicit recomender (naive) - mainly for negatives data gen ?\n",
    "* https://www.kaggle.com/skihikingkevin/some-recommender-system-implementations\n",
    "\n",
    "\n",
    "* Could use **lightFM** - implicit recommender? \n",
    "    * https://github.com/lyst/lightfm/tree/master/examples/dataset\n",
    "    * https://github.com/lyst/lightfm/blob/master/examples/stackexchange/hybrid_crossvalidated.ipynb\n",
    "    * Note use of sparse matrices. Supports metadata\n",
    "* We could use tuple of features for \"user id\" for purposes of recommenders? \n",
    "  \n",
    "* user-item sparse OHE creation - https://github.com/piyushpathak03/Recommendation-systems/blob/master/Recomendation%20system%20end%20to%20end/4)%20Feature%20Creation.ipynb\n",
    "* lightfm -\n",
    "    * https://github.com/piyushpathak03/Recommendation-systems/tree/master/Recomendation%20system%20end%20to%20end  - building the sparse interactions matrix for implecit recc\n",
    "    * https://making.lyst.com/lightfm/docs/examples/dataset.html#building-the-interactions-matrix\n",
    "* https://making.lyst.com/lightfm/docs/examples/hybrid_crossvalidated.html - example of metadata features for lightfm\n",
    "  \n",
    "* https://github.com/zhangruiskyline/DeepLearning/blob/master/doc/Recommendation.md#ranking  - includes negatives sampling! \n",
    "\n",
    "**Spotlight**\n",
    "*  https://maciejkula.github.io/spotlight/interactions.html\n",
    "    * Also has sequence support easily\n",
    "    * Example of loading custom dataset for implicit recc - https://github.com/maciejkula/spotlight/issues/30\n",
    "\n",
    "**SVD/ALS**\n",
    "    * https://stats.stackexchange.com/questions/354355/what-is-the-relation-between-svd-and-als\n",
    "    * https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.svds.html\n",
    "    \n",
    "    \n",
    "RankFM (implicit package) - don't know if adds anything vs lightfm?\n",
    "* https://github.com/etlundquist/rankfm\n",
    "* Does seem easier to \"productionize\"\n",
    "\n",
    "```\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from scipy import sparse\n",
    "\n",
    "def matrix_decomposition(matrix, k, i):\n",
    "    matrix = sparse.csr_matrix(matrix.T)\n",
    "    model = AlternatingLeastSquares(factors=k, iterations=i)\n",
    "    model.fit(matrix)\n",
    "    user_latent = model.user_factors\n",
    "    item_latent = model.item_factors\n",
    "\n",
    "    return user_latent, item_latent\n",
    "```\n",
    "\n",
    "Neural Collaborative Filtering\n",
    "* https://calvinfeng.gitbook.io/machine-learning-notebook/supervised-learning/recommender/neural_collaborative_filtering\n",
    "     * Using the model from here: https://nanx.me/blog/post/recsys-binary-implicit-feedback-r-keras/ and https://github.com/hexiangnan/neural_collaborative_filtering/blob/master/GMF.py\n",
    "\n",
    "```\n",
    "\\# Create the Training Set\n",
    "APPROX_NEGATIVE_SAMPLE_SIZE = int(len(train)*1.2)\n",
    "n_users = c_user.categories.shape[0]\n",
    "n_tracks = c_track.categories.shape[0]\n",
    "\\# Create Training Set\n",
    "train_users = train['username'].cat.codes.values\n",
    "train_tracks = train['track_id'].cat.codes.values\n",
    "train_labels = np.ones(len(train_users))\n",
    "\\# insert negative samples\n",
    "u = np.random.randint(n_users, size=APPROX_NEGATIVE_SAMPLE_SIZE)\n",
    "i = np.random.randint(n_tracks, size=APPROX_NEGATIVE_SAMPLE_SIZE)\n",
    "non_neg_idx = np.where(train_data[u,i] == 0)\n",
    "train_users = np.concatenate([train_users, u[non_neg_idx[1]]])\n",
    "train_tracks = np.concatenate([train_tracks, i[non_neg_idx[1]]])\n",
    "train_labels = np.concatenate([train_labels, np.zeros(u[non_neg_idx[1]].shape[0])])\n",
    "print((train_users.shape, train_tracks.shape, train_labels.shape))\n",
    "\n",
    "\\# random shuffle the data (because Keras takes last 10% as validation split)\n",
    "X = np.stack([train_users, train_tracks, train_labels], axis=1)\n",
    "np.random.shuffle(X)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "* https://vitobellini.github.io/posts/2018/01/03/how-to-build-a-recommender-system-in-tensorflow.html  - easily turn df into matrix (need to add \"as_sparse) - autoencoder approach: \n",
    "    ```\n",
    "    # Convert DataFrame in user-item matrix\n",
    "    matrix = df.pivot(index='user', columns='item', values='rating')\n",
    "    matrix.fillna(0, inplace=True)\n",
    "    ...\n",
    "    # Users and items ordered as they are in matrix\n",
    "\n",
    "    users = matrix.index.tolist()\n",
    "    items = matrix.columns.tolist()\n",
    "\n",
    "    matrix = matrix.as_matrix()\n",
    "    ```\n",
    "    \n",
    "Triplets/siamese + triplet mining - \n",
    "* https://github.com/maciejkula/triplet_recommendations_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommenders embedding - fit generator\n",
    "# https://towardsdatascience.com/building-a-recommendation-system-using-neural-network-embeddings-1ef92e5c80c9\n",
    "# Also has code for generator to generate positive, negative pairs per batch - good for siamese/triplets/metric! \n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "def generate_batch(pairs, n_positive = 50, negative_ratio = 1.0):\n",
    "    \"\"\"Generate batches of samples for training. \n",
    "       Random select positive samples\n",
    "       from pairs and randomly select negatives.\"\"\"\n",
    "    \n",
    "    # Create empty array to hold batch\n",
    "    batch_size = n_positive * (1 + negative_ratio)\n",
    "    batch = np.zeros((batch_size, 3))\n",
    "    \n",
    "    # Continue to yield samples\n",
    "    while True:\n",
    "        # Randomly choose positive examples\n",
    "        for idx, (book_id, link_id) in enumerate(random.sample(pairs, n_positive)):\n",
    "            batch[idx, :] = (book_id, link_id, 1)\n",
    "        idx += 1\n",
    "        \n",
    "        # Add negative examples until reach batch size\n",
    "        while idx < batch_size:\n",
    "            \n",
    "            # Random selection\n",
    "            random_book = random.randrange(len(books))\n",
    "            random_link = random.randrange(len(links))\n",
    "            \n",
    "            # Check to make sure this is not a positive example\n",
    "            if (random_book, random_link) not in pairs_set:\n",
    "                \n",
    "                # Add to batch and increment index\n",
    "                batch[idx, :] = (random_book, random_link, neg_label)\n",
    "                idx += 1\n",
    "                \n",
    "        # Make sure to shuffle order\n",
    "        np.random.shuffle(batch)\n",
    "        yield {'book': batch[:, 0], 'link': batch[:, 1]}, batch[:, 2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible approahc + negatives - https://github.com/zhangruiskyline/DeepLearning/blob/master/doc/Recommendation.md#ranking \n",
    "\n",
    "\n",
    "* Negative sampling from the sparse user-item cooccurrence matrix\n",
    "    * https://stackoverflow.com/questions/49971318/how-to-generate-negative-samples-in-tensorflow\n",
    "    ```\n",
    "    def subsampler(data, num_pos=10, num_neg=10):\n",
    "    \"\"\" Obtain random batch size made up of positive and negative samples\n",
    "    Returns\n",
    "    -------\n",
    "    positive_row : np.array\n",
    "       Row ids of the positive samples\n",
    "    positive_col : np.array\n",
    "       Column ids of the positive samples\n",
    "    positive_data : np.array\n",
    "       Data values in the positive samples\n",
    "    negative_row : np.array\n",
    "       Row ids of the negative samples\n",
    "    negative_col : np.array\n",
    "       Column ids of the negative samples\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    We are not return negative data, since the negative values\n",
    "    are always zero.\n",
    "    \"\"\"\n",
    "    N, D = data.shape\n",
    "    y_data = data.data\n",
    "    y_row = data.row\n",
    "    y_col = data.col\n",
    "\n",
    "    \\# store all of the positive (i, j) coords\n",
    "    idx = np.vstack((y_row, y_col)).T\n",
    "    idx = set(map(tuple, idx.tolist()))\n",
    "    while True:\n",
    "        \\# get positive sample\n",
    "        positive_idx = np.random.choice(len(y_data), num_pos)\n",
    "        positive_row = y_row[positive_idx].astype(np.int32)\n",
    "        positive_col = y_col[positive_idx].astype(np.int32)\n",
    "        positive_data = y_data[positive_idx].astype(np.float32)\n",
    "\n",
    "        \\# get negative sample\n",
    "        negative_row = np.zeros(num_neg, dtype=np.int32)\n",
    "        negative_col = np.zeros(num_neg, dtype=np.int32)\n",
    "        for k in range(num_neg):\n",
    "            i, j = np.random.randint(N), np.random.randint(D)\n",
    "            while (i, j) in idx:\n",
    "                i, j = np.random.randint(N), np.random.randint(D)\n",
    "                negative_row[k] = i\n",
    "                negative_col[k] = j\n",
    "\n",
    "        yield (positive_row, positive_col, positive_data,\n",
    "               negative_row, negative_col)\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, GroupShuffleSplit\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.metrics import TopKCategoricalAccuracy, Precision, SparseTopKCategoricalAccuracy # @4\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## https://www.tensorflow.org/guide/mixed_precision ## TF mixed precision - pytorch requires other setup\n",
    "# from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "\n",
    "# policy = mixed_precision.Policy('mixed_float16')\n",
    "# mixed_precision.set_policy(policy)\n",
    "# ## will need to correct in places, e.g.: \n",
    "# ## outputs = layers.Activation('softmax', dtype='float32', name='predictions')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features to add:\n",
    "* Lag \n",
    "* Rank (popularity) of city, country (in general, +- given booker country)\n",
    "* Count of hotel; user, trip size ? (may be leaky )\n",
    "* Seasonal features - Holidays? , datetime\n",
    "\n",
    "Aggregate feats:\n",
    "* user changed country? last booking (lag 1) country change? \n",
    "* max/min/avg popularity rank of previous locations visited\n",
    "\n",
    "\n",
    "\n",
    "We should create a dictionary of the rank, count, city/country etc' feats, so we can easily merge them when making more \"negative\" samples/feats for ranking.\n",
    "\n",
    "\n",
    "* Consider using a df2 of df without dates + drop_duplicates, +- without user/trip id (After calcing that) .\n",
    "\n",
    "\n",
    "Leaky or potentially leaky (Dependso n test set): \n",
    "* Target freq features - frequency of target city, given source county +- affiliate +- month of year +- given country (and interactions of target freq). \n",
    "    * Risk of leaks - depends of test data has temporal split or not. \n",
    "    * cartboost can do target encode, but this lets us do it for interactions, e.g. target city freq given the 2 countries and affiliate.\n",
    "    * beware overfitting! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_TARGET_FREQ = 40 # drop target/city_id values that appear less than this many times, as final step's target \n",
    "KEEP_TOP_K_TARGETS = 0 # keep K most frequent city ID targets (redundnat with the above, )\n",
    "\n",
    "## (some) categorical variables that appear less than this many times will be replaced with a placeholder value!\n",
    "## Includes CITY id (but done after target filtering, to avoid creating a \"rare class\" target:\n",
    "LOW_COUNT_THRESH = 10\n",
    "\n",
    "RUN_TABNET = False\n",
    "max_epochs = 4\n",
    "\n",
    "GET_COUNT_AGG_FEATS = False ## disable getting count, rank etc' groupby features , for speedup\n",
    "\n",
    "## for matrix factorization/CF:\n",
    "### morte possible ID_cols :  # last (last step in trip) - would double data per user incorrectly\n",
    "### hotel_country_lag1 , city_id_lag1  (very relevant - needs shared embeddingm and would increase cardinality a lot.. ) \n",
    "ID_COLS = ['device_class','affiliate_id', 'booker_country',\n",
    "           'checkin_quarter',\"last\"] \n",
    "MF_KEEP_COLS = [\"ID\"]+ID_COLS+['city_id',\"hotel_country\"]\n",
    "\n",
    "SAVE_TO_DISK = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most basic categorical columns , without 'user_id', , 'utrip_id' ordevice_class - used for count encoding/filtering\n",
    "BASE_CAT_COLS = ['city_id',  'affiliate_id', 'booker_country', 'hotel_country']\n",
    "\n",
    "# ### features to get lags for. Not very robust. May want different feats for lags before -1\n",
    "# LAG_FEAT_COLS = ['city_id', 'device_class',\n",
    "#        'affiliate_id', 'booker_country', 'hotel_country', \n",
    "#        'duration', 'same_country', 'checkin_day', 'checkin_weekday',\n",
    "#        'checkin_week',\n",
    "#         'checkout_weekday','checkout_week',\n",
    "#        'city_id_count', 'affiliate_id_count',\n",
    "#        'booker_country_count', 'hotel_country_count', \n",
    "#        'checkin_month_count', 'checkin_week_count', 'city_id_nunique',\n",
    "#        'affiliate_id_nunique', 'booker_country_nunique',\n",
    "#        'hotel_country_nunique', 'city_id_rank_by_hotel_country',\n",
    "#        'city_id_rank_by_booker_country', 'city_id_rank_by_affiliate',\n",
    "#        'affiliate_id_rank_by_hotel_country',\n",
    "#        'affiliate_id_rank_by_booker_country', 'affiliate_id_rank_by_affiliate',\n",
    "#        'booker_country_rank_by_hotel_country',\n",
    "#        'booker_country_rank_by_booker_country',\n",
    "#        'booker_country_rank_by_affiliate',\n",
    "#        'hotel_country_rank_by_hotel_country',\n",
    "#        'hotel_country_rank_by_booker_country',\n",
    "#        'hotel_country_rank_by_affiliate',\n",
    "#        'checkin_month_rank_by_hotel_country',\n",
    "#        'checkin_month_rank_by_booker_country',\n",
    "#        'checkin_month_rank_by_affiliate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/33907537/groupby-and-lag-all-columns-of-a-dataframe\n",
    "# https://stackoverflow.com/questions/62924987/lag-multiple-variables-grouped-by-columns\n",
    "## lag features with groupby over many columns: \n",
    "def groupbyLagFeatures(df:pd.DataFrame,lag:[]=[1,2],group=\"utrip_id\",lag_feature_cols=[]):\n",
    "    \"\"\"\n",
    "    lag features with groupby over many columns\n",
    "    https://stackoverflow.com/questions/62924987/lag-multiple-variables-grouped-by-columns\"\"\"\n",
    "    if len(lag_feature_cols)>0:\n",
    "        df=pd.concat([df]+[df.groupby(group)[lag_feature_cols].shift(x).add_prefix('lag'+str(x)+\"_\") for x in lag],axis=1)\n",
    "    else:\n",
    "         df=pd.concat([df]+[df.groupby(group).shift(x).add_prefix('lag'+str(x)+\"_\") for x in lag],axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "######## Get n most popular items, per group\n",
    "def most_popular(group, n_max=4):\n",
    "    \"\"\"Find most popular hotel clusters by destination\n",
    "    Define a function to get most popular hotels for a destination group.\n",
    "\n",
    "    Previous version used nlargest() Series method to get indices of largest elements. But the method is rather slow.\n",
    "    Source: https://www.kaggle.com/dvasyukova/predict-hotel-type-with-pandas\n",
    "    \"\"\"\n",
    "    relevance = group['relevance'].values\n",
    "    hotel_cluster = group['hotel_cluster'].values\n",
    "    most_popular = hotel_cluster[np.argsort(relevance)[::-1]][:n_max]\n",
    "    return np.array_str(most_popular)[1:-1] # remove square brackets\n",
    "\n",
    "\n",
    "## https://codereview.stackexchange.com/questions/149306/select-the-n-most-frequent-items-from-a-pandas-groupby-dataframe\n",
    "# https://stackoverflow.com/questions/52073054/group-by-a-column-to-find-the-most-frequent-value-in-another-column\n",
    "## can get modes (sorted)\n",
    "# https://stackoverflow.com/questions/50592762/finding-most-common-values-with-pandas-groupby-and-value-counts\n",
    "## df.groupby('tag')['category'].agg(lambda x: x.value_counts().index[0])\n",
    "# https://stackoverflow.com/questions/15222754/groupby-pandas-dataframe-and-select-most-common-value\n",
    "# source2.groupby(['Country','City'])['Short name'].agg(pd.Series.mode)\n",
    "\n",
    "\n",
    "\n",
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=64,target_col=\"target\"):\n",
    "    \"\"\"\n",
    "    Wrap dataframes with tf.data. \n",
    "    This will enable us to use feature columns as a bridge to map from the columns in a dataframe to features used to train the model.\n",
    "    https://www.tensorflow.org/tutorials/structured_data/feature_columns#create_an_input_pipeline_using_tfdata\n",
    "    \"\"\"\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop(target_col)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>checkin</th>\n",
       "      <th>checkout</th>\n",
       "      <th>city_id</th>\n",
       "      <th>device_class</th>\n",
       "      <th>affiliate_id</th>\n",
       "      <th>booker_country</th>\n",
       "      <th>hotel_country</th>\n",
       "      <th>utrip_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117277</th>\n",
       "      <td>136</td>\n",
       "      <td>2016-09-20</td>\n",
       "      <td>2016-09-22</td>\n",
       "      <td>52933</td>\n",
       "      <td>desktop</td>\n",
       "      <td>9924</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Osterlich</td>\n",
       "      <td>136_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117278</th>\n",
       "      <td>136</td>\n",
       "      <td>2016-09-22</td>\n",
       "      <td>2016-09-23</td>\n",
       "      <td>51685</td>\n",
       "      <td>desktop</td>\n",
       "      <td>9924</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Osterlich</td>\n",
       "      <td>136_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117279</th>\n",
       "      <td>136</td>\n",
       "      <td>2016-09-23</td>\n",
       "      <td>2016-09-24</td>\n",
       "      <td>43323</td>\n",
       "      <td>desktop</td>\n",
       "      <td>9924</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Osterlich</td>\n",
       "      <td>136_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117280</th>\n",
       "      <td>136</td>\n",
       "      <td>2016-09-24</td>\n",
       "      <td>2016-09-26</td>\n",
       "      <td>55990</td>\n",
       "      <td>desktop</td>\n",
       "      <td>9924</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Osterlich</td>\n",
       "      <td>136_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117281</th>\n",
       "      <td>136</td>\n",
       "      <td>2016-09-26</td>\n",
       "      <td>2016-09-27</td>\n",
       "      <td>46411</td>\n",
       "      <td>desktop</td>\n",
       "      <td>9924</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Osterlich</td>\n",
       "      <td>136_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64260</th>\n",
       "      <td>6257973</td>\n",
       "      <td>2016-08-12</td>\n",
       "      <td>2016-08-15</td>\n",
       "      <td>15470</td>\n",
       "      <td>tablet</td>\n",
       "      <td>5755</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Rolisica</td>\n",
       "      <td>6257973_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180788</th>\n",
       "      <td>6258041</td>\n",
       "      <td>2016-04-28</td>\n",
       "      <td>2016-04-29</td>\n",
       "      <td>57109</td>\n",
       "      <td>mobile</td>\n",
       "      <td>9452</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>Glubbdubdrib</td>\n",
       "      <td>6258041_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180789</th>\n",
       "      <td>6258041</td>\n",
       "      <td>2016-04-29</td>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>57109</td>\n",
       "      <td>mobile</td>\n",
       "      <td>9452</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>Glubbdubdrib</td>\n",
       "      <td>6258041_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180790</th>\n",
       "      <td>6258041</td>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>2016-05-01</td>\n",
       "      <td>7529</td>\n",
       "      <td>mobile</td>\n",
       "      <td>9452</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>Glubbdubdrib</td>\n",
       "      <td>6258041_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180791</th>\n",
       "      <td>6258041</td>\n",
       "      <td>2016-05-01</td>\n",
       "      <td>2016-05-02</td>\n",
       "      <td>17338</td>\n",
       "      <td>mobile</td>\n",
       "      <td>9452</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>Glubbdubdrib</td>\n",
       "      <td>6258041_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>223456 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id    checkin   checkout  city_id device_class  affiliate_id  \\\n",
       "117277      136 2016-09-20 2016-09-22    52933      desktop          9924   \n",
       "117278      136 2016-09-22 2016-09-23    51685      desktop          9924   \n",
       "117279      136 2016-09-23 2016-09-24    43323      desktop          9924   \n",
       "117280      136 2016-09-24 2016-09-26    55990      desktop          9924   \n",
       "117281      136 2016-09-26 2016-09-27    46411      desktop          9924   \n",
       "...         ...        ...        ...      ...          ...           ...   \n",
       "64260   6257973 2016-08-12 2016-08-15    15470       tablet          5755   \n",
       "180788  6258041 2016-04-28 2016-04-29    57109       mobile          9452   \n",
       "180789  6258041 2016-04-29 2016-04-30    57109       mobile          9452   \n",
       "180790  6258041 2016-04-30 2016-05-01     7529       mobile          9452   \n",
       "180791  6258041 2016-05-01 2016-05-02    17338       mobile          9452   \n",
       "\n",
       "              booker_country hotel_country   utrip_id  \n",
       "117277  The Devilfire Empire     Osterlich      136_4  \n",
       "117278  The Devilfire Empire     Osterlich      136_4  \n",
       "117279  The Devilfire Empire     Osterlich      136_4  \n",
       "117280  The Devilfire Empire     Osterlich      136_4  \n",
       "117281  The Devilfire Empire     Osterlich      136_4  \n",
       "...                      ...           ...        ...  \n",
       "64260                 Gondal      Rolisica  6257973_1  \n",
       "180788               Elbonia  Glubbdubdrib  6258041_1  \n",
       "180789               Elbonia  Glubbdubdrib  6258041_1  \n",
       "180790               Elbonia  Glubbdubdrib  6258041_1  \n",
       "180791               Elbonia  Glubbdubdrib  6258041_1  \n",
       "\n",
       "[223456 rows x 9 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"booking_train_set.csv\",\n",
    "                 nrows=223456,\n",
    "                 index_col=[0],\n",
    "                 parse_dates=[\"checkin\",\"checkout\"],infer_datetime_format=True)\n",
    "\n",
    "df.sort_values([\"user_id\",\"checkin\",\"checkout\"],inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### i disabled most of thefeature eztraction here for simplicity\n",
    "\n",
    "# df[\"duration\"] = (df[\"checkout\"] - df[\"checkin\"]).dt.days\n",
    "# df[\"same_country\"] = (df[\"booker_country\"]==df[\"hotel_country\"]).astype(int)\n",
    "\n",
    "# df[\"checkin_day\"] = df[\"checkin\"].dt.day\n",
    "# df[\"checkin_weekday\"] = df[\"checkin\"].dt.weekday\n",
    "df[\"checkin_week\"] = df[\"checkin\"].dt.isocalendar().week.astype(int) ## week of year\n",
    "df[\"checkin_month\"] = df[\"checkin\"].dt.month\n",
    "# df[\"checkin_year\"] = df[\"checkin\"].dt.year-2016\n",
    "\n",
    "df[\"checkin_quarter\"] = df[\"checkin\"].dt.quarter # relatively redundant but may be used for \"id\"\n",
    "\n",
    "# df[\"checkin_quarter\"] = df[\"checkin_quarter\"]/4 # scale. could also do cos, sin extraction. makesi t a float instead of int/embedding\n",
    "\n",
    "\n",
    "# df[\"checkout_weekday\"] = df[\"checkout\"].dt.weekday\n",
    "# df[\"checkout_week\"] = df[\"checkout\"].dt.isocalendar().week.astype(int) ## week of year\n",
    "# df[\"checkout_day\"] = df[\"checkout\"].dt.day ## day of month\n",
    "\n",
    "## cyclical datetime embeddings\n",
    "## drop originakl variables? \n",
    "## TODO:L add for other variables, +- those that we'll embed (week?)\n",
    "\n",
    "# df['checkin_weekday_sin'] = np.sin(df[\"checkin_weekday\"]*(2.*np.pi/7))\n",
    "# df['checkin_weekday_cos'] = np.cos(df[\"checkin_weekday\"]*(2.*np.pi/7))\n",
    "df['checkin_month_sin'] = np.sin((df[\"checkin_month\"]-1)*(2.*np.pi/12))\n",
    "df['checkin_month_cos'] = np.cos((df[\"checkin_month\"]-1)*(2.*np.pi/12))\n",
    "\n",
    "# #############\n",
    "# # last number in utrip id - probably which trip number it is:\n",
    "# df[\"utrip_number\"] = df[\"utrip_id\"].str.split(\"_\",expand=True)[1].astype(int)\n",
    "\n",
    "### encode string columns - must be consistent with test data \n",
    "### IF we can concat test with train, we can just do a single transformation  for the NON TARGET cols\n",
    "# obj_cols_list = df.select_dtypes(\"O\").columns.values\n",
    "obj_cols_list = ['device_class','booker_country','hotel_country',\n",
    "#                 \"city_id\"\n",
    "                ] # we could also define when loading data, dtype\n",
    "\n",
    "for c in obj_cols_list:\n",
    "    df[c] = df[c].astype(\"category\")\n",
    "    df[c] = df[c].cat.codes.astype(int)\n",
    "#     print(\"min\",df[c].min()) min is 0 - which is what the embedding (indices) will expect\n",
    "\n",
    "## view steps of a trip per user & trip, in order. ## last step == 1.\n",
    "## count #/pct step in a trip (utrip_id) per user. Useful to get the \"final\" step per trip - for prediction\n",
    "## note that the order is ascending, so we would need to select by \"last\" . (i.e \"1\" is the first step, 2 the second, etc') , or we could use pct .rank(ascending=True,pct=True)\n",
    "#### this feature overlaps with the count of each trip id (for the final row)\n",
    "##  = df.sort_values([\"checkin\",\"checkout\"])... - df already sorted above\n",
    "df[\"utrip_steps_from_end\"] = df.groupby(\"utrip_id\")[\"checkin\"].rank(ascending=True,pct=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             row_num     total_rows\n",
      "count  223456.000000  223456.000000\n",
      "mean        3.552189       6.104378\n",
      "std         2.368781       2.787018\n",
      "min         1.000000       1.000000\n",
      "25%         2.000000       4.000000\n",
      "50%         3.000000       5.000000\n",
      "75%         5.000000       7.000000\n",
      "max        48.000000      48.000000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### add features to be consistent with test set of row in trip, and total trips in trip\n",
    "\n",
    "df[\"row_num\"] = df.groupby(\"utrip_id\")[\"checkin\"].rank(ascending=True,pct=False).astype(int)\n",
    "utrip_counts = df[\"utrip_id\"].value_counts()\n",
    "df[\"total_rows\"] = df[\"utrip_id\"].map(utrip_counts)\n",
    "\n",
    "### last step in trip\n",
    "df[\"last\"] = (df[\"total_rows\"]==df[\"row_num\"]).astype(int)\n",
    "print(df[[\"row_num\",\"total_rows\"]].describe())\n",
    "\n",
    "df[\"total_rows\"].hist(); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### replace rare categorical variable(s) - affiliates\n",
    "* replace rare variables (under 2 occurrences) with \"-1\" dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: 9924    53075\n",
      "359     33238\n",
      "384     16955\n",
      "9452    16316\n",
      "4541     8089\n",
      "        ...  \n",
      "5080        1\n",
      "6617        1\n",
      "6361        1\n",
      "2139        1\n",
      "4094        1\n",
      "Name: affiliate_id, Length: 1655, dtype: int64\n",
      "uniques 1655\n",
      "after\n",
      " 9924    53075\n",
      "359     33238\n",
      "384     16955\n",
      "9452    16316\n",
      "4541     8089\n",
      "        ...  \n",
      "8632        3\n",
      "7871        3\n",
      "6078        3\n",
      "1760        3\n",
      "4395        3\n",
      "Name: affiliate_id, Length: 1016, dtype: int64\n",
      "uniques 1016\n"
     ]
    }
   ],
   "source": [
    "### replace rare variables (under 2 occurrences) with \"-1\" dummy\n",
    "\n",
    "affiliates_counts = df[\"affiliate_id\"].value_counts()\n",
    "print(\"before:\", affiliates_counts)\n",
    "print(\"uniques\",df[\"affiliate_id\"].nunique())\n",
    "affiliates_counts = affiliates_counts.to_dict()\n",
    "# df[\"affiliate_id\"] = df[\"affiliate_id\"].where(df[\"affiliate_id\"].apply(lambda x: x.map(x.value_counts()))>=3, -1)\n",
    "df[\"affiliate_id\"] = df[\"affiliate_id\"].where(df[\"affiliate_id\"].map(affiliates_counts)>=3, -2)\n",
    "df[\"affiliate_id\"] = df[\"affiliate_id\"].astype(int)\n",
    "\n",
    "print(\"after\\n\",df[\"affiliate_id\"].value_counts())\n",
    "print(\"uniques\",df[\"affiliate_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"total_rows\"].map(pd.cut(df[\"total_rows\"],bins=3,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create pseudo ID for use with CF - composed of semi-distinct tuple of variables\n",
    "* should have moderate uniqueness. Main purpose is to get good embeddings for those variables \n",
    "\n",
    "* `'device_class','affiliate_id', 'booker_country','checkin_quarter'` - 14K \"uniques\"\n",
    "* `'device_class','affiliate_id', 'booker_country'` - 7.5 K \"uniques\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_class          3\n",
      "affiliate_id       1016\n",
      "booker_country        5\n",
      "checkin_quarter       4\n",
      "last                  2\n",
      "checkin_month        12\n",
      "total_rows           29\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device_class  affiliate_id  booker_country  checkin_quarter  last\n",
       "0             -2            0               2                0        3\n",
       "                                            3                0        7\n",
       "                                                             1        2\n",
       "                                            4                0       11\n",
       "                                                             1        2\n",
       "                                                                     ..\n",
       "2              10613        2               4                0        2\n",
       "               10615        1               2                0        2\n",
       "                                            3                0        7\n",
       "                                                             1        1\n",
       "               10668        2               1                0        1\n",
       "Length: 8083, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### for possible \"user id\" embedding/ID : How many unique values are there for these source tuple? :\n",
    "### Could also maybe add previous location/lag1 country/city ? \n",
    "## 'device_class','affiliate_id', 'booker_country' - 7.5 K \"uniques\"\n",
    "## 'device_class','affiliate_id', 'booker_country','checkin_month' - 24 K \"uniques\"\n",
    "## 'device_class','affiliate_id', 'booker_country','checkin_quarter' 14K \"uniques\"\n",
    "## ,\"total_rows\" \n",
    "\n",
    "print(df[ID_COLS + ['checkin_month',\"total_rows\"]].nunique(axis=0))\n",
    "df.groupby(ID_COLS).size()\n",
    "\n",
    "# id2 = [item for item in ID_COLS if item != \"last\"]\n",
    "# df.groupby(id2).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180789    19452120\n",
      "180790    19452120\n",
      "180791    19452121\n",
      "Name: ID, dtype: object\n",
      "8083\n"
     ]
    }
   ],
   "source": [
    "# df[\"ID\"] = df['device_class'].astype(str)+\"_\"+df['affiliate_id'].astype(str)\\\n",
    "# +\"_\"+df['booker_country'].astype(str)+\"_\"+df['checkin_quarter'].astype(str)\n",
    "\n",
    "df[\"ID\"] = df[ID_COLS].astype(str).sum(1)#.astype(\"category\")\n",
    "\n",
    "# df[\"ID\"] = df[\"ID\"].astype(\"category\")\n",
    "print(df[\"ID\"].tail(3))\n",
    "print(df[\"ID\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keep seperate DF of IDs and component features - will make merging id feats simpler later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>device_class</th>\n",
       "      <th>affiliate_id</th>\n",
       "      <th>booker_country</th>\n",
       "      <th>checkin_quarter</th>\n",
       "      <th>last</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>09924430</th>\n",
       "      <td>0</td>\n",
       "      <td>9924</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09924431</th>\n",
       "      <td>0</td>\n",
       "      <td>9924</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>03417410</th>\n",
       "      <td>0</td>\n",
       "      <td>3417</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>05583411</th>\n",
       "      <td>0</td>\n",
       "      <td>5583</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07974440</th>\n",
       "      <td>0</td>\n",
       "      <td>7974</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07063131</th>\n",
       "      <td>0</td>\n",
       "      <td>7063</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07652210</th>\n",
       "      <td>0</td>\n",
       "      <td>7652</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>05939340</th>\n",
       "      <td>0</td>\n",
       "      <td>5939</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>05939341</th>\n",
       "      <td>0</td>\n",
       "      <td>5939</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11601110</th>\n",
       "      <td>1</td>\n",
       "      <td>1601</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8083 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          device_class  affiliate_id  booker_country  checkin_quarter  last\n",
       "ID                                                                         \n",
       "09924430             0          9924               4                3     0\n",
       "09924431             0          9924               4                3     1\n",
       "03417410             0          3417               4                1     0\n",
       "05583411             0          5583               4                1     1\n",
       "07974440             0          7974               4                4     0\n",
       "...                ...           ...             ...              ...   ...\n",
       "07063131             0          7063               1                3     1\n",
       "07652210             0          7652               2                1     0\n",
       "05939340             0          5939               3                4     0\n",
       "05939341             0          5939               3                4     1\n",
       "11601110             1          1601               1                1     0\n",
       "\n",
       "[8083 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ID = df[[\"ID\"]+ID_COLS].drop_duplicates().set_index(\"ID\")\n",
    "df_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    223456.000000\n",
      "mean        322.684314\n",
      "std         488.074165\n",
      "min           1.000000\n",
      "25%          17.000000\n",
      "50%         101.000000\n",
      "75%         381.000000\n",
      "max        2226.000000\n",
      "Name: city_id_count, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>checkin</th>\n",
       "      <th>checkout</th>\n",
       "      <th>city_id</th>\n",
       "      <th>device_class</th>\n",
       "      <th>affiliate_id</th>\n",
       "      <th>booker_country</th>\n",
       "      <th>hotel_country</th>\n",
       "      <th>utrip_id</th>\n",
       "      <th>checkin_week</th>\n",
       "      <th>checkin_month</th>\n",
       "      <th>checkin_quarter</th>\n",
       "      <th>checkin_month_sin</th>\n",
       "      <th>checkin_month_cos</th>\n",
       "      <th>utrip_steps_from_end</th>\n",
       "      <th>row_num</th>\n",
       "      <th>total_rows</th>\n",
       "      <th>last</th>\n",
       "      <th>ID</th>\n",
       "      <th>city_id_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117277</th>\n",
       "      <td>136</td>\n",
       "      <td>2016-09-20</td>\n",
       "      <td>2016-09-22</td>\n",
       "      <td>52933</td>\n",
       "      <td>0</td>\n",
       "      <td>9924</td>\n",
       "      <td>4</td>\n",
       "      <td>105</td>\n",
       "      <td>136_4</td>\n",
       "      <td>38</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>09924430</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117278</th>\n",
       "      <td>136</td>\n",
       "      <td>2016-09-22</td>\n",
       "      <td>2016-09-23</td>\n",
       "      <td>51685</td>\n",
       "      <td>0</td>\n",
       "      <td>9924</td>\n",
       "      <td>4</td>\n",
       "      <td>105</td>\n",
       "      <td>136_4</td>\n",
       "      <td>38</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>09924430</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117279</th>\n",
       "      <td>136</td>\n",
       "      <td>2016-09-23</td>\n",
       "      <td>2016-09-24</td>\n",
       "      <td>43323</td>\n",
       "      <td>0</td>\n",
       "      <td>9924</td>\n",
       "      <td>4</td>\n",
       "      <td>105</td>\n",
       "      <td>136_4</td>\n",
       "      <td>38</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>09924430</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117280</th>\n",
       "      <td>136</td>\n",
       "      <td>2016-09-24</td>\n",
       "      <td>2016-09-26</td>\n",
       "      <td>55990</td>\n",
       "      <td>0</td>\n",
       "      <td>9924</td>\n",
       "      <td>4</td>\n",
       "      <td>105</td>\n",
       "      <td>136_4</td>\n",
       "      <td>38</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>09924430</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117281</th>\n",
       "      <td>136</td>\n",
       "      <td>2016-09-26</td>\n",
       "      <td>2016-09-27</td>\n",
       "      <td>46411</td>\n",
       "      <td>0</td>\n",
       "      <td>9924</td>\n",
       "      <td>4</td>\n",
       "      <td>105</td>\n",
       "      <td>136_4</td>\n",
       "      <td>39</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>09924430</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64260</th>\n",
       "      <td>6257973</td>\n",
       "      <td>2016-08-12</td>\n",
       "      <td>2016-08-15</td>\n",
       "      <td>15470</td>\n",
       "      <td>2</td>\n",
       "      <td>5755</td>\n",
       "      <td>2</td>\n",
       "      <td>119</td>\n",
       "      <td>6257973_1</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>25755231</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180788</th>\n",
       "      <td>6258041</td>\n",
       "      <td>2016-04-28</td>\n",
       "      <td>2016-04-29</td>\n",
       "      <td>57109</td>\n",
       "      <td>1</td>\n",
       "      <td>9452</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>6258041_1</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>19452120</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180789</th>\n",
       "      <td>6258041</td>\n",
       "      <td>2016-04-29</td>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>57109</td>\n",
       "      <td>1</td>\n",
       "      <td>9452</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>6258041_1</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>19452120</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180790</th>\n",
       "      <td>6258041</td>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>2016-05-01</td>\n",
       "      <td>7529</td>\n",
       "      <td>1</td>\n",
       "      <td>9452</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>6258041_1</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>19452120</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180791</th>\n",
       "      <td>6258041</td>\n",
       "      <td>2016-05-01</td>\n",
       "      <td>2016-05-02</td>\n",
       "      <td>17338</td>\n",
       "      <td>1</td>\n",
       "      <td>9452</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>6258041_1</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>19452121</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>223456 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id    checkin   checkout  city_id  device_class  affiliate_id  \\\n",
       "117277      136 2016-09-20 2016-09-22    52933             0          9924   \n",
       "117278      136 2016-09-22 2016-09-23    51685             0          9924   \n",
       "117279      136 2016-09-23 2016-09-24    43323             0          9924   \n",
       "117280      136 2016-09-24 2016-09-26    55990             0          9924   \n",
       "117281      136 2016-09-26 2016-09-27    46411             0          9924   \n",
       "...         ...        ...        ...      ...           ...           ...   \n",
       "64260   6257973 2016-08-12 2016-08-15    15470             2          5755   \n",
       "180788  6258041 2016-04-28 2016-04-29    57109             1          9452   \n",
       "180789  6258041 2016-04-29 2016-04-30    57109             1          9452   \n",
       "180790  6258041 2016-04-30 2016-05-01     7529             1          9452   \n",
       "180791  6258041 2016-05-01 2016-05-02    17338             1          9452   \n",
       "\n",
       "        booker_country  hotel_country   utrip_id  checkin_week  checkin_month  \\\n",
       "117277               4            105      136_4            38              9   \n",
       "117278               4            105      136_4            38              9   \n",
       "117279               4            105      136_4            38              9   \n",
       "117280               4            105      136_4            38              9   \n",
       "117281               4            105      136_4            39              9   \n",
       "...                ...            ...        ...           ...            ...   \n",
       "64260                2            119  6257973_1            32              8   \n",
       "180788               1             55  6258041_1            17              4   \n",
       "180789               1             55  6258041_1            17              4   \n",
       "180790               1             55  6258041_1            17              4   \n",
       "180791               1             55  6258041_1            17              5   \n",
       "\n",
       "        checkin_quarter  checkin_month_sin  checkin_month_cos  \\\n",
       "117277                3          -0.866025      -5.000000e-01   \n",
       "117278                3          -0.866025      -5.000000e-01   \n",
       "117279                3          -0.866025      -5.000000e-01   \n",
       "117280                3          -0.866025      -5.000000e-01   \n",
       "117281                3          -0.866025      -5.000000e-01   \n",
       "...                 ...                ...                ...   \n",
       "64260                 3          -0.500000      -8.660254e-01   \n",
       "180788                2           1.000000       6.123234e-17   \n",
       "180789                2           1.000000       6.123234e-17   \n",
       "180790                2           1.000000       6.123234e-17   \n",
       "180791                2           0.866025      -5.000000e-01   \n",
       "\n",
       "        utrip_steps_from_end  row_num  total_rows  last        ID  \\\n",
       "117277              0.142857        1           7     0  09924430   \n",
       "117278              0.285714        2           7     0  09924430   \n",
       "117279              0.428571        3           7     0  09924430   \n",
       "117280              0.571429        4           7     0  09924430   \n",
       "117281              0.714286        5           7     0  09924430   \n",
       "...                      ...      ...         ...   ...       ...   \n",
       "64260               1.000000        4           4     1  25755231   \n",
       "180788              0.250000        1           4     0  19452120   \n",
       "180789              0.500000        2           4     0  19452120   \n",
       "180790              0.750000        3           4     0  19452120   \n",
       "180791              1.000000        4           4     1  19452121   \n",
       "\n",
       "        city_id_count  \n",
       "117277            232  \n",
       "117278             71  \n",
       "117279             11  \n",
       "117280             32  \n",
       "117281            174  \n",
       "...               ...  \n",
       "64260              69  \n",
       "180788             13  \n",
       "180789             13  \n",
       "180790              3  \n",
       "180791             20  \n",
       "\n",
       "[223456 rows x 20 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Following aggregation features - would be best to use time window (sort data) to generate, otherwise they will LEAK! (e.g. nunique countries visited)\n",
    "\n",
    "### count features (can also later add rank inside groups).\n",
    "### Some may be leaks (# visits in a trip should use time window?) , and do users repeat? \n",
    "### can add more counts of group X time period (e.g. affiliate X month of year)\n",
    "\n",
    "## alt way to get counts/freq :\n",
    "\n",
    "if GET_COUNT_AGG_FEATS:\n",
    "    count_cols = [ 'city_id','affiliate_id', 'booker_country', 'hotel_country', \n",
    "    #               'utrip_id','user_id', \n",
    "     \"checkin_month\",\"checkin_week\"]\n",
    "    for c in count_cols:\n",
    "        df[f\"{c}_count\"] = df.groupby([c])[\"duration\"].transform(\"size\")\n",
    "\n",
    "    ########################################################\n",
    "    ## nunique per trip\n",
    "    ### https://stackoverflow.com/questions/46470743/how-to-efficiently-compute-a-rolling-unique-count-in-a-pandas-time-series\n",
    "\n",
    "    nunique_cols = [ 'city_id','affiliate_id', 'booker_country', 'hotel_country']\n",
    "    # df[\"nunique_booker_countries\"] = df.groupby(\"utrip_id\")[\"booker_country\"].nunique()\n",
    "    # df[\"nunique_hotel_country\"] = df.groupby(\"utrip_id\")[\"hotel_country\"].nunique()\n",
    "    for c in nunique_cols:\n",
    "        df[f\"{c}_nunique\"] = df.groupby([\"utrip_id\"])[c].transform(\"nunique\")\n",
    "    print(df.nunique())\n",
    "\n",
    "    ########################################################\n",
    "    ## get frequency/count feature's rank within a group - e.g. within a country (or affiliate) \n",
    "    ## add \"_count\" to column name to get count col name, then add rank col \n",
    "\n",
    "    ### ALT/ duplicate feat - add percent rank (instead or in addition)\n",
    "\n",
    "    rank_cols = ['city_id','affiliate_id', 'booker_country','hotel_country',\n",
    "     \"checkin_month\"]\n",
    "    ### what is meaning of groupby and rank of smae variable by same var? Surely should be 1 / unary? \n",
    "    for c in rank_cols:\n",
    "        df[f\"{c}_rank_by_hotel_country\"] = df.groupby(['hotel_country'])[f\"{c}_count\"].transform(\"rank\")\n",
    "        df[f\"{c}_rank_by_booker_country\"] = df.groupby(['booker_country'])[f\"{c}_count\"].transform(\"rank\")\n",
    "        df[f\"{c}_rank_by_affiliate\"] = df.groupby(['affiliate_id'])[f\"{c}_count\"].transform(\"rank\")     \n",
    "else:\n",
    "    freq = df[\"city_id\"].value_counts()\n",
    "    df[\"city_id_count\"] = df[\"city_id\"].map(freq)\n",
    "    print(df[\"city_id_count\"].describe())\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3944"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df[\"city_id_count\"]>=7][\"city_id\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df.isna().sum().max() ==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197722\n",
      "df2 nunique (cities without duplicate user visits) 21238\n",
      "city counts\n",
      "23921    1589\n",
      "47499    1458\n",
      "55128    1362\n",
      "64876    1284\n",
      "29319    1250\n",
      "         ... \n",
      "49444       1\n",
      "2237        1\n",
      "188         1\n",
      "32126       1\n",
      "6147        1\n",
      "Name: city_id, Length: 21238, dtype: int64\n",
      "count    21238.000000\n",
      "mean         9.309822\n",
      "std         45.507467\n",
      "min          1.000000\n",
      "25%          1.000000\n",
      "50%          2.000000\n",
      "75%          4.000000\n",
      "max       1589.000000\n",
      "Name: city_id, dtype: float64\n",
      "cities with at least 3: 7870\n",
      "cities with at least 7: 3734\n",
      "cities with at least 15: 1944\n",
      "cities with at least 30: 1109\n",
      "cities with at least 100: 335\n",
      "cities with at least 300: 78\n",
      "top 4 sum coverage (normalized):  0.029\n",
      "top 50 sum coverage (normalized):  0.184\n",
      "top 100 sum coverage (normalized):  0.264\n",
      "top 400 sum coverage (normalized):  0.48\n",
      "top 1,000 sum coverage (normalized):  0.641\n",
      "top 5,000 sum coverage (normalized):  0.865\n",
      "top 8,000 sum coverage (normalized):  0.916\n"
     ]
    }
   ],
   "source": [
    "# df2 = df[[\"user_id\",\"city_id\"]].drop_duplicates().copy()\n",
    "df2 = df.drop_duplicates(subset=[\"user_id\",\"city_id\"])[\"city_id\"].copy()\n",
    "print(df2.shape[0])\n",
    "print(\"df2 nunique (cities without duplicate user visits)\",df2.nunique())\n",
    "\n",
    "# c2_counts = df2[\"city_id\"].value_counts()\n",
    "c2_counts = df2.value_counts()\n",
    "# df2[\"new_counts\"] = df2[\"city_id\"].map(c2_counts)\n",
    "# df2[\"new_counts\"] = df2.map(c2_counts)\n",
    "print(\"city counts\")\n",
    "print(c2_counts)\n",
    "print(c2_counts.describe())\n",
    "print(\"cities with at least 3:\",(c2_counts>=3).sum())\n",
    "print(\"cities with at least 7:\",(c2_counts>=7).sum())\n",
    "print(\"cities with at least 15:\",(c2_counts>=15).sum())\n",
    "print(\"cities with at least 30:\",(c2_counts>=30).sum())\n",
    "print(\"cities with at least 100:\",(c2_counts>=100).sum())\n",
    "print(\"cities with at least 300:\",(c2_counts>=300).sum())\n",
    "\n",
    "c2_freq = df2.value_counts(normalize=True)\n",
    "print(\"top 4 sum coverage (normalized): \",c2_freq[0:4].sum().round(3))\n",
    "print(\"top 50 sum coverage (normalized): \",c2_freq[0:50].sum().round(3))\n",
    "print(\"top 100 sum coverage (normalized): \",c2_freq[0:100].sum().round(3))\n",
    "print(\"top 400 sum coverage (normalized): \",c2_freq[0:400].sum().round(3))\n",
    "print(\"top 1,000 sum coverage (normalized): \",c2_freq[0:1000].sum().round(3))\n",
    "print(\"top 5,000 sum coverage (normalized): \",c2_freq[0:5000].sum().round(3))\n",
    "print(\"top 8,000 sum coverage (normalized): \",c2_freq[0:8000].sum().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent city target List + City count encoding\n",
    "* Get the K most frequent target city IDs - selected based on frequency as final destination (not just overall)\n",
    "* +- Also after this, replace rare city IDs categorical features with count encoding to reduce dimensionality\n",
    "    * Keep them as count, or aggregate all of them as \"under_K\"?\n",
    "\n",
    "##### Output  : `TOP_TARGETS` - filter data by this *after* creation of lag features ! \n",
    "\n",
    "* Drop duplicates by the same user (reduce possible bias of frequent users? Only relevant if test is seperater from \"frequent travellers\") \n",
    "    * results in 216,633 , vs 217,686 without dropping duplicates by users\n",
    "    * ~19.9k unique cities\n",
    "    \n",
    "* Could do other encodings - https://contrib.scikit-learn.org/category_encoders/count.html\n",
    "\n",
    "* Note that all this is after we've added rank, count features beforehand, so that information won't be lost for these variables, despite these transforms\n",
    "\n",
    "\n",
    "\n",
    "* **NOTE** he most frequent final destinations are NOT the same as the most popular overall destinations +- first location ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if KEEP_TOP_K_TARGETS > 0 :\n",
    "    df_end = df.loc[df[\"utrip_steps_from_end\"]==1].drop_duplicates(subset=[\"city_id\",\"hotel_country\",\"user_id\"])[[\"city_id\",\"hotel_country\"]].copy()\n",
    "    print(df_end.shape[0])\n",
    "    end_city_counts = df_end.city_id.value_counts()\n",
    "    print(end_city_counts)\n",
    "    \n",
    "    TOP_TARGETS = end_city_counts.head(KEEP_TOP_K_TARGETS).index.values\n",
    "    print(f\"top {KEEP_TOP_K_TARGETS} targets \\n\",TOP_TARGETS)\n",
    "    \n",
    "#     assert df.loc[df[\"city_id\"].isin(TOP_TARGETS)][\"city_id\"].nunique() == KEEP_TOP_K_TARGETS\n",
    "\n",
    "####        \n",
    "# replace low frequency categoircal features    \n",
    "\n",
    "# ##replace with count encoding if have at least k, group rarest as \"-1\":# df[BASE_CAT_COLS] = df[BASE_CAT_COLS].where(df[BASE_CAT_COLS].apply(lambda x: x.map(x.value_counts()))>=LOW_COUNT_THRESH, -1)   \n",
    "# ## replace/group only the rare variables : \n",
    "# df[BASE_CAT_COLS] = df[BASE_CAT_COLS].where(df[BASE_CAT_COLS].apply(lambda x: x.map(x.value_counts()))>=LOW_COUNT_THRESH, -1)\n",
    "# df[BASE_CAT_COLS].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Long tail of targets warning!\n",
    "* 75% of cities appear less than 4 times in the data (as a final destination!) \n",
    "    * Dropping them will mean a maximum accuracy of 25% at best!!\n",
    "    * training on intermediates may help overcome improve this. \n",
    "* Using ~2d step+ , still leaves us with 75% appearing less than 7 times\n",
    "\n",
    "* Top 4,000 cities (just for those as final trip destination) - offers 89% coverage - \n",
    "\n",
    "* Unsure how to handle this - too amny targets to learn, and no auxiliary data to help learn it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df_end.city_id.value_counts(normalize=True)[0:4000].sum().round(3)# 4k: 89% coverage  (note, this is just for the end count cities, not all cities overall)\n",
    "\n",
    "# df_end.city_id.value_counts(normalize=True)[0:2000].sum().round(3) #7k: 97% coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter data by city id frequency\n",
    "#### df2 - smaller df (may not ben ecessary to make\n",
    "\n",
    "* drop rows if it's city id appears less than X times - this is prior to CF\n",
    "* We could also add inclusion/exclusion based on target appearing/frequency as target in final stage of rtip - optional\n",
    "* maybe also drop (end exclude in freq counting) thefirst point in a trip ? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropping users with less than 4 trips\n",
      "abnormal users dropped 386\n",
      "dropping cities with less than 40 occurences:\n",
      "145035\n",
      "nunique cities after freq filt 914\n",
      "nunique city_id per hotel_country:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count     78.000000\n",
       "mean      11.717949\n",
       "std       18.890592\n",
       "min        1.000000\n",
       "25%        1.250000\n",
       "50%        5.000000\n",
       "75%       12.750000\n",
       "max      118.000000\n",
       "Name: city_id, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### unsure about this filtering - depends if data points are real or mistake\n",
    "print(\"dropping users with less than 4 trips\")\n",
    "df2 = df.loc[df[\"total_rows\"]>=4].copy()\n",
    "print(\"abnormal users dropped\",df.shape[0]-df2.shape[0])\n",
    "\n",
    "print(f\"dropping cities with less than {MIN_TARGET_FREQ} occurences:\")\n",
    "df2 = df2.loc[df2.groupby([\"city_id\"])[\"hotel_country\"].transform(\"count\")>=MIN_TARGET_FREQ] ## update count\n",
    "# df2 = df2.loc[df2[\"city_id_count\"]>=MIN_TARGET_FREQ]\n",
    "\n",
    "print(df2.shape[0])\n",
    "\n",
    "print(\"nunique cities after freq filt\",df2[\"city_id\"].nunique())\n",
    "print(\"nunique city_id per hotel_country:\")\n",
    "df2.groupby([\"hotel_country\"])[\"city_id\"].nunique().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAG_FEAT_COLS = ['city_id', 'device_class',\n",
    "#        'affiliate_id', 'booker_country', 'hotel_country', \n",
    "#        'duration', 'same_country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ### lag features - last n visits\n",
    "# groupbyLagFeatures(df=df2.head(20), # .set_index([\"checkin\",\"checkout\",\"user_id\"])\n",
    "#                    lag=[1,2],group=\"utrip_id\",lag_feature_cols=LAG_FEAT_COLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get a DF of all cities per country\n",
    "* +- get from original DF, +- remove cities that appear less than 4? times , and countries with less than 4 hotels? (Or keep - to avoid messing up training?)\n",
    "* Weighted Sample from it, for negatives, +- most freq by country/affiliate/etc\n",
    "* Don't drop duplicates by user, keep orig freq? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city_id          914\n",
      "hotel_country     78\n",
      "city_id_count    291\n",
      "dtype: int64\n",
      "914\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_id</th>\n",
       "      <th>hotel_country</th>\n",
       "      <th>city_id_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>179186</th>\n",
       "      <td>47499</td>\n",
       "      <td>71</td>\n",
       "      <td>2226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211121</th>\n",
       "      <td>23921</td>\n",
       "      <td>34</td>\n",
       "      <td>2043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138117</th>\n",
       "      <td>36063</td>\n",
       "      <td>56</td>\n",
       "      <td>1838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222070</th>\n",
       "      <td>17013</td>\n",
       "      <td>21</td>\n",
       "      <td>1746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162153</th>\n",
       "      <td>29319</td>\n",
       "      <td>34</td>\n",
       "      <td>1593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171047</th>\n",
       "      <td>15564</td>\n",
       "      <td>93</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100018</th>\n",
       "      <td>14378</td>\n",
       "      <td>56</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183141</th>\n",
       "      <td>14197</td>\n",
       "      <td>49</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11330</th>\n",
       "      <td>12445</td>\n",
       "      <td>56</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33307</th>\n",
       "      <td>9383</td>\n",
       "      <td>21</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>914 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        city_id  hotel_country  city_id_count\n",
       "179186    47499             71           2226\n",
       "211121    23921             34           2043\n",
       "138117    36063             56           1838\n",
       "222070    17013             21           1746\n",
       "162153    29319             34           1593\n",
       "...         ...            ...            ...\n",
       "171047    15564             93             40\n",
       "100018    14378             56             40\n",
       "183141    14197             49             40\n",
       "11330     12445             56             40\n",
       "33307      9383             21             40\n",
       "\n",
       "[914 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_cities = df[[\"city_id\",\"hotel_country\",\"city_id_count\"]] ## +- drop duplicates by tripid? \n",
    "df_cities = df2[[\"city_id\",\"hotel_country\",\"city_id_count\"]].drop_duplicates().sort_values([\"city_id_count\",\"city_id\"],ascending=False).head(9123)\n",
    "# print(df_cities.nunique())\n",
    "# df_cities = df_cities.loc[df_cities.groupby(\"hotel_country\")[\"city_id\"].transform(\"nunique\")>4]\n",
    "# df_cities = df_cities.loc[df_cities[\"city_id_count\"]>=10].sort_values(\"city_id_count\",ascending=False)\n",
    "print(df_cities.nunique())\n",
    "print(df_cities.shape[0])\n",
    "df_cities\n",
    "\n",
    "# ### 5 most frequent overall\n",
    "# df_city_samples = df_cities.drop_duplicates().sort_values(\"city_id_count\",ascending=False).groupby(\"city_id\").head(5) \n",
    "# df_city_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     914.000000\n",
       "mean      158.959519\n",
       "std       229.407856\n",
       "min        40.000000\n",
       "25%        56.000000\n",
       "50%        84.000000\n",
       "75%       159.750000\n",
       "max      2226.000000\n",
       "Name: city_id_count, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cities.city_id_count.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add lag features + Train/test/data split\n",
    "* Lag feats (remember for categorical)\n",
    "* Drop leak features (target values - country, city)\n",
    "\n",
    "* drop instances  that lack history (e.g. at least 3d step and onwards) - by dropna in lag feat\n",
    "* fill nans\n",
    "* Split train/test by `user id` / split could maybe be by `utrip ID` ? ? \n",
    "    * Test - only last trip\n",
    "    *  stratified train/test split by class - then drop any train rows with overlap with tests' IDs.  \n",
    "        * Could also stratify by users, but risks some classes being non present in test\n",
    "        \n",
    "###### Big possible improvement to lag features: Have \"first location\" (starting point) \"lag\" feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### features to drop - not usable, or leaks (e.g. aggregations on target)\n",
    "\n",
    "TARGET_COL = 'city_id'\n",
    "# DROP_FEATS = ['user_id',\n",
    "#     'checkin', 'checkout',\n",
    "#               'hotel_country','city_id_count','same_country',\n",
    "#               'utrip_id',\n",
    "# #               'utrip_steps_from_end',\n",
    "#              'city_id_count','hotel_country_count',\n",
    "#               'city_id_nunique', 'hotel_country_nunique',\n",
    "#               'city_id_rank_by_hotel_country','city_id_rank_by_booker_country', 'city_id_rank_by_affiliate',\n",
    "#               'affiliate_id_rank_by_hotel_country','affiliate_id_rank_by_booker_country', 'affiliate_id_rank_by_affiliate',\n",
    "#               'hotel_country_rank_by_hotel_country',\n",
    "#        'hotel_country_rank_by_booker_country','hotel_country_rank_by_affiliate',\n",
    "#               'booker_country_rank_by_hotel_country','booker_country_rank_by_booker_country',\n",
    "#               'checkin_month_rank_by_hotel_country',\n",
    "#              ]\n",
    "\n",
    "# df2.drop(DROP_FEATS,axis=1).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "* we will want later to split also by user/`utrip_Id`...  - add that ot variables\n",
    "\n",
    "* We may want to keep the count of the targets - for explicit reccomenders (i.e frequncy of an id visiting some location) - could use to augment feature matrix, e.g. with svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID', 'device_class', 'affiliate_id', 'booker_country', 'checkin_quarter', 'last', 'city_id', 'hotel_country']\n",
      "(145035, 20)\n",
      "IDs: 6639\n",
      "drop duplicates after subset of variables shape: (78034, 8)\n",
      "unique cities: 914\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>device_class</th>\n",
       "      <th>affiliate_id</th>\n",
       "      <th>booker_country</th>\n",
       "      <th>checkin_quarter</th>\n",
       "      <th>last</th>\n",
       "      <th>city_id</th>\n",
       "      <th>hotel_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117277</th>\n",
       "      <td>09924430</td>\n",
       "      <td>0</td>\n",
       "      <td>9924</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>52933</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117278</th>\n",
       "      <td>09924430</td>\n",
       "      <td>0</td>\n",
       "      <td>9924</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>51685</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117281</th>\n",
       "      <td>09924430</td>\n",
       "      <td>0</td>\n",
       "      <td>9924</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>46411</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117283</th>\n",
       "      <td>09924431</td>\n",
       "      <td>0</td>\n",
       "      <td>9924</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>41113</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117410</th>\n",
       "      <td>03417410</td>\n",
       "      <td>0</td>\n",
       "      <td>3417</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10485</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58380</th>\n",
       "      <td>07974420</td>\n",
       "      <td>0</td>\n",
       "      <td>7974</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>31870</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98848</th>\n",
       "      <td>09924430</td>\n",
       "      <td>0</td>\n",
       "      <td>9924</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>43329</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98850</th>\n",
       "      <td>09924430</td>\n",
       "      <td>0</td>\n",
       "      <td>9924</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8359</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98854</th>\n",
       "      <td>23631440</td>\n",
       "      <td>2</td>\n",
       "      <td>3631</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>58413</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64260</th>\n",
       "      <td>25755231</td>\n",
       "      <td>2</td>\n",
       "      <td>5755</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>15470</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78034 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID  device_class  affiliate_id  booker_country  checkin_quarter  \\\n",
       "117277  09924430             0          9924               4                3   \n",
       "117278  09924430             0          9924               4                3   \n",
       "117281  09924430             0          9924               4                3   \n",
       "117283  09924431             0          9924               4                3   \n",
       "117410  03417410             0          3417               4                1   \n",
       "...          ...           ...           ...             ...              ...   \n",
       "58380   07974420             0          7974               4                2   \n",
       "98848   09924430             0          9924               4                3   \n",
       "98850   09924430             0          9924               4                3   \n",
       "98854   23631440             2          3631               4                4   \n",
       "64260   25755231             2          5755               2                3   \n",
       "\n",
       "        last  city_id  hotel_country  \n",
       "117277     0    52933            105  \n",
       "117278     0    51685            105  \n",
       "117281     0    46411            105  \n",
       "117283     1    41113            105  \n",
       "117410     0    10485             71  \n",
       "...      ...      ...            ...  \n",
       "58380      0    31870             49  \n",
       "98848      0    43329             34  \n",
       "98850      0     8359             34  \n",
       "98854      0    58413             34  \n",
       "64260      1    15470            119  \n",
       "\n",
       "[78034 rows x 8 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## n ote - we will want later to split also by user/utrip Id... \n",
    "print(MF_KEEP_COLS)\n",
    "print(df2.shape)\n",
    "df2 = df2[MF_KEEP_COLS]\n",
    "print(\"IDs:\",df2[\"ID\"].nunique())\n",
    "## for now - drop duplicates/frequency info. (could keeo as count ?)\n",
    "df2 = df2.drop_duplicates()\n",
    "print(\"drop duplicates after subset of variables shape:\",df2.shape)\n",
    "print(\"unique cities:\",df2[\"city_id\"].nunique())\n",
    "# print(\"positives targets/cities per id\\n\",df2.groupby(\"ID\"))\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    6639.000000\n",
       "mean       11.753879\n",
       "std        42.553760\n",
       "min         1.000000\n",
       "25%         1.000000\n",
       "50%         2.000000\n",
       "75%         5.000000\n",
       "max       716.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.groupby(\"ID\",observed=True).size().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cities_per_id\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    6639.000000\n",
       "mean       11.753879\n",
       "std        42.553760\n",
       "min         1.000000\n",
       "25%         1.000000\n",
       "50%         2.000000\n",
       "75%         5.000000\n",
       "max       716.000000\n",
       "Name: city_id, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities_per_id = df2.groupby(\"ID\",observed=True)[\"city_id\"].nunique() ## if we don't user observed = True, we'd get all combinations counted #.transform(\"nunique\")\n",
    "print(\"cities_per_id\")\n",
    "cities_per_id.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join DF with negatives of all possible, filtered city/county combinations\n",
    "* Note that this will yield a **LOT** of rows - with lots of sparsity (vast majority are 0)\n",
    "* Ideally - we would sample from a sparse user-item cooccurrence, or sample from scratch with every epoch. But that's hard. \n",
    "    * If sampling - we should be sure to keep the most frequent/popular overall. e.g. sample 1k + 10 most popular ? \n",
    "    \n",
    "    \n",
    "* Cartesian product ? https://stackoverflow.com/questions/13269890/cartesian-product-in-pandas\n",
    "    * https://stackoverflow.com/questions/34161978/pandas-two-dataframe-cross-join\n",
    "    \n",
    "* This would be **MUCH** more effecient with a sparse matrix (\"user-item cooccurrence\"), and we could even apply SVD directly to a spars ematrix - but it is harder to use TF/keras on such a matrix, without a special generator. \n",
    "\n",
    "* I will downsample the result randomly, keeping all positives, and downsampling negatives per \"ID\", to a given ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Any input passed containing Categorical data will have all of its categories included in the cross-tabulation, even if the actual data does not contain any instances of a particular category.\"\n",
    "* https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.crosstab.html\n",
    "\n",
    "\n",
    "USe crosstab, stack/unstack, with categorical - \n",
    "https://stackoverflow.com/questions/57385009/pandas-groupby-observed-parameter-with-multiple-categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf2.groupby([\"ID\",\"city_id\"],observed=False,dropna=False)[\"target\"].count().fillna(0).astype(int).reset_index().describe()\\n\\n# df2.groupby([\"ID\",\"city_id\"],observed=True,dropna=False)[\"target\"].count().reset_index().describe()\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "### (on subset) 2 million rows, mean target: 1.7%\n",
    "\"\"\"\n",
    "df2.groupby([\"ID\",\"city_id\"],observed=False,dropna=False)[\"target\"].count().fillna(0).astype(int).reset_index().describe()\n",
    "\n",
    "# df2.groupby([\"ID\",\"city_id\"],observed=True,dropna=False)[\"target\"].count().reset_index().describe()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            city_id        target\n",
      "count  6.068046e+06  6.068046e+06\n",
      "mean   3.290268e+04  1.285982e-02\n",
      "std    1.938140e+04  1.126697e-01\n",
      "min    3.900000e+01  0.000000e+00\n",
      "25%    1.599000e+04  0.000000e+00\n",
      "50%    3.172400e+04  0.000000e+00\n",
      "75%    4.896800e+04  0.000000e+00\n",
      "max    6.736500e+04  1.000000e+00\n",
      "Wall time: 1.87 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>city_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-2030</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0-2030</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0-2030</td>\n",
       "      <td>343</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0-2030</td>\n",
       "      <td>382</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0-2030</td>\n",
       "      <td>657</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6068041</th>\n",
       "      <td>29970441</td>\n",
       "      <td>67307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6068042</th>\n",
       "      <td>29970441</td>\n",
       "      <td>67334</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6068043</th>\n",
       "      <td>29970441</td>\n",
       "      <td>67353</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6068044</th>\n",
       "      <td>29970441</td>\n",
       "      <td>67361</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6068045</th>\n",
       "      <td>29970441</td>\n",
       "      <td>67365</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6068046 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID  city_id  target\n",
       "0          0-2030       39       0\n",
       "1          0-2030       55       0\n",
       "2          0-2030      343       0\n",
       "3          0-2030      382       0\n",
       "4          0-2030      657       0\n",
       "...           ...      ...     ...\n",
       "6068041  29970441    67307       0\n",
       "6068042  29970441    67334       0\n",
       "6068043  29970441    67353       0\n",
       "6068044  29970441    67361       0\n",
       "6068045  29970441    67365       0\n",
       "\n",
       "[6068046 rows x 3 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "### all combinations - slow, outputs all combinations, 3d column [\"0\"] is sum/count \n",
    "## solution from : https://stackoverflow.com/questions/57385009/pandas-groupby-observed-parameter-with-multiple-categoricals\n",
    "\n",
    "df_interactions = pd.crosstab(df2[\"ID\"],df2[\"city_id\"]).stack().reset_index()\n",
    "df_interactions.rename(columns={0:\"target\"},inplace=True)\n",
    "\n",
    "print(df_interactions.describe())\n",
    "# df_interactions.loc[df_interactions.iloc[:,2]>0].describe()\n",
    "df_interactions[\"target\"] = df_interactions[\"target\"].clip(upper=1)\n",
    "df_interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save intermediate results to disk + add id feats\n",
    "* all interactions (after the filtering, befoer class wise subsampling)\n",
    "* also join with categoircal feats\n",
    "\n",
    "* jion with subset of most popular cities + sample from other cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interactions = df_interactions.join(df_ID,on=\"ID\")\n",
    "df_interactions\n",
    "\n",
    "#save to disk\n",
    "if SAVE_TO_DISK:\n",
    "    df_interactions.to_csv(\"interactions_filtered_raw.csv.gz\",index=False,compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cities_top = df_cities.head(99).drop([\"city_id_count\"],axis=1) ## top K most popular cities\n",
    "df_cities_bottom = df_cities[99:].drop([\"city_id_count\"],axis=1) ## all remainign cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Subsample training data by class and group\n",
    "* Afterwards, join with individual variables (per id, and also city to country)\n",
    "\n",
    "* Note that we're taking negatives only from our target subset here, and aren't addressing frequency either...\n",
    "    * Approach would be best if we resampled many times (per epoch?)   to improve coverage \n",
    "\n",
    "* NOTE: Currently missing any split by user, or final stage in trip level! \n",
    "* May leak to train like this.. !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.286"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(100*df_interactions[\"target\"].mean(),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    6639.000000\n",
       "mean       11.753879\n",
       "std        42.553760\n",
       "min         1.000000\n",
       "25%         1.000000\n",
       "50%         2.000000\n",
       "75%         5.000000\n",
       "max       716.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_interactions.loc[df_interactions[\"target\"]==1].groupby('ID',observed=True).size().describe() #.sample(frac=.3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    6639.000000\n",
       "mean      902.246121\n",
       "std        42.553760\n",
       "min       198.000000\n",
       "25%       909.000000\n",
       "50%       912.000000\n",
       "75%       913.000000\n",
       "max       913.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_interactions.loc[df_interactions[\"target\"]==0].groupby('ID',observed=True).size().describe() #.sample(frac=.3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5990012"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_interactions.loc[df_interactions[\"target\"]==0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "NEGATIVES_DOWNSAMPLE_FRAC = 0.1\n",
    "\n",
    "### guarantee K most popular destinations are included if in negatives: \n",
    "df_neg1 = df_interactions.loc[df_interactions[\"target\"]==0].merge(df_cities_top,on=\"city_id\",how=\"inner\")\n",
    "\n",
    "### sample randomly k or % interactions per \"ID\": \n",
    "df_neg2 = df_interactions.loc[df_interactions[\"target\"]==0].merge(df_cities_bottom,on=\"city_id\",how=\"inner\")\n",
    "df_neg2 = df_neg2.groupby(\"ID\",observed=True).sample(frac=NEGATIVES_DOWNSAMPLE_FRAC)\n",
    "# df_neg2\n",
    "\n",
    "### I do not htink there is a need to add a downsample specifically for \"last\"==1 (since we keep all the positives anyway)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert df_interactions.loc[df_interactions[\"target\"]==1].isna().sum().max() ==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target mean %: 6.269\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>city_id</th>\n",
       "      <th>target</th>\n",
       "      <th>device_class</th>\n",
       "      <th>affiliate_id</th>\n",
       "      <th>booker_country</th>\n",
       "      <th>checkin_quarter</th>\n",
       "      <th>last</th>\n",
       "      <th>hotel_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>684984</th>\n",
       "      <td>02418410</td>\n",
       "      <td>65322</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2418</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891217</th>\n",
       "      <td>05243411</td>\n",
       "      <td>23798</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5243</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172255</th>\n",
       "      <td>14132130</td>\n",
       "      <td>8766</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4132</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206220</th>\n",
       "      <td>24797210</td>\n",
       "      <td>27695</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4797</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85055</th>\n",
       "      <td>01601230</td>\n",
       "      <td>382</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1601</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623948</th>\n",
       "      <td>09420330</td>\n",
       "      <td>60143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9420</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188432</th>\n",
       "      <td>03417431</td>\n",
       "      <td>10485</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3417</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293390</th>\n",
       "      <td>110332421</td>\n",
       "      <td>21328</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10332</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479041</th>\n",
       "      <td>166340</td>\n",
       "      <td>47486</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211700</th>\n",
       "      <td>210100230</td>\n",
       "      <td>14549</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10100</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1244710 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID  city_id  target  device_class  affiliate_id  \\\n",
       "684984    02418410    65322       0             0          2418   \n",
       "891217    05243411    23798       0             0          5243   \n",
       "172255    14132130     8766       0             1          4132   \n",
       "1206220   24797210    27695       0             2          4797   \n",
       "85055     01601230      382       0             0          1601   \n",
       "...            ...      ...     ...           ...           ...   \n",
       "623948    09420330    60143       0             0          9420   \n",
       "188432    03417431    10485       0             0          3417   \n",
       "293390   110332421    21328       0             1         10332   \n",
       "479041      166340    47486       0             1            66   \n",
       "211700   210100230    14549       0             2         10100   \n",
       "\n",
       "         booker_country  checkin_quarter  last  hotel_country  \n",
       "684984                4                1     0             34  \n",
       "891217                4                1     1             81  \n",
       "172255                1                3     0            109  \n",
       "1206220               2                1     0             73  \n",
       "85055                 2                3     0             49  \n",
       "...                 ...              ...   ...            ...  \n",
       "623948                3                3     0            109  \n",
       "188432                4                3     1             71  \n",
       "293390                4                2     1             34  \n",
       "479041                3                4     0             64  \n",
       "211700                2                3     0             71  \n",
       "\n",
       "[1244710 rows x 9 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat = pd.concat([df_interactions.loc[df_interactions[\"target\"]==1].merge(df_cities[[\"city_id\",\"hotel_country\"]],on=\"city_id\",how=\"inner\"),\n",
    "                    df_neg1,df_neg2],ignore_index=True).drop_duplicates().sample(frac=1)\n",
    "del df_neg1,df_neg2\n",
    "\n",
    "print(\"target mean %:\",round(100*df_feat[\"target\"].mean(),3))\n",
    "\n",
    "assert df_feat.isna().sum().max()==0\n",
    "# df_feat.dropna(how=\"any\",inplace=True,axis=0)\n",
    "assert df_feat.drop_duplicates([\"ID\",\"city_id\"]).shape[0] == df_feat.shape[0]\n",
    "\n",
    "## for some reason, affilaite id gets turned into a float, fix this: \n",
    "assert df_feat[\"affiliate_id\"].nunique() == df_feat[\"affiliate_id\"].astype(int).nunique()\n",
    "df[\"affiliate_id\"] = df[\"affiliate_id\"].astype(int)\n",
    "\n",
    "## if doing quarter as embedding instead of continous :\n",
    "# assert df_feat[\"checkin_quarter\"].nunique() == df_feat[\"checkin_quarter\"].astype(int).nunique()\n",
    "# df[\"checkin_quarter\"] = df[\"checkin_quarter\"].astype(int)\n",
    "\n",
    "### save intermediate result to disk\n",
    "if SAVE_TO_DISK:\n",
    "    df_feat.to_csv(\"interactions_filtered_train.csv.gz\",index=False,compression=\"gzip\")\n",
    "\n",
    "df_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropping ID\n"
     ]
    }
   ],
   "source": [
    "print(\"dropping ID\")\n",
    "df_feat.drop(\"ID\",axis=1,inplace=True,errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### city ID encoder & other features ordinal encoder\n",
    "* Encode city id into integer range with label_encoder\n",
    "* This will make embedding more effecient \n",
    "* IMPORTANT: We will need to map back using this encoder, in order to get original IDs. \n",
    "\n",
    "* We do it at this step, since we've dropped so many targets beforehand. We could do this as first step in pipeline as well ! \n",
    "\n",
    "* I do this also for the other categorical variables. For them we don't save the transformer since we don't care about reversing ? \n",
    "     * **WARNING**: Check this doesn't mess things when predicting on new data !!!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAT_COLS_LIST = ['device_class', 'affiliate_id', 'booker_country']\n",
    "\n",
    "# print(df_feat[\"city_id\"].min())\n",
    "# df_feat[\"city_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_encoder = LabelEncoder().fit(df_feat[\"city_id\"])\n",
    "country_encoder = LabelEncoder().fit(df_feat[\"hotel_country\"]) \n",
    "\n",
    "df_feat[\"city_id\"] = city_encoder.transform(df_feat[\"city_id\"])\n",
    "df_feat[\"hotel_country\"] = country_encoder.transform(df_feat[\"hotel_country\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       device_class  affiliate_id  booker_country\n",
      "count  1.244710e+06  1.244710e+06    1.244710e+06\n",
      "mean   5.203574e-01  5.187693e+03    2.242218e+00\n",
      "std    7.222074e-01  3.247364e+03    1.197902e+00\n",
      "min    0.000000e+00 -2.000000e+00    0.000000e+00\n",
      "25%    0.000000e+00  2.436000e+03    1.000000e+00\n",
      "50%    0.000000e+00  4.886000e+03    2.000000e+00\n",
      "75%    1.000000e+00  8.072000e+03    3.000000e+00\n",
      "max    2.000000e+00  1.069700e+04    4.000000e+00\n",
      "       device_class  affiliate_id  booker_country\n",
      "count  1.244710e+06  1.244710e+06    1.244710e+06\n",
      "mean   5.203574e-01  4.877547e+02    2.242218e+00\n",
      "std    7.222074e-01  2.931817e+02    1.197902e+00\n",
      "min    0.000000e+00  0.000000e+00    0.000000e+00\n",
      "25%    0.000000e+00  2.360000e+02    1.000000e+00\n",
      "50%    0.000000e+00  4.700000e+02    2.000000e+00\n",
      "75%    1.000000e+00  7.500000e+02    3.000000e+00\n",
      "max    2.000000e+00  9.790000e+02    4.000000e+00\n"
     ]
    }
   ],
   "source": [
    "print(df_feat[CAT_COLS_LIST].describe())\n",
    "categories_ordinal_encoder = OrdinalEncoder(dtype=np.int64)\n",
    "categories_ordinal_encoder.fit(df_feat[CAT_COLS_LIST])\n",
    "df_feat[CAT_COLS_LIST] = categories_ordinal_encoder.transform(df_feat[CAT_COLS_LIST])\n",
    "print(df_feat[CAT_COLS_LIST].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Artifical train/test split grouping \n",
    "\n",
    "* Make additional grouping for train/test split.\n",
    "WARNING: This is NOT as good/reliable as splitting by USER_ID or utripID, but we do this for now, since we don't have that variable here\n",
    "* This splitting would result in some variables not being learned ? \n",
    "\n",
    "* Ignore some of the ID columns for this groups creation\n",
    "    * warning - manua/fragile/may break if columns used change\n",
    "    \n",
    "This splitting is **not reliable**!! It is just to try to approximate something like splitting across trips\n",
    "\n",
    "* Mainly - groups should include rows with last 0/1 for same trip\n",
    "    * How bad is it that we group/exclude by city_id (for a specific grouping)  ?? \n",
    "    * Alt - could do group also with lag1_city_id ? (If using that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['city_id', 'target', 'device_class', 'affiliate_id', 'booker_country',\n",
       "       'checkin_quarter', 'last', 'hotel_country'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006342\n"
     ]
    }
   ],
   "source": [
    "df_feat[\"Group_id\"] = df_feat[['city_id', 'device_class', 'affiliate_id', 'booker_country',\n",
    "       'checkin_quarter']].astype(str).sum(1)\n",
    "print(df_feat[\"Group_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 933721 valid 124624 test 68058\n"
     ]
    }
   ],
   "source": [
    "train_inds, test_inds = next(GroupShuffleSplit(test_size=.25, n_splits=2, random_state = 7).split(df_feat, groups=df_feat['Group_id']))\n",
    "\n",
    "train = df_feat.iloc[train_inds].drop(\"Group_id\",axis=1)\n",
    "test = df_feat.iloc[test_inds]\n",
    "\n",
    "## split test into validation and final test (only last stage of trip). We could also spliut by group here as well\n",
    "\n",
    "valid_inds, test_inds  = next(GroupShuffleSplit(test_size=.6, n_splits=2, random_state = 7).split(test, groups=test['Group_id']))\n",
    "valid = test.iloc[valid_inds].drop(\"Group_id\",axis=1)\n",
    "test = test.iloc[test_inds].drop(\"Group_id\",axis=1)\n",
    "test = test.loc[test[\"last\"]==1]\n",
    "\n",
    "print(\"train\",train.shape[0],\"valid\",valid.shape[0],\"test\",test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_id</th>\n",
       "      <th>target</th>\n",
       "      <th>device_class</th>\n",
       "      <th>affiliate_id</th>\n",
       "      <th>booker_country</th>\n",
       "      <th>checkin_quarter</th>\n",
       "      <th>last</th>\n",
       "      <th>hotel_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>172255</th>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>398</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823241</th>\n",
       "      <td>721</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>343</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762024</th>\n",
       "      <td>863</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>183</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650046</th>\n",
       "      <td>840</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077487</th>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>321</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623948</th>\n",
       "      <td>817</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>854</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188432</th>\n",
       "      <td>143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>339</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293390</th>\n",
       "      <td>298</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>948</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479041</th>\n",
       "      <td>664</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211700</th>\n",
       "      <td>202</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>930</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>933721 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         city_id  target  device_class  affiliate_id  booker_country  \\\n",
       "172255       121       0             1           398               1   \n",
       "823241       721       0             0           343               3   \n",
       "762024       863       0             0           183               1   \n",
       "650046       840       0             1            30               1   \n",
       "1077487      264       0             1           321               2   \n",
       "...          ...     ...           ...           ...             ...   \n",
       "623948       817       0             0           854               3   \n",
       "188432       143       0             0           339               4   \n",
       "293390       298       0             1           948               4   \n",
       "479041       664       0             1             6               3   \n",
       "211700       202       0             2           930               2   \n",
       "\n",
       "         checkin_quarter  last  hotel_country  \n",
       "172255                 3     0             53  \n",
       "823241                 4     0              2  \n",
       "762024                 3     1             63  \n",
       "650046                 1     0              5  \n",
       "1077487                4     0             27  \n",
       "...                  ...   ...            ...  \n",
       "623948                 3     0             53  \n",
       "188432                 3     1             34  \n",
       "293390                 2     1             20  \n",
       "479041                 4     0             31  \n",
       "211700                 3     0             34  \n",
       "\n",
       "[933721 rows x 8 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    933721.000000\n",
       "mean        460.103505\n",
       "std         271.148202\n",
       "min           0.000000\n",
       "25%         234.000000\n",
       "50%         448.000000\n",
       "75%         705.000000\n",
       "max         913.000000\n",
       "Name: city_id, dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"city_id\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df2.shape)\n",
    "# # ### lag features - last n visits\n",
    "# df_feat = groupbyLagFeatures(df=df2.copy(), \n",
    "#                    lag=[1],group=\"utrip_id\",lag_feature_cols=LAG_FEAT_COLS)\n",
    "# df_feat = df_feat.dropna(subset=[\"lag3_city_id\"]).sample(frac=1)\n",
    "\n",
    "### filter for only trip targets that are among the K most popular :\n",
    "\n",
    "# df_feat = df_feat.drop(DROP_FEATS,axis=1,errors=\"ignore\")\n",
    "# print(df_feat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "### filter for most frequent targets\n",
    "\n",
    "if KEEP_TOP_K_TARGETS > 0 :\n",
    "    print(df_feat.shape[0])\n",
    "    df_feat = df_feat.loc[df_feat[\"city_id\"].isin(TOP_TARGETS)]\n",
    "    print(df_feat.shape[0])    \n",
    "    assert df_feat[\"city_id\"].nunique() == KEEP_TOP_K_TARGETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########################\n",
    "# ## stratified train/test split by class - then drop any train rows with overlap wit htest IDs.  Could also stratify by users, but risks some classes being non present in test\n",
    "# ### split could maybe be by utrip ID ? \n",
    "# ### orig - split by group : \n",
    "\n",
    "# # train_inds, test_inds = next(GroupShuffleSplit(test_size=.2, n_splits=2, random_state = 7).split(df_feat, groups=df_feat['user_id']))\n",
    "# # X_train = df_feat.iloc[train_inds].drop(DROP_FEATS,axis=1,errors=\"ignore\")\n",
    "# # X_test = df_feat.iloc[test_inds].drop(DROP_FEATS,axis=1,errors=\"ignore\")\n",
    "# # assert (set(X_train[TARGET_COL].unique()) == set(X_test[TARGET_COL].unique()))\n",
    "# #################\n",
    "# ## alt: split by class. May be leaky! \n",
    "# X_train, X_test = train_test_split(df_feat,stratify=df_feat[TARGET_COL])\n",
    "\n",
    "# ########################\n",
    "# print(\"X_train\",X_train.shape[0])\n",
    "# ## get last row in trip only in test/eval set: \n",
    "# print(\"X_test\",X_test.shape[0])\n",
    "\n",
    "# ### following is for splitting by group - can't do so currebntly as group/user id col is missing\n",
    "# # X_test = X_test.loc[X_test[\"utrip_steps_from_end\"]==1] # last row per trip\n",
    "# # print(\"X_test after filtering for last instance per trip\",X_test.shape[0])\n",
    "\n",
    "# # y_train = X_train.pop(TARGET_COL)\n",
    "# # y_test = X_test.pop(TARGET_COL)\n",
    "\n",
    "# # print(\"# classes\",y_train.nunique())\n",
    "\n",
    "# # # ## check that same classes in train and test - \n",
    "# # # assert (set(y_train.unique()) == set(y_test.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "* For now - simple multiclass model (Tabnet? LSTM?) ; +- subsample - only most frequent classes/cities\n",
    "\n",
    "    * Tabnet: `pip install pytorch-tabnet`\n",
    "        * https://github.com/dreamquark-ai/tabnet/blob/develop/forest_example.ipynb\n",
    "    * TensorFlow Tabmet: https://github.com/ostamand/tensorflow-tabnet/blob/master/examples/train_mnist.py\n",
    "\n",
    "* split train/test by groups\n",
    "    * Test - only last trip. \n",
    "\n",
    "\n",
    "* Preprocess for TF/keras + dot product, emebddings etc'\n",
    "\n",
    "* categoircal + embedding example : \n",
    "    * https://www.tensorflow.org/tutorials/structured_data/feature_columns\n",
    "    * https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_identity\n",
    "    \n",
    "    \n",
    "* Shared embeddings (for use of lag1 city, country id : https://www.tensorflow.org/api_docs/python/tf/feature_column/shared_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add tf feature columns\n",
    "* https://www.tensorflow.org/tutorials/structured_data/feature_columns#choose_which_columns_to_use\n",
    "* Assumes use of a tf.data pipeline  (`df_to_dataset`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ({city_id: (None,), device_class: (None,), affiliate_id: (None,), booker_country: (None,), checkin_quarter: (None,), last: (None,), hotel_country: (None,)}, (None,)), types: ({city_id: tf.int64, device_class: tf.int64, affiliate_id: tf.int64, booker_country: tf.int64, checkin_quarter: tf.int64, last: tf.int32, hotel_country: tf.int64}, tf.int64)>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = df_to_dataset(train, shuffle=False, batch_size=256)\n",
    "val_ds = df_to_dataset(valid, shuffle=False, batch_size=512)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=512)\n",
    "val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['device_class', 'affiliate_id', 'booker_country', 'city_id', 'hotel_country']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CAT_COLS_LIST + [\"city_id\",\"hotel_country\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_class\n",
      "affiliate_id\n",
      "booker_country\n",
      "city_id\n",
      "hotel_country\n"
     ]
    }
   ],
   "source": [
    "feature_columns = []\n",
    "\n",
    "# numeric cols\n",
    "for header in ['checkin_quarter',\"last\"]:\n",
    "    feature_columns.append(feature_column.numeric_column(header))\n",
    "    \n",
    "# Categorical cols (\"generic\") & embedding (We could use strings here instead of numbers - https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_list\n",
    "# embedding columns\n",
    "for col_name in CAT_COLS_LIST + [\"city_id\",\"hotel_country\"]:\n",
    "    print(col_name)\n",
    "    # breed1 = feature_column.categorical_column_with_vocabulary_list(\n",
    "    #       'Breed1', dataframe.Breed1.unique())\n",
    "    c_uniques = df_feat[col_name].nunique()\n",
    "    categorical_column = feature_column.categorical_column_with_identity(\n",
    "        col_name, c_uniques)\n",
    "    col_embedding = feature_column.embedding_column(categorical_column, dimension=min(c_uniques//2,50))\n",
    "    feature_columns.append(col_embedding)\n",
    "    \n",
    "\n",
    "# # indicator_columns - do one hot encoding - maybe instead of embedding ?? \n",
    "indicator_column_names = [\"device_class\"] ## \"booker_country\",\n",
    "for col_name in indicator_column_names:\n",
    "#     categorical_column = feature_column.categorical_column_with_vocabulary_list(\n",
    "#         col_name, dataframe[col_name].unique())\n",
    "    categorical_column = feature_column.categorical_column_with_identity(\n",
    "        col_name, df_feat[col_name].nunique())\n",
    "    \n",
    "    indicator_column = feature_column.indicator_column(categorical_column)\n",
    "    feature_columns.append(indicator_column)\n",
    "\n",
    "    \n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_id</th>\n",
       "      <th>target</th>\n",
       "      <th>device_class</th>\n",
       "      <th>affiliate_id</th>\n",
       "      <th>booker_country</th>\n",
       "      <th>checkin_quarter</th>\n",
       "      <th>last</th>\n",
       "      <th>hotel_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1206220</th>\n",
       "      <td>399</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>460</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85055</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>167</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911040</th>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>566</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1080541</th>\n",
       "      <td>656</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>346</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174923</th>\n",
       "      <td>130</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>218</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523497</th>\n",
       "      <td>690</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>760</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169509</th>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>433</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1162911</th>\n",
       "      <td>795</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>874</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260908</th>\n",
       "      <td>262</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>913</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813022</th>\n",
       "      <td>553</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>316</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124624 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         city_id  target  device_class  affiliate_id  booker_country  \\\n",
       "1206220      399       0             2           460               2   \n",
       "85055          3       0             0           167               2   \n",
       "911040        88       0             0           566               4   \n",
       "1080541      656       0             1           346               2   \n",
       "174923       130       0             0           218               4   \n",
       "...          ...     ...           ...           ...             ...   \n",
       "523497       690       0             1           760               3   \n",
       "169509       121       0             0           433               2   \n",
       "1162911      795       0             1           874               2   \n",
       "260908       262       0             0           913               4   \n",
       "813022       553       0             0           316               2   \n",
       "\n",
       "         checkin_quarter  last  hotel_country  \n",
       "1206220                1     0             36  \n",
       "85055                  3     0             25  \n",
       "911040                 4     1             33  \n",
       "1080541                3     1             18  \n",
       "174923                 2     1             34  \n",
       "...                  ...   ...            ...  \n",
       "523497                 1     0             58  \n",
       "169509                 3     0             53  \n",
       "1162911                1     1             20  \n",
       "260908                 3     1              7  \n",
       "813022                 3     1             28  \n",
       "\n",
       "[124624 rows x 8 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NumericColumn(key='checkin_quarter', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='last', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " EmbeddingColumn(categorical_column=IdentityCategoricalColumn(key='device_class', number_buckets=3, default_value=None), dimension=1, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x000002A99D806808>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True, use_safe_embedding_lookup=True),\n",
       " EmbeddingColumn(categorical_column=IdentityCategoricalColumn(key='affiliate_id', number_buckets=980, default_value=None), dimension=50, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x000002A99D89F6C8>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True, use_safe_embedding_lookup=True),\n",
       " EmbeddingColumn(categorical_column=IdentityCategoricalColumn(key='booker_country', number_buckets=5, default_value=None), dimension=2, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x000002A992B38C48>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True, use_safe_embedding_lookup=True),\n",
       " EmbeddingColumn(categorical_column=IdentityCategoricalColumn(key='city_id', number_buckets=914, default_value=None), dimension=50, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x000002A99D89F088>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True, use_safe_embedding_lookup=True),\n",
       " EmbeddingColumn(categorical_column=IdentityCategoricalColumn(key='hotel_country', number_buckets=78, default_value=None), dimension=39, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x000002A99D89F948>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True, use_safe_embedding_lookup=True),\n",
       " IndicatorColumn(categorical_column=IdentityCategoricalColumn(key='device_class', number_buckets=3, default_value=None))]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Todo : how to get dot product(s) between the inputs? \n",
    "\n",
    "\n",
    "https://stackoverflow.com/questions/54375298/how-to-use-tensorflow-feature-columns-as-input-to-a-keras-model\n",
    "\n",
    "https://towardsdatascience.com/how-to-build-a-wide-and-deep-model-using-keras-in-tensorflow-2-0-2f7a236b5a4b\n",
    "\n",
    "* https://www.tensorflow.org/tutorials/load_data/csv#mixed_data_types\n",
    "\n",
    "* https://www.kaggle.com/blaskowitz100/dnn-keras-and-categorical-feature-embedding  - without using `tf.feature_column`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set output bias for better initial guesses / class imbalance : \n",
    "## https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#optional_set_the_correct_initial_bias\n",
    "output_bias = tf.keras.initializers.Constant(np.log([train[\"target\"].mean()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FC model, without the dot product\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "#   feature_layer,\n",
    "# #   layers.Dense(128, activation='elu'),\n",
    "# #     layers.Dropout(.2),\n",
    "#   layers.Dense(64, activation='relu'),\n",
    "#   layers.Dropout(.1),\n",
    "#   layers.Dense(1, activation='sigmoid',bias_initializer=output_bias)\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss=\"binary_crossentropy\", #tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#               metrics=['accuracy',\"AUC\",\"Recall\",\"Precision\"]) # 'accuracy', ,tf.keras.metrics.TopKCategoricalAccuracy(4) - not used in ranking\n",
    "\n",
    "\n",
    "# model.fit(train_ds,\n",
    "#           validation_data=val_ds,\n",
    "#           epochs=max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "### https://towardsdatascience.com/how-to-build-a-wide-and-deep-model-using-keras-in-tensorflow-2-0-2f7a236b5a4b\n",
    "\n",
    "# feature_layer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'city_id': array([399,   3,  88, ..., 795, 262, 553], dtype=int64),\n",
       " 'target': array([0, 0, 0, ..., 0, 0, 0], dtype=int64),\n",
       " 'device_class': array([2, 0, 0, ..., 1, 0, 0], dtype=int64),\n",
       " 'affiliate_id': array([460, 167, 566, ..., 874, 913, 316], dtype=int64),\n",
       " 'booker_country': array([2, 2, 4, ..., 2, 4, 2], dtype=int64),\n",
       " 'checkin_quarter': array([1, 3, 4, ..., 1, 3, 3], dtype=int64),\n",
       " 'last': array([0, 0, 1, ..., 1, 1, 1]),\n",
       " 'hotel_country': array([36, 25, 33, ..., 20,  7, 28], dtype=int64)}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.kaggle.com/blaskowitz100/dnn-keras-and-categorical-feature-embedding\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def get_keras_dataset(df):\n",
    "    X = {str(col) : np.array(df[col]) for col in df.columns}\n",
    "    return X\n",
    "\n",
    "\n",
    "get_keras_dataset(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_id</th>\n",
       "      <th>target</th>\n",
       "      <th>device_class</th>\n",
       "      <th>affiliate_id</th>\n",
       "      <th>booker_country</th>\n",
       "      <th>checkin_quarter</th>\n",
       "      <th>last</th>\n",
       "      <th>hotel_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>172255</th>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>398</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823241</th>\n",
       "      <td>721</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>343</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762024</th>\n",
       "      <td>863</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>183</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650046</th>\n",
       "      <td>840</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077487</th>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>321</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623948</th>\n",
       "      <td>817</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>854</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188432</th>\n",
       "      <td>143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>339</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293390</th>\n",
       "      <td>298</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>948</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479041</th>\n",
       "      <td>664</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211700</th>\n",
       "      <td>202</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>930</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>933721 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         city_id  target  device_class  affiliate_id  booker_country  \\\n",
       "172255       121       0             1           398               1   \n",
       "823241       721       0             0           343               3   \n",
       "762024       863       0             0           183               1   \n",
       "650046       840       0             1            30               1   \n",
       "1077487      264       0             1           321               2   \n",
       "...          ...     ...           ...           ...             ...   \n",
       "623948       817       0             0           854               3   \n",
       "188432       143       0             0           339               4   \n",
       "293390       298       0             1           948               4   \n",
       "479041       664       0             1             6               3   \n",
       "211700       202       0             2           930               2   \n",
       "\n",
       "         checkin_quarter  last  hotel_country  \n",
       "172255                 3     0             53  \n",
       "823241                 4     0              2  \n",
       "762024                 3     1             63  \n",
       "650046                 1     0              5  \n",
       "1077487                4     0             27  \n",
       "...                  ...   ...            ...  \n",
       "623948                 3     0             53  \n",
       "188432                 3     1             34  \n",
       "293390                 2     1             20  \n",
       "479041                 4     0             31  \n",
       "211700                 3     0             34  \n",
       "\n",
       "[933721 rows x 8 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['city_id', 'target', 'device_class', 'affiliate_id', 'booker_country',\n",
       "       'checkin_quarter', 'last', 'hotel_country'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "city_id               457\n",
       "target                  1\n",
       "device_class            2\n",
       "affiliate_id          490\n",
       "booker_country          3\n",
       "checkin_quarter         2\n",
       "last                    1\n",
       "hotel_country          39\n",
       "Group_id           503171\n",
       "dtype: int64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_feat.nunique()+1)//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "device_class (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "booker_country (InputLayer)     [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "checkin_quarter (InputLayer)    [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "affiliate_id (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "city_id (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hotel_country (InputLayer)      [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "device_class_emb (Embedding)    (None, 1, 2)         6           device_class[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "booker_country_emb (Embedding)  (None, 1, 3)         15          booker_country[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "checkin_quarter_emb (Embedding) (None, 1, 2)         10          checkin_quarter[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "affiliate_id_emb (Embedding)    (None, 1, 50)        49000       affiliate_id[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "city_id_emb (Embedding)         (None, 1, 57)        52098       city_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "hotel_country_emb (Embedding)   (None, 1, 7)         546         hotel_country[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_43 (Concatenate)    (None, 1, 57)        0           device_class_emb[0][0]           \n",
      "                                                                 booker_country_emb[0][0]         \n",
      "                                                                 checkin_quarter_emb[0][0]        \n",
      "                                                                 affiliate_id_emb[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_44 (Concatenate)    (None, 1, 7)         0           device_class_emb[0][0]           \n",
      "                                                                 booker_country_emb[0][0]         \n",
      "                                                                 checkin_quarter_emb[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_45 (Concatenate)    (None, 1, 121)       0           city_id_emb[0][0]                \n",
      "                                                                 hotel_country_emb[0][0]          \n",
      "                                                                 device_class_emb[0][0]           \n",
      "                                                                 booker_country_emb[0][0]         \n",
      "                                                                 checkin_quarter_emb[0][0]        \n",
      "                                                                 affiliate_id_emb[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dot_26 (Dot)                    (None, 1, 1)         0           city_id_emb[0][0]                \n",
      "                                                                 concatenate_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dot_27 (Dot)                    (None, 1, 1)         0           hotel_country_emb[0][0]          \n",
      "                                                                 concatenate_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_46 (Concatenate)    (None, 1, 123)       0           concatenate_45[0][0]             \n",
      "                                                                 dot_26[0][0]                     \n",
      "                                                                 dot_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 1, 128)       15872       concatenate_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 1, 128)       0           dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 1, 64)        8256        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 1, 64)        0           dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "last (InputLayer)               [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 1, 1)         65          dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 125,868\n",
      "Trainable params: 125,868\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/4\n",
      "3648/3648 [==============================] - 15s 4ms/step - loss: 0.1555 - accuracy: 0.9479 - auc: 0.8758 - val_loss: 0.1264 - val_accuracy: 0.9578 - val_auc: 0.9223\n",
      "Epoch 2/4\n",
      "3648/3648 [==============================] - 12s 3ms/step - loss: 0.1255 - accuracy: 0.9570 - auc: 0.9276 - val_loss: 0.1257 - val_accuracy: 0.9579 - val_auc: 0.9239\n",
      "Epoch 3/4\n",
      "3648/3648 [==============================] - 12s 3ms/step - loss: 0.1193 - accuracy: 0.9584 - auc: 0.9376 - val_loss: 0.1294 - val_accuracy: 0.9573 - val_auc: 0.9199\n",
      "Epoch 4/4\n",
      "3648/3648 [==============================] - 13s 3ms/step - loss: 0.1128 - accuracy: 0.9595 - auc: 0.9477 - val_loss: 0.1352 - val_accuracy: 0.9574 - val_auc: 0.9151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2a9a37245c8>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## based on:  https://www.kaggle.com/blaskowitz100/dnn-keras-and-categorical-feature-embedding\n",
    "cat_inputs = []\n",
    "num_inputs = []\n",
    "embeddings = []\n",
    "embedding_layer_names = []\n",
    "# emb_n = 57\n",
    "CITY_EMB_DIM = 57 # dimensions for hotel_country embedding - limited by user level variables\n",
    "HOTEL_COUNTRY_EMB_DIM = 7 # dimensions for hotel_country embedding - limited by subset of user level variables\n",
    "\n",
    "numeric_cols = [\"last\"] ## \"checkin_quarter\"\n",
    "\n",
    "categorical_cols_user = ['device_class', 'booker_country',\"checkin_quarter\", 'affiliate_id'] # , 'affiliate_id'\n",
    "categorical_cols_item = ['city_id', 'hotel_country']\n",
    "categorical_cols = categorical_cols_item + categorical_cols_user \n",
    "\n",
    "\n",
    "## Embedding for categorical, \"X\" features (vs \"targets\") - do items first for easy selection. Do twice, to control embed sizes for dot product\n",
    "for col in categorical_cols_item[0:1]: # city only\n",
    "    _input = layers.Input(shape=[1], name=col)    \n",
    "    c_uniques = df_feat[col].nunique()\n",
    "    _embed = layers.Embedding(df_feat[col].max() + 1, CITY_EMB_DIM, name=col+'_emb')(_input)\n",
    "    cat_inputs.append(_input)\n",
    "    embeddings.append(_embed) \n",
    "    embedding_layer_names.append(col+'_emb')\n",
    "for col in categorical_cols_item[1:2]: # hotel_country only only\n",
    "    _input = layers.Input(shape=[1], name=col)    \n",
    "    c_uniques = df_feat[col].nunique()\n",
    "    _embed = layers.Embedding(df_feat[col].max() + 1, HOTEL_COUNTRY_EMB_DIM, name=col+'_emb')(_input)\n",
    "    cat_inputs.append(_input)\n",
    "    embeddings.append(_embed) \n",
    "    embedding_layer_names.append(col+'_emb')\n",
    "    \n",
    "    \n",
    "for col in categorical_cols_user:\n",
    "    _input = layers.Input(shape=[1], name=col)    \n",
    "    c_uniques = df_feat[col].nunique()\n",
    "    _embed = layers.Embedding(df_feat[col].max() + 1, min((c_uniques+1)//2,50), name=col+'_emb')(_input)\n",
    "    cat_inputs.append(_input)\n",
    "    embeddings.append(_embed) \n",
    "    embedding_layer_names.append(col+'_emb')## doesn't seem to be used? \n",
    "    \n",
    "# Simple inputs for the numeric features\n",
    "for col in numeric_cols:\n",
    "    numeric_input = layers.Input(shape=(1,), name=col)\n",
    "    num_inputs.append(numeric_input)\n",
    "   \n",
    "    \n",
    "# # Merge the numeric inputs\n",
    "# merged_num_inputs = layers.concatenate(num_inputs)\n",
    "\n",
    "#numeric_dense = layers.Dense(20, activation='relu')(merged_num_inputs)\n",
    "\n",
    "### try to get just user, and just item embeddings cincated seperately. Based on input order\n",
    "\n",
    "# target_inputs =  layers.concatenate(embeddings[0:2])\n",
    "\n",
    "### TODO - concat with numerics also\n",
    "user_inputs =  layers.concatenate(embeddings[2:])\n",
    "# user_inputs =  layers.concatenate([embeddings[2:],num_inputs]) ### I currently couldn't get the concat working!! \n",
    "\n",
    "# dot_score = layers.Dot(axes=(2))([target_inputs,user_inputs]) ## meantto be used on all users X all item (city+country)\n",
    "\n",
    "dot_score_city1 = layers.Dot(axes=(2),normalize=False)([embeddings[0],user_inputs])\n",
    "dot_score_country1 = layers.Dot(axes=(2),normalize=False)([embeddings[1],layers.concatenate(embeddings[2:-1])])\n",
    "\n",
    "# # Merge embedding and use a Droput to prevent overfittting\n",
    "merged_inputs = layers.concatenate(embeddings)\n",
    "# # spatial_dropout = layers.SpatialDropout1D(0.2)(merged_inputs)\n",
    "# flat_embed = layers.Flatten()(merged_inputs)#(spatial_dropout)\n",
    "\n",
    "# # Merge embedding and numeric features\n",
    "# all_features = layers.concatenate([flat_embed, merged_num_inputs])\n",
    "\n",
    "# all_features = layers.concatenate([dot_score, merged_num_inputs])\n",
    "\n",
    "# x = layers.concatenate([dot_score_city1,dot_score_country1]) # for just dot product model\n",
    "x = layers.concatenate([merged_inputs,dot_score_city1,dot_score_country1])\n",
    "\n",
    "# MLP for classification\n",
    "# x = layers.Dropout(0.2)(layers.Dense(128, activation='relu')(all_features))\n",
    "x = layers.Dropout(0.1)(layers.Dense(128, activation='relu')(x))\n",
    "x = layers.Dropout(0.1)(layers.Dense(64, activation='relu')(x))\n",
    "\n",
    "# Final model\n",
    "output = layers.Dense(1, activation='sigmoid',bias_initializer=output_bias)(x)\n",
    "model = models.Model(inputs=cat_inputs + num_inputs, outputs=output)\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss=\"binary_crossentropy\", \n",
    "              metrics=['accuracy',\"AUC\"]) #,\"Precision\"\"Recall\", 'accuracy', ,tf.keras.metrics.TopKCategoricalAccuracy(4) - not used in ranking\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(train_ds,\n",
    "          validation_data=val_ds,\n",
    "          epochs=max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After 4 epochs (1.23k sampled data - only dot product model + embeddings (no FC), 4 epochs\n",
    "    * loss: 0.1380 - accuracy: 0.9503 - auc: 0.9185 - val_loss: 0.1426 - val_accuracy: 0.9492 - val_auc: 0.9104\n",
    "    \n",
    "* Above + sigmoid bias, + 128 elu FC last layer, 20% dropout: \n",
    "    * 12s 3ms/step - loss: 0.1255 - accuracy: 0.9539 - auc: 0.9362 - val_loss: 0.1421 - val_accuracy: 0.9519 - val_auc: 0.9084\n",
    "* Above + sigmoid bias, + 64 + 128 elu FC last layer, 10% dropout:     \n",
    "    * 13s 4ms/step - loss: 0.1150 - accuracy: 0.9586 - auc: 0.9456 - val_loss: 0.1409 - val_accuracy: 0.9540 - val_auc: 0.9078\n",
    "* (directly) above, + relu (instead of elu):\n",
    "    * 14s 4ms/step - loss: 0.1100 - accuracy: 0.9601 - auc: 0.9515 - val_loss: 0.1367 - val_accuracy: 0.9568 - val_auc: 0.9152\n",
    "    \n",
    "* (directly) above,(relu) + normalize Dot product:\n",
    "    * 13s 4ms/step - loss: 0.1060 - accuracy: 0.9605 - auc: 0.9564 - val_loss: 0.1372 - val_accuracy: 0.9564 - val_auc: 0.9119\n",
    "    * Normalizing seems to be the same or overfit by an iota? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 1, 57) dtype=float32 (created by layer 'concatenate_19')>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 1, 57) dtype=float32 (created by layer 'city_id_emb')>,\n",
       " <KerasTensor: shape=(None, 1, 7) dtype=float32 (created by layer 'hotel_country_emb')>,\n",
       " <KerasTensor: shape=(None, 1, 2) dtype=float32 (created by layer 'device_class_emb')>,\n",
       " <KerasTensor: shape=(None, 1, 3) dtype=float32 (created by layer 'booker_country_emb')>,\n",
       " <KerasTensor: shape=(None, 1, 2) dtype=float32 (created by layer 'checkin_quarter_emb')>,\n",
       " <KerasTensor: shape=(None, 1, 50) dtype=float32 (created by layer 'affiliate_id_emb')>]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 1, 57) dtype=float32 (created by layer 'city_id_emb')>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 1, 57) dtype=float32 (created by layer 'concatenate_19')>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 1, 2) dtype=float32 (created by layer 'device_class_emb')>,\n",
       " <KerasTensor: shape=(None, 1, 3) dtype=float32 (created by layer 'booker_country_emb')>,\n",
       " <KerasTensor: shape=(None, 1, 2) dtype=float32 (created by layer 'checkin_quarter_emb')>,\n",
       " <KerasTensor: shape=(None, 1, 50) dtype=float32 (created by layer 'affiliate_id_emb')>]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'last')>]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation: \n",
    "* don't enable shuffle on the tf_ds, or results will be scrambled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9108393699884039\n"
     ]
    }
   ],
   "source": [
    "# y_val_preds_proba = model.predict(val_ds)\n",
    "# print(roc_auc_score(y_true = valid[\"target\"],y_score=y_val_preds_proba[:,0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.48\n"
     ]
    }
   ],
   "source": [
    "y_test_preds_proba = model.predict(test_ds)\n",
    "print(round(100*roc_auc_score(y_true = test[\"target\"],y_score=y_test_preds_proba[:,0,0]),2))\n",
    "\n",
    "# classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAT_FEAT_NAMES = [\"booker_country\", \"device_class\",\"affiliate_id\",\n",
    "# #                   \"user_id\", ## ? could use lower dim - depends on train/test overlap\n",
    "#                   \"checkin_week\",\"checkout_week\",\n",
    "#                     \"checkin_weekday\",\n",
    "#     \"lag1_city_id\",\"lag1_booker_country\",\"lag1_hotel_country\",\"lag1_affiliate_id\", \"lag1_device_class\",\n",
    "#      \"lag2_city_id\",\"lag2_booker_country\",\"lag2_hotel_country\",\"lag2_affiliate_id\",\"lag2_device_class\",\n",
    "#       \"lag3_city_id\",\"lag3_booker_country\",\"lag3_hotel_country\",\"lag3_affiliate_id\",\"lag3_device_class\",\n",
    "#                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NUMERIC_COLS = [item for item in list(df_feat.columns.drop(TARGET_COL))  if item not in CAT_FEAT_NAMES]\n",
    "# print(len(NUMERIC_COLS))\n",
    "# print(\"numeric cols\",NUMERIC_COLS)\n",
    "\n",
    "# for c in NUMERIC_COLS:\n",
    "#     l_enc =   StandardScaler() # MinMaxScaler()#\n",
    "#     l_enc.fit(df_feat[c].values.reshape(-1,1))\n",
    "#     X_train[c] = l_enc.transform(X_train[c].values.reshape(-1,1))\n",
    "#     X_test[c] = l_enc.transform(X_test[c].values.reshape(-1,1))\n",
    "    \n",
    "# for c in CAT_FEAT_NAMES:\n",
    "#     l_enc = LabelEncoder().fit(df_feat[c])\n",
    "#     X_train[c] = l_enc.transform(X_train[c])\n",
    "#     X_test[c] = l_enc.transform(X_test[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(\"sum top4 total percentage:\",y_train.value_counts(normalize=True)[0:4].sum().round(3))\n",
    "# y_train.value_counts(normalize=True).round(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature importance & evaluation\n",
    "* Look for leaks!\n",
    "* May be bug with ordering of results - evaluation doesn't make sense. Note that diff # outputs/classes, likely culprit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"y_test nunique classes\",y_test.nunique())\n",
    "# y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
