{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate features on all test + train and get predictions\n",
    "* Todo: seperate feature generation stage, and allow addition of CV models, etc'. \n",
    "* uses list of candidates from lightfm\n",
    "    * preprocess moti's predictions using code in `transform_moti_preds.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install catboost shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import\n",
    "\n",
    "from catboost import Pool, cv, CatBoostClassifier, CatBoostRegressor,CatBoost\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import Model\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "# import shap\n",
    "# shap.initjs()\n",
    "\n",
    "from copy import deepcopy\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit, GridSearchCV, GroupKFold\n",
    "from tensorflow.keras.metrics import TopKCategoricalAccuracy, Precision, SparseTopKCategoricalAccuracy # @4\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# from sklearn.metrics import top_k_accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "# from sklearn.linear_model import  LogisticRegressionCV\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "# from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "# from pytorch_tabnet.metrics import Metric\n",
    "# import torch\n",
    "\n",
    "import datetime as dt\n",
    "d1 = dt.datetime(2015,12,30) # arbitrary min start date , use for converting checkin, checkout to numbers in nice format\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## https://www.tensorflow.org/guide/mixed_precision ## TF mixed precision - pytorch requires other setup\n",
    "# from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "\n",
    "# policy = mixed_precision.Policy('mixed_float16')\n",
    "# mixed_precision.set_policy(policy)\n",
    "# ## will need to correct in places, e.g.: \n",
    "# ## outputs = layers.Activation('softmax', dtype='float32', name='predictions')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features to add:\n",
    "* Lag \n",
    "* Rank (popularity) of city, country (in general, +- given booker country)\n",
    "* Count of hotel; user, trip size ? (may be leaky )\n",
    "* Seasonal features - Holidays? , datetime\n",
    "\n",
    "Aggregate feats:\n",
    "* user changed country? last booking (lag 1) country change? \n",
    "* max/min/avg popularity rank of previous locations visited\n",
    "\n",
    "\n",
    "\n",
    "We should create a dictionary of the rank, count, city/country etc' feats, so we can easily merge them when making more \"negative\" samples/feats for ranking.\n",
    "\n",
    "\n",
    "* Consider using a df2 of df without dates + drop_duplicates, +- without user/trip id (After calcing that) .\n",
    "\n",
    "\n",
    "Leaky or potentially leaky (Dependso n test set): \n",
    "* Target freq features - frequency of target city, given source county +- affiliate +- month of year +- given country (and interactions of target freq). \n",
    "    * Risk of leaks - depends of test data has temporal split or not. \n",
    "    * cartboost can do target encode, but this lets us do it for interactions, e.g. target city freq given the 2 countries and affiliate.\n",
    "    * beware overfitting! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "FASTRUN = False #True False\n",
    "\n",
    "if FASTRUN:\n",
    "    max_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_OUTPUTS = False ## save intermediate files to disk\n",
    "\n",
    "TARGET_COL = \"label\"\n",
    "# most basic categorical columns , without 'user_id', , 'utrip_id' ordevice_class - used for count encoding/filtering\n",
    "BASE_CAT_COLS = ['city_id',  'affiliate_id', 'booker_country', 'hotel_country']\n",
    "\n",
    "### features to get lags for. Not very robust. May want different feats for lags before -1\n",
    "LAG_FEAT_COLS = ['city_id', \n",
    "#                  'device_class',\n",
    "       'affiliate_id',\n",
    "#                  'booker_country',\n",
    "                 'hotel_country', \n",
    "       'duration', 'same_country', \n",
    "#                  'checkin_weekday',\n",
    "       'checkin_week',\n",
    "#         'checkout_weekday',\n",
    "       'city_id_count', 'affiliate_id_count',\n",
    "                 'hotel_country_count', \n",
    "#                  'checkin_week_count',\n",
    "                 'city_id_rank_by_hotel_country',\n",
    "#        'city_id_rank_by_booker_country', ## there are only 5 booker countries\n",
    "                 'city_id_rank_by_affiliate',\n",
    "#        'affiliate_id_rank_by_hotel_country',\n",
    "#        'affiliate_id_rank_by_booker_country', \n",
    "                ]\n",
    "\n",
    "CAT_FEAT_NAMES = [\"booker_country\", \"device_class\",\"affiliate_id\",\n",
    "                  \"city_id\",\"hotel_country\",\n",
    "#                   \"utrip_id\", ## changed\n",
    "#                   \"user_id\", ## ? could use lower dim - depends on train/test overlap\n",
    "                  \"checkin_week\",#\"checkout_week\",\n",
    "#                     \"checkin_weekday\",\n",
    "    \"lag1_city_id\",\"lag1_hotel_country\",\"lag1_affiliate_id\", #\"lag1_device_class\",\n",
    "     \"lag2_city_id\",\"lag2_hotel_country\",#\"lag2_affiliate_id\",#\"lag2_device_class\",\n",
    "       \"lag3_city_id\",\"lag3_hotel_country\", #\"lag3_booker_country\",\"lag3_affiliate_id\",\"lag3_device_class\",\n",
    "                  \"first_hotel_country\",\"first_city_id\"\n",
    "                 ]\n",
    "\n",
    "TRAIN_FILE_PATH = \"booking_train_set.csv\" #\"/content/drive/MyDrive/booking_wisdom/booking_train_set.csv\" #\"booking_train_set.csv\"\n",
    "\n",
    "LIST_FILE_PATH =   \"all_candidates_list.parquet\"#\"predictions_list_proc_v2.csv.gz\"#\"predictions_list_proc_v2.csv.gz\"\n",
    "\n",
    "TEST_FILE_PATH = \"booking_test_set.csv\" #\"/content/drive/MyDrive/booking_wisdom/booking_train_set.csv\" #\"booking_train_set.csv\"\n",
    "# TEST_LIST_FILE_PATH =  \"list_booking_train.csv.gz\"#\"/content/drive/MyDrive/booking_wisdom/list_booking_train.csv.gz\" #\"list_booking_train.csv.gz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/33907537/groupby-and-lag-all-columns-of-a-dataframe\n",
    "# https://stackoverflow.com/questions/62924987/lag-multiple-variables-grouped-by-columns\n",
    "## lag features with groupby over many columns: \n",
    "def groupbyLagFeatures(df:pd.DataFrame,lag:[]=[1,2],group=\"utrip_id\",lag_feature_cols=[]):\n",
    "    \"\"\"\n",
    "    lag features with groupby over many columns\n",
    "    https://stackoverflow.com/questions/62924987/lag-multiple-variables-grouped-by-columns\"\"\"\n",
    "    if len(lag_feature_cols)>0:\n",
    "        df=pd.concat([df]+[df.groupby(group)[lag_feature_cols].shift(x).add_prefix('lag'+str(x)+\"_\") for x in lag],axis=1)\n",
    "    else:\n",
    "         df=pd.concat([df]+[df.groupby(group).shift(x).add_prefix('lag'+str(x)+\"_\") for x in lag],axis=1)\n",
    "    return df\n",
    "\n",
    "def groupbyFirstLagFeatures(df:pd.DataFrame,group=\"user_id\",lag_feature_cols=[]):\n",
    "    \"\"\"\n",
    "    Get  first/head value lag-like of features with groupby over columns. Assumes sorted data!\n",
    "    \"\"\"\n",
    "    if len(lag_feature_cols)>0:\n",
    "        df=pd.concat([df]+[df.groupby(group)[lag_feature_cols].transform(\"first\").add_prefix(\"first_\")],axis=1)\n",
    "    else:\n",
    "#          df=pd.concat([df]+[df.groupby(group).first().add_prefix(\"first_\")],axis=1)\n",
    "        df=pd.concat([df]+[df.groupby(group).transform(\"first\").add_prefix(\"first_\")],axis=1)\n",
    "    return df\n",
    "\n",
    "######## Get n most popular items, per group\n",
    "def most_popular(group, n_max=4):\n",
    "    \"\"\"Find most popular hotel clusters by destination\n",
    "    Define a function to get most popular hotels for a destination group.\n",
    "\n",
    "    Previous version used nlargest() Series method to get indices of largest elements. But the method is rather slow.\n",
    "    Source: https://www.kaggle.com/dvasyukova/predict-hotel-type-with-pandas\n",
    "    \"\"\"\n",
    "    relevance = group['relevance'].values\n",
    "    hotel_cluster = group['hotel_cluster'].values\n",
    "    most_popular = hotel_cluster[np.argsort(relevance)[::-1]][:n_max]\n",
    "    return np.array_str(most_popular)[1:-1] # remove square brackets\n",
    "\n",
    "from tensorflow.keras.metrics import top_k_categorical_accuracy\n",
    "def top_4_accuracy(y_true, y_pred):\n",
    "    \"\"\"will only work if doing multiclass predictions\"\"\"\n",
    "    return top_k_categorical_accuracy(y_true, y_pred, k=4)\n",
    "\n",
    "## https://codereview.stackexchange.com/questions/149306/select-the-n-most-frequent-items-from-a-pandas-groupby-dataframe\n",
    "# https://stackoverflow.com/questions/52073054/group-by-a-column-to-find-the-most-frequent-value-in-another-column\n",
    "## can get modes (sorted)\n",
    "# https://stackoverflow.com/questions/50592762/finding-most-common-values-with-pandas-groupby-and-value-counts\n",
    "## df.groupby('tag')['category'].agg(lambda x: x.value_counts().index[0])\n",
    "# https://stackoverflow.com/questions/15222754/groupby-pandas-dataframe-and-select-most-common-value\n",
    "# source2.groupby(['Country','City'])['Short name'].agg(pd.Series.mode)\n",
    "\n",
    "\n",
    "# def fit_cbr_model(loss_function, additional_params=None, train_pool=train_pool, test_pool=test_pool,\n",
    "#                   CV=True,return_model=False):\n",
    "#     \"\"\"\n",
    "#     Fit catboost ranking model, or CV (cross validate) and return scores ; with a given loss function\n",
    "#     https://colab.research.google.com/github/catboost/tutorials/blob/master/ranking/ranking_tutorial.ipynb#scrollTo=RVNW0nowbtxH\n",
    "#     >>>model = fit_model('PairLogit', {'custom_metric': ['PrecisionAt:top=10', 'RecallAt:top=4']})\n",
    "    \n",
    "#     \"\"\"\n",
    "#     parameters = deepcopy(default_parameters)\n",
    "#     parameters['loss_function'] = loss_function\n",
    "#     parameters['train_dir'] = loss_function\n",
    "    \n",
    "#     if additional_params is not None:\n",
    "#         parameters.update(additional_params)\n",
    "         \n",
    "#     if CV:\n",
    "#         scores = cv(train_pool,params=parameters, plot=True,fold_count=2)\n",
    "#         scores = scores[scores.columns[~scores.columns.str.contains(\"std|pair\",case=False)]]\n",
    "#         display(scores.tail())\n",
    "        \n",
    "#     ## train model to get hacky top4 accuracy\n",
    "#     model = CatBoost(parameters)\n",
    "#     model.fit(train_pool, eval_set=test_pool, plot=False)        \n",
    "#     y_test_pred = model.predict(X_test,prediction_type=\"Probability\")[:,1]\n",
    "#     print(\"Test:\")\n",
    "#     top4_acc = top4_accuracy(model,y_test_pred)\n",
    "#     auc_score = roc_auc_score(y_true=y_test,y_score=y_test_pred)\n",
    "#     print(\"Roc-AUC (test): {0:.4f}\".format(100*auc_score))\n",
    " \n",
    "#     if return_model:\n",
    "#         return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes_dict = {\n",
    "#     \"utrip_id\":\"category\",\n",
    "#     \"city_id\":\"int32\",\n",
    "#     \"rank\":\"int16\",\n",
    "    \"label\": \"int8\",#\"uint8\",\n",
    "    \"affiliate_id\":\"int32\",\n",
    "#     \"device_class\":\"category\",\n",
    "#     \"booker_country\":\"string\", ## category would save more memory but intereferes with feature engineering\n",
    "#     \"hotel_country\":\"string\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv(TRAIN_FILE_PATH,usecols=[\"city_id\"]).min() ## lowest normal num in train city_id is 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>checkin</th>\n",
       "      <th>checkout</th>\n",
       "      <th>city_id</th>\n",
       "      <th>device_class</th>\n",
       "      <th>affiliate_id</th>\n",
       "      <th>booker_country</th>\n",
       "      <th>hotel_country</th>\n",
       "      <th>utrip_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1004862</th>\n",
       "      <td>29</td>\n",
       "      <td>2016-07-09</td>\n",
       "      <td>2016-07-11</td>\n",
       "      <td>47054</td>\n",
       "      <td>desktop</td>\n",
       "      <td>1601</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>29_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004863</th>\n",
       "      <td>29</td>\n",
       "      <td>2016-07-11</td>\n",
       "      <td>2016-07-13</td>\n",
       "      <td>34444</td>\n",
       "      <td>desktop</td>\n",
       "      <td>1601</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>29_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004864</th>\n",
       "      <td>29</td>\n",
       "      <td>2016-07-13</td>\n",
       "      <td>2016-07-16</td>\n",
       "      <td>12291</td>\n",
       "      <td>desktop</td>\n",
       "      <td>1601</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>29_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004865</th>\n",
       "      <td>29</td>\n",
       "      <td>2016-07-16</td>\n",
       "      <td>2016-07-18</td>\n",
       "      <td>16386</td>\n",
       "      <td>desktop</td>\n",
       "      <td>8132</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>29_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897811</th>\n",
       "      <td>81</td>\n",
       "      <td>2016-05-15</td>\n",
       "      <td>2016-05-16</td>\n",
       "      <td>33665</td>\n",
       "      <td>desktop</td>\n",
       "      <td>9924</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>81_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1072993</th>\n",
       "      <td>6258065</td>\n",
       "      <td>2016-04-19</td>\n",
       "      <td>2016-04-20</td>\n",
       "      <td>55044</td>\n",
       "      <td>mobile</td>\n",
       "      <td>9452</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Pullamawang</td>\n",
       "      <td>6258065_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420479</th>\n",
       "      <td>6258087</td>\n",
       "      <td>2016-08-03</td>\n",
       "      <td>2016-08-04</td>\n",
       "      <td>17754</td>\n",
       "      <td>desktop</td>\n",
       "      <td>2436</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>6258087_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420480</th>\n",
       "      <td>6258087</td>\n",
       "      <td>2016-08-04</td>\n",
       "      <td>2016-08-05</td>\n",
       "      <td>50073</td>\n",
       "      <td>desktop</td>\n",
       "      <td>2436</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>6258087_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420481</th>\n",
       "      <td>6258087</td>\n",
       "      <td>2016-08-05</td>\n",
       "      <td>2016-08-06</td>\n",
       "      <td>11662</td>\n",
       "      <td>desktop</td>\n",
       "      <td>2436</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>6258087_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420482</th>\n",
       "      <td>6258087</td>\n",
       "      <td>2016-08-06</td>\n",
       "      <td>2016-08-07</td>\n",
       "      <td>41484</td>\n",
       "      <td>desktop</td>\n",
       "      <td>2436</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>6258087_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1166835 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id    checkin   checkout  city_id device_class  affiliate_id  \\\n",
       "1004862       29 2016-07-09 2016-07-11    47054      desktop          1601   \n",
       "1004863       29 2016-07-11 2016-07-13    34444      desktop          1601   \n",
       "1004864       29 2016-07-13 2016-07-16    12291      desktop          1601   \n",
       "1004865       29 2016-07-16 2016-07-18    16386      desktop          8132   \n",
       "897811        81 2016-05-15 2016-05-16    33665      desktop          9924   \n",
       "...          ...        ...        ...      ...          ...           ...   \n",
       "1072993  6258065 2016-04-19 2016-04-20    55044       mobile          9452   \n",
       "420479   6258087 2016-08-03 2016-08-04    17754      desktop          2436   \n",
       "420480   6258087 2016-08-04 2016-08-05    50073      desktop          2436   \n",
       "420481   6258087 2016-08-05 2016-08-06    11662      desktop          2436   \n",
       "420482   6258087 2016-08-06 2016-08-07    41484      desktop          2436   \n",
       "\n",
       "        booker_country hotel_country   utrip_id  \n",
       "1004862        Elbonia       Elbonia       29_1  \n",
       "1004863        Elbonia       Elbonia       29_1  \n",
       "1004864        Elbonia       Elbonia       29_1  \n",
       "1004865        Elbonia       Elbonia       29_1  \n",
       "897811         Elbonia       Elbonia       81_1  \n",
       "...                ...           ...        ...  \n",
       "1072993         Gondal   Pullamawang  6258065_1  \n",
       "420479          Gondal        Gondal  6258087_1  \n",
       "420480          Gondal        Gondal  6258087_1  \n",
       "420481          Gondal        Gondal  6258087_1  \n",
       "420482          Gondal        Gondal  6258087_1  \n",
       "\n",
       "[1166835 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if FASTRUN:\n",
    "    df = pd.read_csv(TRAIN_FILE_PATH,\n",
    "                     nrows=350_280,\n",
    "                     index_col=[0],\n",
    "                     parse_dates=[\"checkin\",\"checkout\"],infer_datetime_format=True, dtype=dtypes_dict)\n",
    "else:\n",
    "    df = pd.read_csv(TRAIN_FILE_PATH,\n",
    "                     index_col=[0],\n",
    "                     parse_dates=[\"checkin\",\"checkout\"],infer_datetime_format=True, dtype=dtypes_dict)\n",
    "\n",
    "# ## list of candidate predictions (100 per utrip_id)\n",
    "# df_list = pd.read_parquet(LIST_FILE_PATH,\n",
    "# #                         usecols=[0,2,3], # skip city_id col\n",
    "# #                         dtype=dtypes_dict\n",
    "#                      )\n",
    "\n",
    "# df_list = df_list.loc[df_list[\"rank\"]<50]\n",
    "# print(\"rank of labels (target =1):\")\n",
    "# # print(df_list.loc[df_list[\"label\"]==1][\"rank\"].describe(percentiles=[.25,.5,.75,.9,.95,.99]))\n",
    "\n",
    "# print(df_list.shape[0])\n",
    "\n",
    "# assert df_list.drop_duplicates([\"utrip_id\",\"city_id\"]).shape[0] == df_list.shape[0]\n",
    "\n",
    "# if FASTRUN:\n",
    "#     df_list = df_list.loc[df_list[\"rank\"]<=25]\n",
    "# #     df_list = df_list.loc[(df_list[\"rank\"]<=12) | (df_list[\"label\"]==1)]  \n",
    "# else:\n",
    "#     #### downsampole train data, less so for test data/candidates\n",
    "#     # # ## HACK due to running out of mempory ! sample of data \n",
    "    \n",
    "#     train_ids_all = list(df[\"utrip_id\"].unique())\n",
    "#     print(\"len(train_ids_all)\",len(train_ids_all))\n",
    "#     df_list_test = df_list.loc[~df_list[\"utrip_id\"].isin(train_ids_all)]\n",
    "    \n",
    "#     ## temp: for faster iterations\n",
    "#     df_list_test = df_list_test.loc[df_list_test[\"rank\"]<= 100]\n",
    "    \n",
    "#     print(\"list test shape\",df_list_test.shape)\n",
    "#     df_list_train = df_list.loc[df_list[\"utrip_id\"].isin(train_ids_all)]\n",
    "#     print(\"train test shape\",df_list_train.shape)\n",
    "    \n",
    "#     df_list_1 = df_list_train.loc[(df_list_train[\"rank\"]<=60) | (df_list_train[\"label\"]==1)]    \n",
    "#     df_list_2 = df_list_train.loc[(df_list_train[\"rank\"]>60) & (df_list_train[\"label\"]==0)].groupby([\"utrip_id\"]).sample(n=15,random_state=42)\n",
    "#     df_list = pd.concat([df_list_1,df_list_2,df_list_test])\n",
    "#     del df_list_1,df_list_2,df_list_test,df_list_train\n",
    "#     gc.collect()\n",
    "\n",
    "# print(\"df list shape\",df_list.shape[0])\n",
    "\n",
    "# ### df_test has \"missing\" cities, hotel countries\n",
    "#  ## replace missing cities with nans or -9\n",
    "# ### df_test[\"city_id\"] = df_test[\"city_id\"].replace(0,np.nan)\n",
    "\n",
    "# # df_list = df_list.loc[df_list[\"city_id\"] >0]\n",
    "\n",
    "# df_list.set_index(\"utrip_id\",inplace=True)\n",
    "                     \n",
    "df.sort_values([\"user_id\",\"checkin\"],inplace=True)\n",
    "# df_list.set_index(\"utrip_id\",inplace=True)\n",
    "\n",
    "\n",
    "## add feature - score relative to rank (could add vs utrip_id - equiv to rank)\n",
    "# df_list[\"score_norm\"] = df_list[\"score\"].div(df_list.groupby(\"rank\")[\"score\"].transform(\"mean\"))#.round(3)\n",
    "\n",
    "\n",
    "display(df)\n",
    "# display(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = pd.read_csv(TEST_FILE_PATH, parse_dates=[\"checkin\",\"checkout\"],infer_datetime_format=True, dtype=dtypes_dict)\n",
    "# ### note the missing city id in last rows\n",
    "# print(df_test.shape)\n",
    "# test_num_ids = df_test[\"utrip_id\"].nunique()\n",
    "# print(\"test_num_ids\",test_num_ids)\n",
    "# df_test[\"train\"] = 0\n",
    "\n",
    "# ### df_test has \"missing\" cities, hotel countries\n",
    "# # df_list[\"city_id\"] = df_list[\"city_id\"].replace(0,-9)# np.nan ## replace missing cities with nans or -9\n",
    "# df_test[\"city_id\"] = df_test[\"city_id\"].replace(0,np.nan) ## 0 are missing cities here\n",
    "\n",
    "# df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.166835e+06\n",
       "mean     6.117143e+00\n",
       "std      2.796383e+00\n",
       "min      1.000000e+00\n",
       "25%      4.000000e+00\n",
       "50%      5.000000e+00\n",
       "75%      7.000000e+00\n",
       "max      4.800000e+01\n",
       "Name: total_rows, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### add features to be consistent with test set of row in trip, and total trips in trip\n",
    "df[\"row_num\"] = df.groupby(\"utrip_id\")[\"checkin\"].rank(ascending=True,pct=False).astype(int)\n",
    "utrip_counts = df[\"utrip_id\"].value_counts()\n",
    "df[\"total_rows\"] = df[\"utrip_id\"].map(utrip_counts)\n",
    "\n",
    "df[\"train\"] = 1\n",
    "\n",
    "df[\"total_rows\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166835\n"
     ]
    }
   ],
   "source": [
    "# ### stack train, test for now (easier feat eng)\n",
    "# df = pd.concat([df,df_test])\n",
    "print(df.shape[0])\n",
    "\n",
    "df[\"last\"] = (df[\"row_num\"] ==df[\"total_rows\"]).astype(int)\n",
    "assert 0 == df[[\"row_num\",\"total_rows\",\"last\"]].isna().sum().max()\n",
    "# df[[\"row_num\",\"total_rows\",\"last\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1102338\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diff_total_rows</th>\n",
       "      <th>total_rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.102338e+06</td>\n",
       "      <td>1.102338e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.314553e-01</td>\n",
       "      <td>6.101682e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.809714e-01</td>\n",
       "      <td>2.728969e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.200000e+01</td>\n",
       "      <td>4.800000e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       diff_total_rows    total_rows\n",
       "count     1.102338e+06  1.102338e+06\n",
       "mean      2.314553e-01  6.101682e+00\n",
       "std       6.809714e-01  2.728969e+00\n",
       "min       0.000000e+00  1.000000e+00\n",
       "25%       0.000000e+00  4.000000e+00\n",
       "50%       0.000000e+00  5.000000e+00\n",
       "75%       0.000000e+00  7.000000e+00\n",
       "max       4.200000e+01  4.800000e+01"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Drop duplicate, consecutive rows\n",
    "df = df.loc[(df[[\"city_id\",\"utrip_id\",\"booker_country\",\"last\"]].shift() != df[[\"city_id\",\"utrip_id\",\"booker_country\",\"last\"]]).max(axis=1)]\n",
    "print(df.shape[0])\n",
    "\n",
    "## add alt total rows as a diff, for marking cases where we dropped dupes.\n",
    "\n",
    "utrip_counts_2 = df[\"utrip_id\"].value_counts()\n",
    "df[\"diff_total_rows\"] = df[\"utrip_id\"].map(utrip_counts_2)\n",
    "df[\"diff_total_rows\"] =df[\"total_rows\"] -  df[\"diff_total_rows\"]\n",
    "\n",
    "df[[\"diff_total_rows\",\"total_rows\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### todo : add \"time since start (min date in data / \"duration\")\"  \n",
    "### todo : \"time since start of trip (min date per trip \"duration\")\"  \n",
    "df[\"duration\"] = ((df[\"checkout\"] - df[\"checkin\"]).dt.days).astype(int)\n",
    "df[\"same_country\"] = (df[\"booker_country\"]==df[\"hotel_country\"]).astype(int)\n",
    "\n",
    "df[\"checkin_day\"] = df[\"checkin\"].dt.day\n",
    "df[\"checkin_weekday\"] = df[\"checkin\"].dt.weekday\n",
    "df[\"checkin_week\"] = df[\"checkin\"].dt.isocalendar().week.astype(int) ## week of year\n",
    "df[\"checkin_month\"] = df[\"checkin\"].dt.month\n",
    "df[\"checkin_year\"] = df[\"checkin\"].dt.year-2015\n",
    "df[\"checkin_day_of_year\"] = df[\"checkin\"].dt.dayofyear  # day of year - may overfit , but will allow learning holidays and other checkins on that date \n",
    "\n",
    "# df[\"checkin_quarter\"] = df[\"checkin\"].dt.quarter # relatively redundant but may be used for \"id\"\n",
    "\n",
    "# df[\"checkout_weekday\"] = df[\"checkout\"].dt.weekday\n",
    "df[\"checkout_week\"] = df[\"checkout\"].dt.isocalendar().week.astype(int) ## week of year\n",
    "df[\"checkout_day\"] = df[\"checkout\"].dt.day ## day of month\n",
    "\n",
    "## cyclical datetime embeddings\n",
    "## drop originakl variables? \n",
    "## TODO:L add for other variables, +- those that we'll embed (week?)\n",
    "\n",
    "# df['checkin_weekday_sin'] = np.sin(df[\"checkin_weekday\"]*(2.*np.pi/7))\n",
    "# df['checkin_weekday_cos'] = np.cos(df[\"checkin_weekday\"]*(2.*np.pi/7))\n",
    "\n",
    "# df['checkin_month_sin'] = np.sin((df[\"checkin_month\"]-1)*(2.*np.pi/12))\n",
    "# df['checkin_month_cos'] = np.cos((df[\"checkin_month\"]-1)*(2.*np.pi/12))\n",
    "\n",
    "df['checkin_week_sin'] = np.sin((df[\"checkin_week\"]-1)*(2.*np.pi/53))\n",
    "df['checkin_week_cos'] = np.cos((df[\"checkin_week\"]-1)*(2.*np.pi/53))\n",
    "\n",
    "df.drop([\"checkin_month\"],axis=1,errors=\"ignore\")\n",
    "\n",
    "### time since first date in dataset (i.,e index proxy)\n",
    "min_date = df[\"checkin\"].min()\n",
    "df[\"time_elapsed_global\"] = (df[\"checkin\"] - min_date).dt.days.astype(int)\n",
    "\n",
    "# #############\n",
    "# # last number in utrip id - probably which trip number it is:\n",
    "# df[\"utrip_number\"] = df[\"utrip_id\"].str.split(\"_\",expand=True)[1].astype(int)\n",
    "\n",
    "### encode string columns - must be consistent with test data \n",
    "### IF we can concat test with train, we can just do a single transformation  for the NON TARGET cols\n",
    "# obj_cols_list = df.select_dtypes(\"O\").columns.values\n",
    "obj_cols_list = ['device_class','booker_country','hotel_country'] # we could also define when loading data, dtype\n",
    "\n",
    "for c in obj_cols_list:\n",
    "    df[c] = df[c].astype(\"category\")\n",
    "    df[c] = df[c].cat.codes.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_rare_feats(df,col,thresh=2,replace_val=-1):\n",
    "    ### replace rare variables (under 2 occurrences) with \"-1\" dummy\n",
    "    affiliates_counts = df[col].value_counts()\n",
    "#     print(\"before:\", affiliates_counts)\n",
    "    print(\"uniques\",df[col].nunique())\n",
    "    affiliates_counts = affiliates_counts.to_dict()\n",
    "    # df[col] = df[col].where(df[col].apply(lambda x: x.map(x.value_counts()))>=3, -1)\n",
    "    df[col] = df[col].where(df[col].map(affiliates_counts)>=thresh, replace_val)\n",
    "    print(\"afterwards uniques\",df[col].nunique())\n",
    "#     df[col] = df[col].astype(int)\n",
    "    return(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uniques 39901\n",
      "afterwards uniques 27714\n",
      "before: #trips 217686\n",
      "after: #trips 215508\n"
     ]
    }
   ],
   "source": [
    "### replace rare cities - note that we must take this into account with target! e.g. drop and account for missings as mistake\n",
    "df[\"city_id\"] = map_rare_feats(df,\"city_id\",thresh=2,replace_val=-1)\n",
    "print(\"before: #trips\",df[\"utrip_id\"].nunique())\n",
    "print(\"after: #trips\",df.loc[(df[\"last\"]==1) & (df[\"city_id\"]>=0)][\"utrip_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1102338\n",
      "trips without dropping rare last targets 217686\n",
      "trips after dropping rare last targets 215508\n",
      "1100160\n"
     ]
    }
   ],
   "source": [
    "print(df.shape[0])\n",
    "print(\"trips without dropping rare last targets\",df.loc[df[\"last\"]==1][\"utrip_id\"].nunique())\n",
    "print(\"trips after dropping rare last targets\",df.loc[(df[\"last\"]==1) & (df[\"city_id\"]>0)][\"utrip_id\"].nunique())\n",
    "df = df.loc[~((df[\"last\"]==1) & (df[\"city_id\"]<0))]\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ### replace rare variables (under 2 occurrences) with \"-1\" dummy\n",
    "# affiliates_counts = df[\"affiliate_id\"].value_counts()\n",
    "# print(\"before:\", affiliates_counts)\n",
    "# print(\"uniques\",df[\"affiliate_id\"].nunique())\n",
    "# affiliates_counts = affiliates_counts.to_dict()\n",
    "# # df[\"affiliate_id\"] = df[\"affiliate_id\"].where(df[\"affiliate_id\"].apply(lambda x: x.map(x.value_counts()))>=3, -1)\n",
    "# df[\"affiliate_id\"] = df[\"affiliate_id\"].where(df[\"affiliate_id\"].map(affiliates_counts)>4, -1)\n",
    "# df[\"affiliate_id\"] = df[\"affiliate_id\"].astype(int)\n",
    "\n",
    "# # print(\"after\\n\",df[\"affiliate_id\"].value_counts())\n",
    "# print(\"affiliate_id nunique\",df[\"affiliate_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.city_id.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### duplicate of city, country id columns, as number (may be redundant - we already have count encoding ! )\n",
    "# ### replace rare variables (under 2 occurrences) with \"-1\" dummy\n",
    "# ## city id:\n",
    "# affiliates_counts = df[\"city_id\"].value_counts()\n",
    "# # print(\"before:\", affiliates_counts)\n",
    "# print(\"uniques\",df[\"city_id\"].nunique())\n",
    "# affiliates_counts = affiliates_counts.to_dict()\n",
    "# df[\"numeric_city_id\"] = df[\"city_id\"].where(df[\"city_id\"].map(affiliates_counts)>3, -1)\n",
    "# df[\"numeric_city_id\"] = df[\"numeric_city_id\"].astype(int)\n",
    "\n",
    "# ## hotel_country: (+ordinal encoding)\n",
    "# affiliates_counts = df[\"hotel_country\"].value_counts()\n",
    "# # print(\"before:\", affiliates_counts)\n",
    "# print(\"uniques\",df[\"hotel_country\"].nunique())\n",
    "# affiliates_counts = affiliates_counts.to_dict()\n",
    "# df[\"numeric_hotel_country\"] = df[\"hotel_country\"].where(df[\"hotel_country\"].map(affiliates_counts)>3, \"\")\n",
    "# df[\"numeric_hotel_country\"] = df[\"numeric_hotel_country\"].astype(\"category\").cat.codes.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Add first country, city visited in a trip. \n",
    "* Drop first row of a trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n## add the \"first\" place visited/values\\n### nopte - will need to drop first row in trip, or impute nans when using this feature \\n\\n### first by user results in too much sparsity/rareness for our IDs purposes\\ndf = groupbyFirstLagFeatures(df,group=\"utrip_id\",lag_feature_cols=[\"hotel_country\",\"city_id\",\"duration\",\"same_country\",\\n                                                                   \"affiliate_id\", \"checkin\"]) \\n## + first checkin date -> use for time delta feature\\n# df = df.loc[df[\"row_num\"]>1] ## can\\'t do yet, needed for lag features\\nprint(df[[\"first_hotel_country\",\"hotel_country\",\"city_id\",\"first_same_country\", \"first_affiliate_id\"]].nunique())\\ndf\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "## add the \"first\" place visited/values\n",
    "### nopte - will need to drop first row in trip, or impute nans when using this feature \n",
    "\n",
    "### first by user results in too much sparsity/rareness for our IDs purposes\n",
    "df = groupbyFirstLagFeatures(df,group=\"utrip_id\",lag_feature_cols=[\"hotel_country\",\"city_id\",\"duration\",\"same_country\",\n",
    "                                                                   \"affiliate_id\", \"checkin\"]) \n",
    "## + first checkin date -> use for time delta feature\n",
    "# df = df.loc[df[\"row_num\"]>1] ## can't do yet, needed for lag features\n",
    "print(df[[\"first_hotel_country\",\"hotel_country\",\"city_id\",\"first_same_country\", \"first_affiliate_id\"]].nunique())\n",
    "df\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add relative date/duration feature(s)\n",
    "* Time since start of trip (first_checkin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf[\"total_trip_duration\"] = ((df[\"checkin\"] -df[\"first_checkin\"]).dt.days).astype(int)\\n\\ndf[\"first_checkin_week\"] = df[\"first_checkin\"].dt.isocalendar().week.astype(int)\\n# df[[\"total_trip_duration\",\"first_checkin\",\"checkin\"]]\\n\\n### change type of date cols to more learnable format\\n\\ndf[\"checkin\"] = (df[\"checkin\"]-d1).dt.days.astype(int16)\\ndf[\"checkout\"] = (df[\"checkout\"]-d1).dt.days.astype(int16)\\ndf[\"first_checkin\"] = (df[\"first_checkin\"]-d1).dt.days.astype(int16)\\n## # (df[\"checkin\"]-d1).astype(\\'int64\\')//1e9\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "df[\"total_trip_duration\"] = ((df[\"checkin\"] -df[\"first_checkin\"]).dt.days).astype(int)\n",
    "\n",
    "df[\"first_checkin_week\"] = df[\"first_checkin\"].dt.isocalendar().week.astype(int)\n",
    "# df[[\"total_trip_duration\",\"first_checkin\",\"checkin\"]]\n",
    "\n",
    "### change type of date cols to more learnable format\n",
    "\n",
    "df[\"checkin\"] = (df[\"checkin\"]-d1).dt.days.astype(int16)\n",
    "df[\"checkout\"] = (df[\"checkout\"]-d1).dt.days.astype(int16)\n",
    "df[\"first_checkin\"] = (df[\"first_checkin\"]-d1).dt.days.astype(int16)\n",
    "## # (df[\"checkin\"]-d1).astype('int64')//1e9\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### Following aggregation features - would be best to use time window (sort data) to generate, otherwise they will LEAK! (e.g. nunique countries visited)\n",
    "\n",
    "### count features (can also later add rank inside groups).\n",
    "### Some may be leaks (# visits in a trip should use time window?) , and do users repeat? \n",
    "### can add more counts of group X time period (e.g. affiliate X month of year)\n",
    "## alt way to get counts/freq :\n",
    "# freq = df[\"city_id\"].value_counts()\n",
    "# df[\"city_id_count\"] = df[\"city_id\"].map(freq)\n",
    "# print(df[\"city_id_count\"].describe())\n",
    "\n",
    "count_cols = [ 'city_id', 'hotel_country', \n",
    "             'user_id', \n",
    "#              \"checkin_month\", 'affiliate_id', \"checkin_week\",\n",
    "#               \"checkin_day_of_year\"\n",
    "             ]\n",
    "\n",
    "for c in count_cols:\n",
    "    df[f\"{c}_count\"] = df.groupby([c])[\"duration\"].transform(\"size\")\n",
    "    \n",
    "# ########################################################\n",
    "# ## nunique per trip\n",
    "# ### https://stackoverflow.com/questions/46470743/how-to-efficiently-compute-a-rolling-unique-count-in-a-pandas-time-series\n",
    "\n",
    "# nunique_cols = [ 'city_id', 'hotel_country','affiliate_id', \n",
    "#                 'booker_country',\"duration\",\"checkin_weekday\"]\n",
    "# # df[\"nunique_booker_countries\"] = df.groupby(\"utrip_id\")[\"booker_country\"].nunique()\n",
    "# # df[\"nunique_hotel_country\"] = df.groupby(\"utrip_id\")[\"hotel_country\"].nunique()\n",
    "# for c in nunique_cols:\n",
    "#     df[f\"{c}_nunique\"] = df.groupby([\"utrip_id\"])[c].transform(\"nunique\")\n",
    "\n",
    "# ### trips taken by user\n",
    "# df[\"user_nunique_trips\"] = df.groupby(\"user_id\")[\"utrip_id\"].transform(\"nunique\")\n",
    "# print(df.nunique())\n",
    "\n",
    "# ########################################################\n",
    "# ## get frequency/count feature's rank within a group - e.g. within a country (or affiliate) \n",
    "# ## add \"_count\" to column name to get count col name, then add rank col \n",
    "\n",
    "# ### ALT/ duplicate feat - add percent rank (instead or in addition)\n",
    "\n",
    "# rank_cols = ['city_id','affiliate_id', 'hotel_country',  # 'booker_country',\n",
    "# #  \"checkin_month\",\n",
    "#              \"checkin_week\"\n",
    "#             ]\n",
    "# ### what is meaning of groupby and rank of smae variable by same var? Surely should be 1 / unary? \n",
    "# for c in rank_cols:\n",
    "#     df[f\"{c}_rank_by_hotel_country\"] = df.groupby(['hotel_country'])[f\"{c}_count\"].transform(\"rank\")\n",
    "#     df[f\"{c}_rank_by_affiliate\"] = df.groupby(['affiliate_id'])[f\"{c}_count\"].transform(\"rank\")\n",
    "#     ### rank by month: potentially strong feature, but must avoid leaks + must be joined back by month\n",
    "# #     df[f\"{c}_rank_by_month\"] = df.groupby(['checkin_month'])[f\"{c}_count\"].transform(\"rank\") ## \"city_id_rank_by_month\" may leak\n",
    "# ## there are just 5 booker countries.. \n",
    "# df[\"city_id_rank_by_booker_country\"] = df.groupby(['booker_country'])[f\"city_id_count\"].transform(\"rank\")\n",
    "    \n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12173"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df[\"city_id_count\"]>=7][\"city_id\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert df.isna().sum().max() ==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum top4 total percentage: 5.800000000000001\n",
      "sum top200 total percentage: 49.0\n",
      "sum top500 total percentage: 62.4\n",
      "sum top1000 total percentage: 72.1\n"
     ]
    }
   ],
   "source": [
    "## check coverage of top k classes\n",
    "y_last = df.loc[(df[\"last\"]==1) & (df[\"train\"]==1)][\"city_id\"].dropna().astype(int)\n",
    "print(\"sum top4 total percentage:\",100*y_last.value_counts(normalize=True)[0:4].sum().round(3)) ##5.7%\n",
    "print(\"sum top200 total percentage:\",100*y_last.value_counts(normalize=True)[0:200].sum().round(3)) ## 48%\n",
    "print(\"sum top500 total percentage:\",100*y_last.value_counts(normalize=True)[0:500].sum().round(3)) ### 61%\n",
    "print(\"sum top1000 total percentage:\",100*y_last.value_counts(normalize=True)[0:1000].sum().round(3)) ### 71 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add lag features + Train/test/data split\n",
    "* Lag feats (remember for categorical)\n",
    "* Drop leak features (target values - country, city)\n",
    "\n",
    "* drop instances  that lack history (e.g. at least 3d step and onwards) - by dropna in lag feat\n",
    "* fill nans\n",
    "* Split train/test by `user id` / split could maybe be by `utrip ID` ? ? \n",
    "    * Test - only last trip\n",
    "    *  stratified train/test split by class - then drop any train rows with overlap with tests' IDs.  \n",
    "        * Could also stratify by users, but risks some classes being non present in test\n",
    "\n",
    "* TODO: aggregate features o ntime series - e.g. mean, max, min duration\n",
    "    * We already have nunique\n",
    "###### WE may not need to drop these features anymore - but we may want to join them by city id\n",
    "*We  join city id features after adding the list of caNDIDATes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### features to drop - not usable, or leaks (e.g. aggregations on target) , or hotel/city specific (should be added from per city features)\n",
    "\n",
    "DROP_FEATS = [ #'user_id', ## drop seperately\n",
    "#     'checkin',  # is ok if turned into number\n",
    "#     'checkout',\n",
    "              'hotel_country',\n",
    "              'city_id_count','same_country',\n",
    "#               'utrip_id',\n",
    "             'city_id_count','hotel_country_count',\n",
    "              'city_id_nunique', 'hotel_country_nunique',\n",
    "              'city_id_rank_by_hotel_country','city_id_rank_by_booker_country', 'city_id_rank_by_affiliate',\n",
    "              'affiliate_id_rank_by_hotel_country','affiliate_id_rank_by_booker_country', 'affiliate_id_rank_by_affiliate',\n",
    "              'hotel_country_rank_by_hotel_country',\n",
    "       'hotel_country_rank_by_booker_country','hotel_country_rank_by_affiliate',\n",
    "              'booker_country_rank_by_hotel_country',\n",
    "              'booker_country_rank_by_booker_country',\n",
    "              'checkin_month_rank_by_hotel_country',\n",
    "             ]\n",
    "\n",
    "# df2.drop(DROP_FEATS,axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    27714.000000\n",
       "mean        39.696904\n",
       "std        245.509379\n",
       "min          2.000000\n",
       "25%          3.000000\n",
       "50%          5.000000\n",
       "75%         15.000000\n",
       "max      10010.000000\n",
       "Name: city_id, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"city_id\"].value_counts().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    17721.000000\n",
       "mean        12.161165\n",
       "std         82.974868\n",
       "min          1.000000\n",
       "25%          1.000000\n",
       "50%          2.000000\n",
       "75%          5.000000\n",
       "max       3792.000000\n",
       "Name: city_id, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df[\"last\"]==1][\"city_id\"].value_counts().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quick filter rare cities\n",
    "* Doesn't necessarily filter target cities only\n",
    "* should maybe use utrip id, not user id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before # trips ['29_1' '81_1' '136_2' ... '6258041_1' '6258065_1' '6258087_1']\n",
      "1100160\n",
      "rows after dropping rare targets 1100160\n",
      "count    17266.000000\n",
      "mean        11.195413\n",
      "std         73.359163\n",
      "min          1.000000\n",
      "25%          1.000000\n",
      "50%          2.000000\n",
      "75%          4.750000\n",
      "max       3083.000000\n",
      "Name: counts, dtype: float64\n",
      "prefiltering # 17266\n",
      "cities after filtering: 5264\n",
      "% kept 30.487663616355842\n",
      "% dropped 69.51233638364415\n",
      "% trips kept 100.0\n",
      "% trips dropped 30.98075245466526\n",
      "771966\n",
      "utrip_id    148742\n",
      "city_id       5255\n",
      "dtype: int64\n",
      "       user_id_count  city_id_count\n",
      "count  148742.000000  148742.000000\n",
      "mean        5.189967    2211.680346\n",
      "std         1.734815    2578.081411\n",
      "min         4.000000       5.000000\n",
      "25%         4.000000     291.000000\n",
      "50%         5.000000    1074.000000\n",
      "75%         6.000000    3404.000000\n",
      "max        35.000000    9959.000000\n",
      "After # trips ['136_2' '136_4' '156_1' ... '6257895_1' '6257933_3' '6258017_1']\n"
     ]
    }
   ],
   "source": [
    "# df = df.loc[df[\"city_id_count\"]>=2]\n",
    "print(\"before # trips\",df[\"utrip_id\"].unique())\n",
    "s1 = df.shape[0]\n",
    "print(s1)\n",
    "df = df.loc[~((df[\"last\"]==1) & (df[\"city_id\"]<0))]\n",
    "print(\"rows after dropping rare targets\",df.shape[0])\n",
    "\n",
    "keep_users = df.loc[df[\"last\"]==1][\"utrip_id\"].unique() # list  of users who still have a last address\n",
    "df = df.loc[df[\"utrip_id\"].isin(keep_users)] ## keep only users who have last destination (After our filtering)\n",
    "\n",
    "df[\"user_id_count\"] = df.groupby([\"utrip_id\"])[\"city_id\"].transform(\"size\")\n",
    "df = df.loc[df[\"user_id_count\"]>=4]\n",
    "##########\n",
    "#filter by frequency of last city target\n",
    "pre_trips_amount = len(keep_users)\n",
    "last_targets_freq = df.loc[df[\"last\"]==1][\"city_id\"].value_counts().reset_index().rename(columns={\"index\":\"city_id\",\"city_id\":\"counts\"})\n",
    "print(last_targets_freq[\"counts\"].describe())\n",
    "### min target frequency : \n",
    "raw_count = last_targets_freq.shape[0]\n",
    "print(\"prefiltering #\",raw_count)\n",
    "last_targets_freq = last_targets_freq.loc[last_targets_freq[\"counts\"]>= 4]\n",
    "print(\"cities after filtering:\",last_targets_freq.shape[0])\n",
    "print(\"% kept\",100*(last_targets_freq.shape[0]/raw_count))\n",
    "print(\"% dropped\",100*(1-last_targets_freq.shape[0]/raw_count))\n",
    "\n",
    "df = df.loc[df[\"city_id\"].isin(last_targets_freq[\"city_id\"])]\n",
    "#######\n",
    "df[\"user_id_count\"] = df.groupby([\"utrip_id\"])[\"city_id\"].transform(\"size\") ## update count\n",
    "df = df.loc[df[\"user_id_count\"]>=4]\n",
    "#######\n",
    "post_trips_amount = df[\"utrip_id\"].nunique()\n",
    "print(\"% trips kept\",100*(post_trips_amount/post_trips_amount))\n",
    "print(\"% trips dropped\",100*(1-post_trips_amount/pre_trips_amount))\n",
    "#######\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df[[\"utrip_id\",\"city_id\"]].nunique())\n",
    "print(df.drop_duplicates(\"utrip_id\")[[\"user_id_count\",\"city_id_count\"]].describe() )\n",
    "\n",
    "print(\"After # trips\",df[\"utrip_id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% trips kept 69.01924754533474\n",
      "% trips dropped 30.98075245466526\n"
     ]
    }
   ],
   "source": [
    "post_trips_amount = df[\"utrip_id\"].nunique()\n",
    "print(\"% trips kept\",100*(post_trips_amount/pre_trips_amount))\n",
    "print(\"% trips dropped\",100*(1-post_trips_amount/pre_trips_amount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lbl encode city, users \n",
    "* reencoding is wasteful, but saves on bugs ? (e.g. for embedding, which assumes start from zero)\n",
    "* We add 1 , due to using zero padding for empty lag features! \n",
    "\n",
    "\n",
    "## I later reencode the target city_id, to save on space/# layers\n",
    "* this loses the original values for now! \n",
    "* this is due to  how the sparse categoircal acc works - and we had encoded city id by the full vocab before, which is more cities than exist as targets in the final dest (and that defines the softmax layer size) \n",
    "    * final_target_encoder = LabelEncoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    148742.000000\n",
       "mean          5.189967\n",
       "std           1.734815\n",
       "min           4.000000\n",
       "25%           4.000000\n",
       "50%           5.000000\n",
       "75%           6.000000\n",
       "max          35.000000\n",
       "Name: utrip_id, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"utrip_id\"].value_counts().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min utrip_id    1000033_1\n",
      "city_id             7\n",
      "dtype: object max utrip_id    999855_1\n",
      "city_id        67560\n",
      "dtype: object\n",
      "min utrip_id    0.0\n",
      "city_id     1.0\n",
      "dtype: float64 max utrip_id    148741.0\n",
      "city_id       5255.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "ord_encoder = OrdinalEncoder()\n",
    "print(\"min\",df[[\"utrip_id\",\"city_id\"]].min(),\"max\", df[[\"utrip_id\",\"city_id\"]].max())\n",
    "df[[\"utrip_id\",\"city_id\"]] = ord_encoder.fit_transform(df[[\"utrip_id\",\"city_id\"]] )\n",
    "df[\"city_id\"] =  df[\"city_id\"] + 1 ## start values at 1\n",
    "print(\"min\",df[[\"utrip_id\",\"city_id\"]].min(),\"max\", df[[\"utrip_id\",\"city_id\"]].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max city id 5256\n"
     ]
    }
   ],
   "source": [
    "vocab_size = int(df[\"city_id\"].max()+1)\n",
    "print(\"max city id\",vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### resume pipeline - lag features for city id only + fillna 0  (sequence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape (771966, 30)\n",
      "df_feat (144236, 33)\n"
     ]
    }
   ],
   "source": [
    "print(\"df.shape\",df.shape)\n",
    "# ### lag features - last n visits\n",
    "### orig: \n",
    "# df_feat = groupbyLagFeatures(df=df.copy(), \n",
    "#                    lag=[1,2],group=\"utrip_id\",lag_feature_cols=LAG_FEAT_COLS)\n",
    "\n",
    "df_feat = groupbyLagFeatures(df=df.copy(), \n",
    "                   lag=[1,2,3,4,5],group=\"utrip_id\",lag_feature_cols=[\"city_id\"])\n",
    "\n",
    "# df_feat = df_feat.dropna(subset=[\"lag2_city_id\"]).sample(frac=1)\n",
    "df_feat = df_feat.loc[df_feat[\"last\"]==1].drop([\"last\",\"row_num\"],axis=1,errors=\"ignore\")\n",
    "# df_feat = df_feat.drop(DROP_FEATS,axis=1,errors=\"ignore\")\n",
    "print(\"df_feat\",df_feat.shape)\n",
    "# assert df_feat[\"utrip_id\"].nunique() == df[\"utrip_id\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_id</th>\n",
       "      <th>lag1_city_id</th>\n",
       "      <th>lag2_city_id</th>\n",
       "      <th>lag3_city_id</th>\n",
       "      <th>lag4_city_id</th>\n",
       "      <th>lag5_city_id</th>\n",
       "      <th>utrip_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>770550</th>\n",
       "      <td>3525</td>\n",
       "      <td>4711</td>\n",
       "      <td>2237</td>\n",
       "      <td>3030</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117283</th>\n",
       "      <td>3214</td>\n",
       "      <td>3543</td>\n",
       "      <td>3624</td>\n",
       "      <td>4375</td>\n",
       "      <td>4022</td>\n",
       "      <td>4128</td>\n",
       "      <td>9696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426973</th>\n",
       "      <td>4503</td>\n",
       "      <td>1513</td>\n",
       "      <td>3283</td>\n",
       "      <td>1308</td>\n",
       "      <td>189</td>\n",
       "      <td>0</td>\n",
       "      <td>15043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818963</th>\n",
       "      <td>1338</td>\n",
       "      <td>3034</td>\n",
       "      <td>368</td>\n",
       "      <td>4707</td>\n",
       "      <td>1338</td>\n",
       "      <td>0</td>\n",
       "      <td>38636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230216</th>\n",
       "      <td>2816</td>\n",
       "      <td>3525</td>\n",
       "      <td>4306</td>\n",
       "      <td>732</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909369</th>\n",
       "      <td>2916</td>\n",
       "      <td>5030</td>\n",
       "      <td>1226</td>\n",
       "      <td>661</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990966</th>\n",
       "      <td>4765</td>\n",
       "      <td>4931</td>\n",
       "      <td>1700</td>\n",
       "      <td>4026</td>\n",
       "      <td>3970</td>\n",
       "      <td>4831</td>\n",
       "      <td>138729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379898</th>\n",
       "      <td>3024</td>\n",
       "      <td>1690</td>\n",
       "      <td>294</td>\n",
       "      <td>2526</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1078894</th>\n",
       "      <td>1229</td>\n",
       "      <td>88</td>\n",
       "      <td>1229</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368021</th>\n",
       "      <td>4009</td>\n",
       "      <td>4009</td>\n",
       "      <td>561</td>\n",
       "      <td>4407</td>\n",
       "      <td>1955</td>\n",
       "      <td>4523</td>\n",
       "      <td>138734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144236 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         city_id  lag1_city_id  lag2_city_id  lag3_city_id  lag4_city_id  \\\n",
       "770550      3525          4711          2237          3030             0   \n",
       "117283      3214          3543          3624          4375          4022   \n",
       "426973      4503          1513          3283          1308           189   \n",
       "818963      1338          3034           368          4707          1338   \n",
       "230216      2816          3525          4306           732             0   \n",
       "...          ...           ...           ...           ...           ...   \n",
       "909369      2916          5030          1226           661             0   \n",
       "990966      4765          4931          1700          4026          3970   \n",
       "379898      3024          1690           294          2526             0   \n",
       "1078894     1229            88          1229            88             0   \n",
       "368021      4009          4009           561          4407          1955   \n",
       "\n",
       "         lag5_city_id  utrip_id  \n",
       "770550              0      9695  \n",
       "117283           4128      9696  \n",
       "426973              0     15043  \n",
       "818963              0     38636  \n",
       "230216              0     54419  \n",
       "...               ...       ...  \n",
       "909369              0    138728  \n",
       "990966           4831    138729  \n",
       "379898              0    138731  \n",
       "1078894             0    138732  \n",
       "368021           4523    138734  \n",
       "\n",
       "[144236 rows x 7 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### keep just lag cols for basic model\n",
    "MINIMAL_COLS = ['city_id','lag1_city_id', 'lag2_city_id',\n",
    "       'lag3_city_id', 'lag4_city_id', \n",
    "                'lag5_city_id',\n",
    "               \"utrip_id\"]\n",
    "df_feat = df_feat[MINIMAL_COLS]\n",
    "df_feat = df_feat.fillna(0)\n",
    "df_feat = df_feat.apply(pd.to_numeric, errors='ignore',downcast=\"integer\")\n",
    "df_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5148\n"
     ]
    }
   ],
   "source": [
    "## reorder/encode target city id - sincei t has less values than full vocab\n",
    "final_target_encoder = LabelEncoder()\n",
    "df_feat[\"city_id\"] = final_target_encoder.fit_transform(df_feat[\"city_id\"])\n",
    "\n",
    "\n",
    "# num_classes = df_feat[\"city_id\"].nunique()+1 ## target outputs\n",
    "num_classes = int(df_feat[\"city_id\"].max())\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save intermediate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if SAVE_OUTPUTS:\n",
    "#     df_feat.to_parquet(path=\"df_merged_train.parquet\",index=False)\n",
    "#     df_candidates.to_parquet(path=\"df_merged_candidates.parquet\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# classes 5036\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lag1_city_id</th>\n",
       "      <th>lag2_city_id</th>\n",
       "      <th>lag3_city_id</th>\n",
       "      <th>lag4_city_id</th>\n",
       "      <th>lag5_city_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>770550</th>\n",
       "      <td>4711</td>\n",
       "      <td>2237</td>\n",
       "      <td>3030</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426973</th>\n",
       "      <td>1513</td>\n",
       "      <td>3283</td>\n",
       "      <td>1308</td>\n",
       "      <td>189</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818963</th>\n",
       "      <td>3034</td>\n",
       "      <td>368</td>\n",
       "      <td>4707</td>\n",
       "      <td>1338</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230216</th>\n",
       "      <td>3525</td>\n",
       "      <td>4306</td>\n",
       "      <td>732</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720067</th>\n",
       "      <td>5080</td>\n",
       "      <td>3106</td>\n",
       "      <td>4138</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1060933</th>\n",
       "      <td>524</td>\n",
       "      <td>1741</td>\n",
       "      <td>3803</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909369</th>\n",
       "      <td>5030</td>\n",
       "      <td>1226</td>\n",
       "      <td>661</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379898</th>\n",
       "      <td>1690</td>\n",
       "      <td>294</td>\n",
       "      <td>2526</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1078894</th>\n",
       "      <td>88</td>\n",
       "      <td>1229</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368021</th>\n",
       "      <td>4009</td>\n",
       "      <td>561</td>\n",
       "      <td>4407</td>\n",
       "      <td>1955</td>\n",
       "      <td>4523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108177 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         lag1_city_id  lag2_city_id  lag3_city_id  lag4_city_id  lag5_city_id\n",
       "770550           4711          2237          3030             0             0\n",
       "426973           1513          3283          1308           189             0\n",
       "818963           3034           368          4707          1338             0\n",
       "230216           3525          4306           732             0             0\n",
       "720067           5080          3106          4138             0             0\n",
       "...               ...           ...           ...           ...           ...\n",
       "1060933           524          1741          3803             0             0\n",
       "909369           5030          1226           661             0             0\n",
       "379898           1690           294          2526             0             0\n",
       "1078894            88          1229            88             0             0\n",
       "368021           4009           561          4407          1955          4523\n",
       "\n",
       "[108177 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################\n",
    "## Groupwise train/test split by user or utrip_id\n",
    "TARGET_COL = \"city_id\" ## overwrite\n",
    "train_inds, test_inds = next(GroupShuffleSplit(test_size=.25,\n",
    "                                               n_splits=2,\n",
    "                                               random_state = 12).split(df_feat,groups=df_feat['utrip_id']))\n",
    "\n",
    "X_train = df_feat.iloc[train_inds].drop(DROP_FEATS+[\"utrip_id\"],axis=1,errors=\"ignore\")#.merge(df_city_features,on=\"city_id\",how=\"left\")\n",
    "X_test = df_feat.iloc[test_inds].drop(DROP_FEATS +[\"utrip_id\"],axis=1,errors=\"ignore\")#.merge(df_city_features,on=\"city_id\",how=\"left\")\n",
    "\n",
    "y_train = X_train.pop(TARGET_COL)\n",
    "y_test = X_test.pop(TARGET_COL)\n",
    "\n",
    "print(\"# classes\",y_train.nunique())\n",
    "\n",
    "# ## check that same classes in train and test - \n",
    "# assert (set(y_train.unique()) == set(y_test.unique()))\n",
    "assert y_train.isna().max() == y_test.isna().max() == 0\n",
    "\n",
    "display(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding + multiclass model\n",
    "* apply self attention\n",
    "* Shared embedding - apply same embedding weights to all entities - https://stackoverflow.com/questions/42461673/apply-a-shared-embedding-layer-on-a-set-of-documents-in-keras\n",
    "\n",
    "* see also previous booking modelsL:\n",
    "    * https://www.kaggle.com/parlin987p/booking-sample-cnn#Bidirectional-+-masking \n",
    "    * https://www.kaggle.com/danofer/booking-sample-lstm-fork-1\n",
    "    * https://www.kaggle.com/danofer/booking-model-dan\n",
    "    \n",
    "Note - we could also add triplet/ ranking loss\n",
    "    * https://github.com/maciejkula/triplet_recommendations_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_recommenders as tfrs\n",
    "from tensorflow_recommenders.layers.dcn import Cross\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, AdditiveAttention, Attention, Dropout\n",
    "\n",
    "callback =[ tf.keras.callbacks.EarlyStopping(patience=3),  #,monitor=“val_mae”,baseline=5\n",
    "tf.keras.callbacks.ReduceLROnPlateau(factor=0.3, patience=2) ## we should reset the LR when finetuning ?\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### basic embedding + maxpooling model - trivial , very fast\n",
    "def pooling_model():\n",
    "    x = Input(shape=(X_train.shape[1],1,),name=\"city_lags\") ## dim set by # of lag features/cols\n",
    "    ## apply same embedding to all city - time distributed \n",
    "    emb = Embedding(input_dim=vocab_size+1, output_dim=300, mask_zero=True)\n",
    "    att = TimeDistributed(emb)(x)\n",
    "    att = TimeDistributed(GlobalMaxPooling1D())(att)\n",
    "    att = Flatten()(att)\n",
    "    out = Dense(num_classes+1,activation=\"softmax\")(att)\n",
    "    model = Model(x, out)\n",
    "    model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\", metrics=[tf.keras.metrics.SparseTopKCategoricalAccuracy(k=4)])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def self_attention_model_flat():\n",
    "    inp = Input(shape=(X_train.shape[1],1,),name=\"city_lags\") ## dim set by # of lag features/cols\n",
    "    ## apply same embedding to all city - time distributed \n",
    "    emb = Embedding(input_dim=vocab_size+1, output_dim=300, mask_zero=True)\n",
    "\n",
    "    x = TimeDistributed(emb)(inp)\n",
    "#     x = Flatten()(x) # orig\n",
    "    x = TimeDistributed(Flatten())(x) ## alt     (seemingly no difference)\n",
    "    att = Cross()(x)\n",
    "    att = Cross()(x,att)\n",
    "    att = Flatten()(att)\n",
    "    out = Dense(num_classes+1,activation=\"softmax\")(att)\n",
    "    model = Model(inp, out)\n",
    "    model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\", metrics=[tf.keras.metrics.SparseTopKCategoricalAccuracy(k=4)])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "def self_cross_model_dense():\n",
    "    inp = Input(shape=(X_train.shape[1],1,),name=\"city_lags\") ## dim set by # of lag features/cols\n",
    "    ## apply same embedding to all city - time distributed \n",
    "    emb = Embedding(input_dim=vocab_size+1, output_dim=300)\n",
    "\n",
    "    x = TimeDistributed(emb)(inp)\n",
    "    x = Cross()(x)\n",
    "#     x = Flatten()(x) # orig\n",
    "    x = TimeDistributed(Flatten())(x) ## alt     \n",
    "    att = Cross()(x)\n",
    "    att = Cross()(x,att)\n",
    "    att = Flatten()(att)\n",
    "    \n",
    "#     C = concatenate([att, x ### add raw inputs +- embedding - semi residual/skip. \n",
    "#                  ])\n",
    "    x = Dense(1024, activation=\"relu\")(att)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(2048, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    out = Dense(num_classes+1,activation=\"softmax\")(x)\n",
    "    model = Model(inp, out)\n",
    "    model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\", metrics=[tf.keras.metrics.SparseTopKCategoricalAccuracy(k=4)])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "def self_attention_flat():\n",
    "    inp = Input(shape=(X_train.shape[1],1,),name=\"city_lags\") ## dim set by # of lag features/cols\n",
    "    ## apply same embedding to all city - time distributed \n",
    "    emb = Embedding(input_dim=vocab_size+1, output_dim=300, mask_zero=True)\n",
    "\n",
    "    x = TimeDistributed(emb)(inp)\n",
    "#     x = Flatten()(x) # orig\n",
    "    x = TimeDistributed(Flatten())(x) ## alt     \n",
    "    att = MultiHeadAttention(num_heads=4, key_dim=4)(x,x) # return_attention_scores=True\n",
    "#     att = MultiHeadAttention(num_heads=4, key_dim=4)(att,att)\n",
    "\n",
    "    att = Flatten()(att)\n",
    "    out = Dense(num_classes+1,activation=\"softmax\")(att)\n",
    "    model = Model(inp, out)\n",
    "    model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\", metrics=[tf.keras.metrics.SparseTopKCategoricalAccuracy(k=4)])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def self_attention_model_dense():\n",
    "    inp = Input(shape=(X_train.shape[1],1,),name=\"city_lags\") ## dim set by # of lag features/cols\n",
    "    ## apply same embedding to all city - time distributed \n",
    "    emb = Embedding(input_dim=vocab_size+1, output_dim=300)\n",
    "\n",
    "    x = TimeDistributed(emb)(inp)\n",
    "#     x = Flatten()(x) # orig\n",
    "    x = TimeDistributed(Flatten())(x) ## alt     \n",
    "    att = MultiHeadAttention(num_heads=4, key_dim=4)(x,x)\n",
    "    att = MultiHeadAttention(num_heads=4, key_dim=4)(att,x)\n",
    "\n",
    "    att = Flatten()(att)\n",
    "    att = LayerNormalization()(att)\n",
    "#     C = concatenate([att, x ### add raw inputs +- embedding - semi residual/skip. \n",
    "#                  ])\n",
    "    \n",
    "    x = Dense(1024, activation=\"relu\")(att)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dense(1024, activation=\"relu\")(x)\n",
    "    att = LayerNormalization()(x)\n",
    "    out = Dense(num_classes+1,activation=\"softmax\")(att)\n",
    "    model = Model(inp, out)\n",
    "    model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\", metrics=[tf.keras.metrics.SparseTopKCategoricalAccuracy(k=4)])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def self_attention_model_2():\n",
    "    inp = Input(shape=(X_train.shape[1],1,),name=\"city_lags\") ## dim set by # of lag features/cols\n",
    "    ## apply same embedding to all city - time distributed \n",
    "    emb = Embedding(input_dim=vocab_size+1, output_dim=300,\n",
    "#                     mask_zero=True\n",
    "                   )\n",
    "\n",
    "    x = TimeDistributed(emb)(inp)\n",
    "#     x = Flatten()(x) # orig\n",
    "    x = TimeDistributed(Flatten())(x) ## alt     \n",
    "    att = Attention(use_scale=True,dropout=0.4)([x,x])\n",
    "    att = Attention(use_scale=True)([att,att])\n",
    "\n",
    "    att = Flatten()(att)\n",
    "    out = Dense(num_classes+1,activation=\"softmax\")(att)\n",
    "    model = Model(inp, out)\n",
    "    model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\", metrics=[tf.keras.metrics.SparseTopKCategoricalAccuracy(k=4)])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def self_attention_model_dense_2():\n",
    "    inp = Input(shape=(X_train.shape[1],1,),name=\"city_lags\") ## dim set by # of lag features/cols\n",
    "    ## apply same embedding to all city - time distributed \n",
    "    emb = Embedding(input_dim=vocab_size+1, output_dim=300)\n",
    "\n",
    "    x1 = TimeDistributed(emb)(inp)\n",
    "#     x = Flatten()(x) # orig\n",
    "    max_pooled = Flatten()(TimeDistributed(GlobalMaxPooling1D())(x1))\n",
    "    mean_pooled = Flatten()(TimeDistributed(GlobalAveragePooling1D())(x1))\n",
    "    x = TimeDistributed(Flatten())(x1) ## alt     \n",
    "    att = Attention(use_scale=True,dropout=0.25)([x,x])\n",
    "    att = Attention(use_scale=True)([att,att])\n",
    "    att = Flatten()(att)\n",
    "#     att = LayerNormalization()(att)\n",
    "    C = concatenate([att, max_pooled,mean_pooled ### add raw inputs +- embedding - semi residual/skip. \n",
    "                 ])\n",
    "#     C = Flatten()(C)\n",
    "    x = Dense(1024, activation=\"relu\")(C)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dense(1024, activation=\"relu\")(x)\n",
    "#     x = LayerNormalization()(x)\n",
    "    out = Dense(num_classes+1,activation=\"softmax\")(att)\n",
    "    model = Model(inp, out)\n",
    "    model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\", metrics=[tf.keras.metrics.SparseTopKCategoricalAccuracy(k=4)])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### could try copy sasREC keras code from : self-attention-recommendation  - it has dependnecies and failed for me (squeeze/dim bugs)\n",
    "# ## https://github.com/KanchiShimono/self-attention-recommendation/blob/master/sasrec/sasrec.py\n",
    "# # from /Users/oferd2/Documents/github/self-attention-recommendation-master import \n",
    "# import sys  \n",
    "# sys.path.insert(0, '/Users/oferd2/Documents/github/self-attention-recommendation-master/sasrec/')\n",
    "# ## dependencies of the sra \n",
    "# # ! pip install keras-self-attention keras-multi-head keras-pos-embd keras-position-wise-feed-forward keras-layer-normalization\n",
    "# # export TF_KERAS=1 # error? \n",
    "# import os\n",
    "# os.environ['TF_KERAS'] = \"1\"\n",
    "\n",
    "# from sasrec import *\n",
    "# # import sampler.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lag1_city_id</th>\n",
       "      <th>lag2_city_id</th>\n",
       "      <th>lag3_city_id</th>\n",
       "      <th>lag4_city_id</th>\n",
       "      <th>lag5_city_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>770550</th>\n",
       "      <td>4711</td>\n",
       "      <td>2237</td>\n",
       "      <td>3030</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426973</th>\n",
       "      <td>1513</td>\n",
       "      <td>3283</td>\n",
       "      <td>1308</td>\n",
       "      <td>189</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818963</th>\n",
       "      <td>3034</td>\n",
       "      <td>368</td>\n",
       "      <td>4707</td>\n",
       "      <td>1338</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230216</th>\n",
       "      <td>3525</td>\n",
       "      <td>4306</td>\n",
       "      <td>732</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720067</th>\n",
       "      <td>5080</td>\n",
       "      <td>3106</td>\n",
       "      <td>4138</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1060933</th>\n",
       "      <td>524</td>\n",
       "      <td>1741</td>\n",
       "      <td>3803</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909369</th>\n",
       "      <td>5030</td>\n",
       "      <td>1226</td>\n",
       "      <td>661</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379898</th>\n",
       "      <td>1690</td>\n",
       "      <td>294</td>\n",
       "      <td>2526</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1078894</th>\n",
       "      <td>88</td>\n",
       "      <td>1229</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368021</th>\n",
       "      <td>4009</td>\n",
       "      <td>561</td>\n",
       "      <td>4407</td>\n",
       "      <td>1955</td>\n",
       "      <td>4523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108177 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         lag1_city_id  lag2_city_id  lag3_city_id  lag4_city_id  lag5_city_id\n",
       "770550           4711          2237          3030             0             0\n",
       "426973           1513          3283          1308           189             0\n",
       "818963           3034           368          4707          1338             0\n",
       "230216           3525          4306           732             0             0\n",
       "720067           5080          3106          4138             0             0\n",
       "...               ...           ...           ...           ...           ...\n",
       "1060933           524          1741          3803             0             0\n",
       "909369           5030          1226           661             0             0\n",
       "379898           1690           294          2526             0             0\n",
       "1078894            88          1229            88             0             0\n",
       "368021           4009           561          4407          1955          4523\n",
       "\n",
       "[108177 rows x 5 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "1691/1691 [==============================] - 50s 29ms/step - loss: 5.1771 - accuracy: 0.2128 - sparse_top_k_categorical_accuracy: 0.3945 - val_loss: 3.8664 - val_accuracy: 0.2808 - val_sparse_top_k_categorical_accuracy: 0.5040\n",
      "Epoch 2/7\n",
      "1691/1691 [==============================] - 49s 29ms/step - loss: 3.3924 - accuracy: 0.2917 - sparse_top_k_categorical_accuracy: 0.5385 - val_loss: 3.7507 - val_accuracy: 0.2922 - val_sparse_top_k_categorical_accuracy: 0.5210\n",
      "Epoch 3/7\n",
      "1691/1691 [==============================] - 48s 28ms/step - loss: 2.8730 - accuracy: 0.3318 - sparse_top_k_categorical_accuracy: 0.6101 - val_loss: 3.8370 - val_accuracy: 0.2928 - val_sparse_top_k_categorical_accuracy: 0.5196\n",
      "Epoch 4/7\n",
      "1691/1691 [==============================] - 50s 30ms/step - loss: 2.4772 - accuracy: 0.3809 - sparse_top_k_categorical_accuracy: 0.6787 - val_loss: 4.0036 - val_accuracy: 0.2854 - val_sparse_top_k_categorical_accuracy: 0.5151\n",
      "Epoch 5/7\n",
      "1691/1691 [==============================] - 50s 30ms/step - loss: 1.9486 - accuracy: 0.4750 - sparse_top_k_categorical_accuracy: 0.7762 - val_loss: 4.1214 - val_accuracy: 0.2959 - val_sparse_top_k_categorical_accuracy: 0.5221\n"
     ]
    }
   ],
   "source": [
    "input_dim = vocab_size\n",
    "import tensorflow\n",
    "def self_attention_residual():\n",
    "    \"\"\"\n",
    "    Based on self attention recc, with changes, and doesn't use positional or multihead, and may not do fc right? \n",
    "    see orig: SAR code: https://github.com/KanchiShimono/self-attention-recommendation \n",
    "    \"\"\"\n",
    "    #     inputs = Input(shape=(1,X_train.shape[1]),name=\"food\") \n",
    "    inputs = keras.Input(shape=(X_train.shape[1]),name=\"food\") \n",
    "    # inputs = Input(shape=(None,),name=\"food\") \n",
    "    # inputs = keras.Input(shape=(None,))\n",
    "\n",
    "    ## apply same embedding to all city - time distributed \n",
    "    # emb = Embedding(input_dim=input_dim, output_dim=64,\n",
    "    #                 input_length=5\n",
    "    #                )###Embedding(input_dim=vocab_size+1, output_dim=128)\n",
    "    # #     x = TimeDistributed(emb)(inputs)\n",
    "    # x = emb()(inputs)\n",
    "\n",
    "    emd_dim = 128\n",
    "    x = Embedding(input_dim=input_dim, output_dim=emd_dim,\n",
    "                    input_length=5,\n",
    "                  mask_zero=True,\n",
    "                 )(inputs)\n",
    "    ### following could be stacked into multiple blocks\n",
    "    att = AdditiveAttention(use_scale=True,dropout=0.5)([x,x])\n",
    "    att = Add()([att, x])\n",
    "    feed_forward_input = tensorflow.keras.layers.LayerNormalization()(att)\n",
    "    feed_forward_x = Dense(emd_dim,activation='relu',)(feed_forward_input)\n",
    "    feed_forward_x = Dropout(0.4)(feed_forward_x)\n",
    "    feed_forward_x = Add()([feed_forward_input, feed_forward_x])\n",
    "    block_output = LayerNormalization(trainable=True)(feed_forward_x)\n",
    "    \n",
    "    ### opt: additional,residual attention block\n",
    "    x = block_output\n",
    "    att = AdditiveAttention(use_scale=True,dropout=0.5)([x,x]) \n",
    "    att = Add()([att, x])\n",
    "    feed_forward_input = tensorflow.keras.layers.LayerNormalization()(att)\n",
    "    feed_forward_x = Dense(emd_dim,activation='relu',)(feed_forward_input)\n",
    "    feed_forward_x = Dropout(0.4)(feed_forward_x)\n",
    "    feed_forward_x = Add()([feed_forward_input, feed_forward_x])\n",
    "    block_output = LayerNormalization(trainable=True)(feed_forward_x)\n",
    "       \n",
    "    block_output = Flatten()(block_output)\n",
    "    block_output = Dropout(0.1)(block_output)\n",
    "    #     att = GlobalAveragePooling1D()(att)\n",
    "    out = Dense(num_classes+1,activation=\"softmax\")(block_output)\n",
    "    model = Model(inputs, out)\n",
    "    #     model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics=[\"accuracy\",tf.keras.metrics.TopKCategoricalAccuracy(k=3)])\n",
    "    model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\",\n",
    "                  metrics=[\"accuracy\",tf.keras.metrics.SparseTopKCategoricalAccuracy(k=4)])\n",
    "#     print(model.summary())\n",
    "    return(model)\n",
    "\n",
    "model_sar = self_attention_residual()\n",
    "hist_sar = model_sar.fit( X_train.values,#np.array(X_train.values.tolist()),\n",
    "                         y_train,\n",
    "                 batch_size=64,\n",
    "                 validation_data=(X_test, y_test),\n",
    "                 epochs=7,\n",
    "                        callbacks= callback, verbose=1)\n",
    "\n",
    "# y_pred =np.argmax(model_sar.predict(X_test), axis=-1)\n",
    "# print(\"accuracy:{:.4f}\".format(metrics.accuracy_score(y_true=y_test,y_pred=y_pred)))\n",
    "\n",
    "#### .35 dropout, 300 embedding dim, 128 FC: \n",
    "\n",
    "    # ### with 1 block:\n",
    "    # Epoch 5/5 - 32s 19ms/step - loss: 1.4337 - accuracy: 0.5882 - sparse_top_k_categorical_accuracy: 0.8592 \n",
    "    #             - val_loss: 4.4571 - val_accuracy: 0.2870 - val_sparse_top_k_categorical_accuracy: 0.5228\n",
    "\n",
    "    ### with 2 self attention/residual blocks : \n",
    "    ## after 1 epoch, val_accuracy: 0.29 - val_sparse_top_k_categorical_accuracy: 0.523. \n",
    "    ### perf dropped (like in prev) after that, on val data\n",
    "    \n",
    "    \n",
    "    \n",
    "#### .45, .25 dropout, 128 embedding/FC dim: \n",
    "## \n",
    "# Epoch 5/7 - 50s 30ms/step - loss: 1.9486 - accuracy: 0.4750 - sparse_top_k_categorical_accuracy: 0.7762 - val_loss: 4.1214 - val_accuracy: 0.2959 - val_sparse_top_k_categorical_accuracy: 0.5221"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "1691/1691 [==============================] - 25s 14ms/step - loss: 5.3114 - accuracy: 0.2175 - sparse_top_k_categorical_accuracy: 0.3836 - val_loss: 3.7152 - val_accuracy: 0.2951 - val_sparse_top_k_categorical_accuracy: 0.5198\n",
      "Epoch 2/7\n",
      "1691/1691 [==============================] - 24s 14ms/step - loss: 3.2655 - accuracy: 0.3112 - sparse_top_k_categorical_accuracy: 0.5622 - val_loss: 3.6519 - val_accuracy: 0.3019 - val_sparse_top_k_categorical_accuracy: 0.5273\n",
      "Epoch 3/7\n",
      "1691/1691 [==============================] - 24s 14ms/step - loss: 2.7188 - accuracy: 0.3600 - sparse_top_k_categorical_accuracy: 0.6422 - val_loss: 3.7661 - val_accuracy: 0.2942 - val_sparse_top_k_categorical_accuracy: 0.5269\n",
      "Epoch 4/7\n",
      "1691/1691 [==============================] - 26s 15ms/step - loss: 2.3371 - accuracy: 0.4086 - sparse_top_k_categorical_accuracy: 0.7102 - val_loss: 3.9591 - val_accuracy: 0.2940 - val_sparse_top_k_categorical_accuracy: 0.5270\n",
      "Epoch 5/7\n",
      "1691/1691 [==============================] - 25s 15ms/step - loss: 1.8988 - accuracy: 0.4923 - sparse_top_k_categorical_accuracy: 0.7885 - val_loss: 4.0083 - val_accuracy: 0.3002 - val_sparse_top_k_categorical_accuracy: 0.5333\n"
     ]
    }
   ],
   "source": [
    "input_dim = vocab_size\n",
    "import tensorflow\n",
    "def self_attention_residual():\n",
    "    \"\"\"\n",
    "    Based on self attention recc, with changes, and doesn't use positional or multihead, and may not do fc right? \n",
    "    see orig: SAR code: https://github.com/KanchiShimono/self-attention-recommendation \n",
    "    \"\"\"\n",
    "    #     inputs = Input(shape=(1,X_train.shape[1]),name=\"food\") \n",
    "    inputs = keras.Input(shape=(X_train.shape[1]),name=\"food\") \n",
    "\n",
    "    emd_dim = 64\n",
    "    x = Embedding(input_dim=input_dim, output_dim=emd_dim,\n",
    "                    input_length=5,\n",
    "                  mask_zero=True,\n",
    "                 )(inputs)\n",
    "    ### following could be stacked into multiple blocks\n",
    "    att = AdditiveAttention(use_scale=True,dropout=0.5)([x,x])\n",
    "    att = Add()([att, x])\n",
    "    feed_forward_input = tensorflow.keras.layers.LayerNormalization()(att)\n",
    "    feed_forward_x = Dense(emd_dim,activation='relu',)(feed_forward_input)\n",
    "    feed_forward_x = Dropout(0.4)(feed_forward_x)\n",
    "    feed_forward_x = Add()([feed_forward_input, feed_forward_x])\n",
    "    block_output = tensorflow.keras.layers.LayerNormalization()(feed_forward_x)\n",
    "       \n",
    "    block_output = Flatten()(block_output)\n",
    "    block_output = Dropout(0.1)(block_output)\n",
    "    #     att = GlobalAveragePooling1D()(att)\n",
    "    out = Dense(num_classes+1,activation=\"softmax\")(block_output)\n",
    "    model = Model(inputs, out)\n",
    "    #     model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics=[\"accuracy\",tf.keras.metrics.TopKCategoricalAccuracy(k=3)])\n",
    "    model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\",\n",
    "                  metrics=[\"accuracy\",tf.keras.metrics.SparseTopKCategoricalAccuracy(k=4)])\n",
    "#     print(model.summary())\n",
    "    return(model)\n",
    "\n",
    "model_sar = self_attention_residual()\n",
    "hist_sar = model_sar.fit( X_train.values,#np.array(X_train.values.tolist()),\n",
    "                         y_train,\n",
    "                 batch_size=64,\n",
    "                 validation_data=(X_test, y_test),\n",
    "                 epochs=7,\n",
    "                        callbacks= callback, verbose=1)\n",
    "\n",
    "## single block, \n",
    "# Epoch 5/7 25s 15ms/step - loss: 1.8988 - accuracy: 0.4923 - sparse_top_k_categorical_accuracy: 0.7885 - val_loss: 4.0083 - val_accuracy: 0.3002 - val_sparse_top_k_categorical_accuracy: 0.5333\n",
    "                        \n",
    "## single att block,Attention (instead of AdditiveAttention) - slightly inferior: val_sparse_top_k_categorical_accuracy: 0.5293"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108177, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4711, 2237, 3030,    0,    0],\n",
       "       [1513, 3283, 1308,  189,    0],\n",
       "       [3034,  368, 4707, 1338,    0],\n",
       "       ...,\n",
       "       [1690,  294, 2526,    0,    0],\n",
       "       [  88, 1229,   88,    0,    0],\n",
       "       [4009,  561, 4407, 1955, 4523]], dtype=int16)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## transformer expect np.array, not multiple cols , so reshape\n",
    "# ## https://stackoverflow.com/questions/54731199/how-to-feed-multiple-pandas-columns-into-keras-for-learning\n",
    "\n",
    "# # X1 =  #np.array([np.array(l) for l in X])\n",
    "# X1 = np.array(X_train.values)\n",
    "# ## X1 = np.array(X_train.values.tolist())\n",
    "# print(X1.shape)\n",
    "# X1\n",
    "\n",
    "# # X1 = np.array(X_train.values.tolist()).astype('float32')# .shape\n",
    "# # print(X1.shape)\n",
    "# # X1\n",
    "\n",
    "\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# X1 = pad_sequences(X_train[cols].values.tolist())\n",
    "# print(X1.shape)\n",
    "# X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "city_lags (InputLayer)          [(None, 5, 1)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 5, 1, 300)    1577100     city_lags[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 5, 300)       0           time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 5, 300)       1           time_distributed_3[0][0]         \n",
      "                                                                 time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 5, 300)       1           attention_1[0][0]                \n",
      "                                                                 attention_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 1500)         0           attention_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 5149)         7728649     flatten_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 9,305,751\n",
      "Trainable params: 9,305,751\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/15\n",
      "1691/1691 [==============================] - 79s 46ms/step - loss: 5.7482 - sparse_top_k_categorical_accuracy: 0.3449 - val_loss: 3.9555 - val_sparse_top_k_categorical_accuracy: 0.4947\n",
      "Epoch 2/15\n",
      "1691/1691 [==============================] - 79s 47ms/step - loss: 3.7187 - sparse_top_k_categorical_accuracy: 0.5061 - val_loss: 3.7565 - val_sparse_top_k_categorical_accuracy: 0.5005\n",
      "Epoch 3/15\n",
      "1691/1691 [==============================] - 80s 47ms/step - loss: 3.4231 - sparse_top_k_categorical_accuracy: 0.5282 - val_loss: 3.7249 - val_sparse_top_k_categorical_accuracy: 0.5059\n",
      "Epoch 4/15\n",
      "1691/1691 [==============================] - 80s 47ms/step - loss: 3.1729 - sparse_top_k_categorical_accuracy: 0.5596 - val_loss: 3.7553 - val_sparse_top_k_categorical_accuracy: 0.5006\n",
      "Epoch 5/15\n",
      "1691/1691 [==============================] - 77s 46ms/step - loss: 2.9632 - sparse_top_k_categorical_accuracy: 0.5895 - val_loss: 3.7689 - val_sparse_top_k_categorical_accuracy: 0.4993\n",
      "Epoch 6/15\n",
      "1691/1691 [==============================] - 78s 46ms/step - loss: 2.6398 - sparse_top_k_categorical_accuracy: 0.6473 - val_loss: 3.7242 - val_sparse_top_k_categorical_accuracy: 0.5067\n",
      "Epoch 7/15\n",
      "1691/1691 [==============================] - 81s 48ms/step - loss: 2.5232 - sparse_top_k_categorical_accuracy: 0.6663 - val_loss: 3.7373 - val_sparse_top_k_categorical_accuracy: 0.5053\n",
      "Epoch 8/15\n",
      "1691/1691 [==============================] - 81s 48ms/step - loss: 2.4637 - sparse_top_k_categorical_accuracy: 0.6780 - val_loss: 3.7579 - val_sparse_top_k_categorical_accuracy: 0.5040\n",
      "Epoch 9/15\n",
      "1691/1691 [==============================] - 78s 46ms/step - loss: 2.3900 - sparse_top_k_categorical_accuracy: 0.6910 - val_loss: 3.7517 - val_sparse_top_k_categorical_accuracy: 0.5055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f96f4a74880>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = self_attention_model_2()\n",
    "\n",
    "model.fit(X_train,y_train,\n",
    "          epochs=15,\n",
    "          callbacks=callback, ## lrplateau, early stopping\n",
    "          validation_data=(X_test,y_test),\n",
    "          batch_size = 64,\n",
    "         )\n",
    "### 54  val topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = self_attention_flat()\n",
    "\n",
    "# model.fit(X_train,y_train,\n",
    "#           epochs=15,\n",
    "#           callbacks=callback, ## lrplateau, early stopping\n",
    "#           validation_data=(X_test,y_test),\n",
    "#           batch_size = 64,\n",
    "#          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "city_lags (InputLayer)          [(None, 5, 1)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, 5, 1, 300)    1577100     city_lags[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (None, 5, 300)       0           time_distributed_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention (MultiHead (None, 5, 300)       19548       time_distributed_7[0][0]         \n",
      "                                                                 time_distributed_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_1 (MultiHe (None, 5, 300)       19548       multi_head_attention[0][0]       \n",
      "                                                                 time_distributed_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 1500)         0           multi_head_attention_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_8 (LayerNor (None, 1500)         3000        flatten_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1024)         1537024     layer_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_9 (LayerNor (None, 1024)         2048        dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1024)         1049600     layer_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_10 (LayerNo (None, 1024)         2048        dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 5149)         5277725     layer_normalization_10[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 9,487,641\n",
      "Trainable params: 9,487,641\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/15\n",
      "1691/1691 [==============================] - 106s 62ms/step - loss: 5.2296 - sparse_top_k_categorical_accuracy: 0.3519 - val_loss: 3.8879 - val_sparse_top_k_categorical_accuracy: 0.4708\n",
      "Epoch 2/15\n",
      "1691/1691 [==============================] - 110s 65ms/step - loss: 3.6185 - sparse_top_k_categorical_accuracy: 0.4917 - val_loss: 3.7665 - val_sparse_top_k_categorical_accuracy: 0.4890\n",
      "Epoch 3/15\n",
      "1691/1691 [==============================] - 115s 68ms/step - loss: 3.3420 - sparse_top_k_categorical_accuracy: 0.5169 - val_loss: 3.7625 - val_sparse_top_k_categorical_accuracy: 0.4945\n",
      "Epoch 4/15\n",
      "1691/1691 [==============================] - 114s 68ms/step - loss: 3.1445 - sparse_top_k_categorical_accuracy: 0.5386 - val_loss: 3.8076 - val_sparse_top_k_categorical_accuracy: 0.4964\n",
      "Epoch 5/15\n",
      "1691/1691 [==============================] - 116s 68ms/step - loss: 2.9437 - sparse_top_k_categorical_accuracy: 0.5663 - val_loss: 3.9034 - val_sparse_top_k_categorical_accuracy: 0.4936\n",
      "Epoch 6/15\n",
      "1691/1691 [==============================] - 116s 68ms/step - loss: 2.5614 - sparse_top_k_categorical_accuracy: 0.6356 - val_loss: 4.2023 - val_sparse_top_k_categorical_accuracy: 0.4950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f96c1344a90>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = self_attention_model_dense()\n",
    "\n",
    "model.fit(X_train,y_train,\n",
    "          epochs=15,\n",
    "          callbacks=callback, ## lrplateau, early stopping\n",
    "          validation_data=(X_test,y_test),\n",
    "          batch_size = 64,\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "city_lags (InputLayer)          [(None, 5, 1)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_12 (TimeDistri (None, 5, 1, 300)    1577100     city_lags[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "cross (Cross)                   (None, 5, 1, 300)    90300       time_distributed_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_13 (TimeDistri (None, 5, 300)       0           cross[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "cross_1 (Cross)                 (None, 5, 300)       90300       time_distributed_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "cross_2 (Cross)                 (None, 5, 300)       90300       time_distributed_13[0][0]        \n",
      "                                                                 cross_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 1500)         0           cross_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 1024)         1537024     flatten_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 1024)         4096        dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 2048)         2099200     batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 2048)         8192        dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 5149)         10550301    batch_normalization_1[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 16,046,813\n",
      "Trainable params: 16,040,669\n",
      "Non-trainable params: 6,144\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/15\n",
      "1691/1691 [==============================] - 157s 92ms/step - loss: 5.5736 - sparse_top_k_categorical_accuracy: 0.3782 - val_loss: 3.8580 - val_sparse_top_k_categorical_accuracy: 0.5135\n",
      "Epoch 2/15\n",
      "1691/1691 [==============================] - 153s 90ms/step - loss: 3.1199 - sparse_top_k_categorical_accuracy: 0.5941 - val_loss: 3.7569 - val_sparse_top_k_categorical_accuracy: 0.5235\n",
      "Epoch 3/15\n",
      "1691/1691 [==============================] - 151s 89ms/step - loss: 2.3099 - sparse_top_k_categorical_accuracy: 0.7261 - val_loss: 4.0014 - val_sparse_top_k_categorical_accuracy: 0.5131\n",
      "Epoch 4/15\n",
      "1691/1691 [==============================] - 144s 85ms/step - loss: 1.7135 - sparse_top_k_categorical_accuracy: 0.8302 - val_loss: 4.5230 - val_sparse_top_k_categorical_accuracy: 0.5062\n",
      "Epoch 5/15\n",
      "1691/1691 [==============================] - 157s 93ms/step - loss: 1.0756 - sparse_top_k_categorical_accuracy: 0.9175 - val_loss: 4.6490 - val_sparse_top_k_categorical_accuracy: 0.5178\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f96c11400d0>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = self_cross_model_dense()\n",
    "\n",
    "model.fit(X_train,y_train,\n",
    "          epochs=15,\n",
    "          callbacks=callback, ## lrplateau, early stopping\n",
    "          validation_data=(X_test,y_test),\n",
    "          batch_size = 64,\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Layer global_max_pooling1d does not support masking, but was passed an input_mask: Tensor(\"Reshape:0\", shape=(None, 1), dtype=bool)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-3ba77d4b313f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpooling_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model.fit(X_train,y_train,\n\u001b[1;32m      4\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m## lrplateau, early stopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-77-c1a8161717b8>\u001b[0m in \u001b[0;36mpooling_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_zero\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0matt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0matt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGlobalMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0matt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;31m# >> model = tf.keras.Model(inputs, outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0m\u001b[1;32m    952\u001b[0m                                                 input_list)\n\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1088\u001b[0m           layer=self, inputs=inputs, build_graph=True, training=training_value):\n\u001b[1;32m   1089\u001b[0m         \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m         outputs = self._keras_tensor_symbolic_call(\n\u001b[0m\u001b[1;32m   1091\u001b[0m             inputs, input_masks, args, kwargs)\n\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 866\u001b[0;31m       self._set_mask_metadata(inputs, outputs, input_masks,\n\u001b[0m\u001b[1;32m    867\u001b[0m                               build_graph=False)\n\u001b[1;32m    868\u001b[0m       outputs = nest.map_structure(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_set_mask_metadata\u001b[0;34m(self, inputs, outputs, previous_mask, build_graph)\u001b[0m\n\u001b[1;32m   2539\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2541\u001b[0;31m     \u001b[0moutput_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2542\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moutput_masks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2543\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/layers/wrappers.py\u001b[0m in \u001b[0;36mcompute_mask\u001b[0;34m(self, inputs, mask)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0minner_input_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_shape_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0minner_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_input_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m     \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moutput_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mcompute_mask\u001b[0;34m(self, inputs, mask)\u001b[0m\n\u001b[1;32m    891\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_supports_masking\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m         raise TypeError('Layer ' + self.name + ' does not support masking, '\n\u001b[0m\u001b[1;32m    894\u001b[0m                         'but was passed an input_mask: ' + str(mask))\n\u001b[1;32m    895\u001b[0m       \u001b[0;31m# masking not explicitly supported: return None as mask.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Layer global_max_pooling1d does not support masking, but was passed an input_mask: Tensor(\"Reshape:0\", shape=(None, 1), dtype=bool)"
     ]
    }
   ],
   "source": [
    "model = pooling_model()\n",
    "\n",
    "model.fit(X_train,y_train,\n",
    "          epochs=15,\n",
    "          callbacks=callback, ## lrplateau, early stopping\n",
    "          validation_data=(X_test,y_test),\n",
    "          batch_size = 64,\n",
    "         )\n",
    "\n",
    "# # Epoch 7/10 - 20s - loss: 1.907 - accuracy: 0.479 - sparse_top_k_categorical_accuracy: 0.789 -\n",
    "# #             val_loss: 3.091 - val_accuracy: 0.3453 - val_sparse_top_k_categorical_accuracy: 0.5922"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = self_attention_model_flat_1()\n",
    "\n",
    "# model.fit(X_train,y_train,\n",
    "#           epochs=15,\n",
    "#           callbacks=callback, ## lrplateau, early stopping\n",
    "#           validation_data=(X_test,y_test),\n",
    "#           batch_size = 64,\n",
    "#          )\n",
    "\n",
    "# # Epoch 2/10\n",
    "# # 1527/1527 [==============================] - 23s 15ms/step - loss: 2.7761 - accuracy: 0.3593 - sparse_top_k_categorical_accuracy: 0.6269 - val_loss: 3.0526 - val_accuracy: 0.3387 - val_sparse_top_k_categorical_accuracy: 0.5939\n",
    "# # Epoch 6/10\n",
    "# # 1527/1527 [==============================] - 23s 15ms/step - loss: 1.3743 - accuracy: 0.5895 - sparse_top_k_categorical_accuracy: 0.8710 - val_loss: 3.8441 - val_accuracy: 0.3205 - val_sparse_top_k_categorical_accuracy: 0.5692"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pooling, 2 epochs: \n",
    "# 5s 8ms/step - loss: 2.9698 - accuracy: 0.3515 - sparse_top_k_categorical_accuracy: 0.6059 - val_loss: 2.8055 - val_accuracy: 0.3643 - val_sparse_top_k_categorical_accuracy: 0.6204"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top4_accuracy(model,y_test_pred=None):\n",
    "    \"\"\"warning - very hacky! - set to use global X_test, y_test variables\"\"\"\n",
    "    if y_test_pred is None:\n",
    "        y_test_pred = model.predict(X_test,prediction_type=\"Probability\")[:,1]\n",
    "\n",
    "    test_preds = pd.DataFrame({\"y_test_pred\":y_test_pred,\n",
    "#                                \"candidate_city_id\":X_test[\"city_id\"],\n",
    "                               \"utrip_id\":X_test[\"utrip_id\"],\n",
    "                               \"rank\":X_test[\"rank\"],\n",
    "#                                \"city_id_count\":X_test[\"city_id_count\"],\n",
    "                               \"label\":y_test\n",
    "                              })    \n",
    "    ## most frequent in overall data (not as target)\n",
    "    n_users = test_preds[\"utrip_id\"].nunique()\n",
    "\n",
    "#     test_preds.sort_values([\"utrip_id\",\"city_id_count\"],inplace=True,ascending=False)\n",
    "#     res_count = 100*test_preds.groupby(\"utrip_id\")[\"label\"].head(4).sum() / n_users\n",
    "#     print(\"top4 acc baseline: by most frequent city_id_count {0:.4f}\".format(res_count))\n",
    "\n",
    "    test_preds.sort_values([\"utrip_id\",\"rank\"],inplace=True,ascending=True)\n",
    "    res_rank = 100*test_preds.groupby(\"utrip_id\")[\"label\"].head(4).sum() / n_users\n",
    "    print(\"top4 acc baseline: by rank {0:.4f}\".format(res_rank))\n",
    "\n",
    "    test_preds.sort_values([\"utrip_id\",\"y_test_pred\"],inplace=True,ascending=False)\n",
    "    res = 100*test_preds.groupby(\"utrip_id\")[\"label\"].head(4).sum() / n_users\n",
    "    print(\"top4 acc: by model {0:.4f}\".format(res))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get predictions on competition/candidate data\n",
    "* `df_candidates`\n",
    "* There appear to be duplicate city_ids in the predictions ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if SAVE_OUTPUTS:\n",
    "    candidates_pool = Pool(data=df_candidates,cat_features=CAT_FEAT_NAMES\n",
    "#                      ,group_id=df_candidates[\"utrip_id\"]\n",
    "                     )\n",
    "    print(\"candidate pool done\")\n",
    "    preds = model.predict(candidates_pool,prediction_type=\"Probability\")[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # errors if using raw data - data type disparities . unclear why\n",
    "# ## thread_count\n",
    "# preds = model.predict(df_candidates,prediction_type=\"Probability\",verbose=True,thread_count=4)[:,1]\n",
    "# print(\"done\")\n",
    "# print(len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_OUTPUTS:\n",
    "    submission = pd.DataFrame({\n",
    "                               \"utrip_id\":df_candidates[\"utrip_id\"],\n",
    "                           \"city_id\":df_candidates[\"city_id\"],\n",
    "                           \"pred\":preds,\n",
    "    #                        \"rank\":df_candidates[\"rank\"],\n",
    "                          })   \n",
    "\n",
    "    submission.sort_values([\"pred\"],ascending=False,inplace=True)\n",
    "\n",
    "    ### unknonw bug - we get multiple cities per ID ???\n",
    "\n",
    "    print(\"duplicate cities suggested:\",submission.shape[0] - submission.drop_duplicates(['utrip_id','city_id']).shape[0])\n",
    "\n",
    "    #  if SAVE_OUTPUTS:\n",
    "    submission = submission.drop_duplicates(['utrip_id','city_id'],keep=\"first\")\n",
    "    print(\"submission.nunique()\",submission.nunique())\n",
    "    display(submission)\n",
    "    ## drop -1s:\n",
    "    submission = submission.loc[submission['city_id']>= 0]\n",
    "    submission = submission.groupby([\"utrip_id\"]).head(4).drop(\"pred\",axis=1)\n",
    "\n",
    "    submission[\"r_num\"] = submission.groupby(\"utrip_id\")[\"city_id\"].rank(ascending=True,pct=False)\n",
    "\n",
    "    submission = submission.pivot(index='utrip_id', columns='r_num', values='city_id').reset_index() ## , values='score'\n",
    "    submission.columns = [\"utrip_id\",'city_id_1','city_id_2','city_id_3','city_id_4']\n",
    "\n",
    "    assert submission[\"utrip_id\"].nunique() == test_num_ids\n",
    "    display(submission)       \n",
    "\n",
    "    submission.to_csv(\"submission.csv\",index=False)\n",
    "    print(submission.nunique())\n",
    "    print(submission.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## gets stuck with many rows\n",
    "\n",
    "# y_test_preds = model.predict(test_pool,prediction_type=\"Probability\")[:,1]\n",
    "# print(top4_accuracy(model,y_test_preds))\n",
    "\n",
    "### from run on sample, with old train-train lightfm ranking: \n",
    "# top4 acc baseline: by most frequent city_id_count 39.6007\n",
    "# top4 acc baseline: by rank 41.2646\n",
    "# top4 acc: by model 77.0383\n",
    "# 77.03826955074875\n",
    "\n",
    "# ## still ~ 160k sample , model without rank/score: \n",
    "# top4 acc baseline: by most frequent city_id_count 32.1688\n",
    "# top4 acc baseline: by rank 41.6647\n",
    "# top4 acc: by model 49.9179"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance\n",
    "* if usingSHAP: We don't want to wait too long/to calculate it over 40 million rows, so we could look at a sample. e.g. top ranked (rank <= 10 | label ==1)  , or first few rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top4_acc = top4_accuracy(model)\n",
    "# # print(top4_accuracy(model))\n",
    "\n",
    "# ## without sqrt balance, top4 acc: by model 56.9870"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_imp_cb = model.get_feature_importance(data=test_pool,\n",
    "#                        prettified=True,shap_calc_type =\"Approximate\")\n",
    "\n",
    "# feat_imp_cb = feat_imp_cb.loc[feat_imp_cb[\"Importances\"]>0].round(2)\n",
    "# display(feat_imp_cb.head(32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Evaluate model on test data\n",
    "    * function may have memory leakage issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using 30% of data:\n",
    "    * \n",
    "        top4 acc baseline: by most frequent city_id_count 23.99\n",
    "        \n",
    "        top4 acc baseline: by rank 41.87\n",
    "        \n",
    "        top4 acc: by model 55.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
