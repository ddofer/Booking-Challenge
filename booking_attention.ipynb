{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate features on all test + train and get predictions\n",
    "* Todo: seperate feature generation stage, and allow addition of CV models, etc'. \n",
    "* uses list of candidates from lightfm\n",
    "    * preprocess moti's predictions using code in `transform_moti_preds.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install catboost shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import\n",
    "\n",
    "from catboost import Pool, cv, CatBoostClassifier, CatBoostRegressor,CatBoost\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import Model\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "# import shap\n",
    "# shap.initjs()\n",
    "\n",
    "from copy import deepcopy\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit, GridSearchCV, GroupKFold\n",
    "from tensorflow.keras.metrics import TopKCategoricalAccuracy, Precision, SparseTopKCategoricalAccuracy # @4\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# from sklearn.metrics import top_k_accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "# from sklearn.linear_model import  LogisticRegressionCV\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "# from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "# from pytorch_tabnet.metrics import Metric\n",
    "# import torch\n",
    "\n",
    "import datetime as dt\n",
    "d1 = dt.datetime(2015,12,30) # arbitrary min start date , use for converting checkin, checkout to numbers in nice format\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## https://www.tensorflow.org/guide/mixed_precision ## TF mixed precision - pytorch requires other setup\n",
    "# from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "\n",
    "# policy = mixed_precision.Policy('mixed_float16')\n",
    "# mixed_precision.set_policy(policy)\n",
    "# ## will need to correct in places, e.g.: \n",
    "# ## outputs = layers.Activation('softmax', dtype='float32', name='predictions')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features to add:\n",
    "* Lag \n",
    "* Rank (popularity) of city, country (in general, +- given booker country)\n",
    "* Count of hotel; user, trip size ? (may be leaky )\n",
    "* Seasonal features - Holidays? , datetime\n",
    "\n",
    "Aggregate feats:\n",
    "* user changed country? last booking (lag 1) country change? \n",
    "* max/min/avg popularity rank of previous locations visited\n",
    "\n",
    "\n",
    "\n",
    "We should create a dictionary of the rank, count, city/country etc' feats, so we can easily merge them when making more \"negative\" samples/feats for ranking.\n",
    "\n",
    "\n",
    "* Consider using a df2 of df without dates + drop_duplicates, +- without user/trip id (After calcing that) .\n",
    "\n",
    "\n",
    "Leaky or potentially leaky (Dependso n test set): \n",
    "* Target freq features - frequency of target city, given source county +- affiliate +- month of year +- given country (and interactions of target freq). \n",
    "    * Risk of leaks - depends of test data has temporal split or not. \n",
    "    * cartboost can do target encode, but this lets us do it for interactions, e.g. target city freq given the 2 countries and affiliate.\n",
    "    * beware overfitting! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "FASTRUN = False #True False\n",
    "\n",
    "if FASTRUN:\n",
    "    max_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_OUTPUTS = False ## save intermediate files to disk\n",
    "\n",
    "TARGET_COL = \"label\"\n",
    "# most basic categorical columns , without 'user_id', , 'utrip_id' ordevice_class - used for count encoding/filtering\n",
    "BASE_CAT_COLS = ['city_id',  'affiliate_id', 'booker_country', 'hotel_country']\n",
    "\n",
    "### features to get lags for. Not very robust. May want different feats for lags before -1\n",
    "LAG_FEAT_COLS = ['city_id', \n",
    "#                  'device_class',\n",
    "       'affiliate_id',\n",
    "#                  'booker_country',\n",
    "                 'hotel_country', \n",
    "       'duration', 'same_country', \n",
    "#                  'checkin_weekday',\n",
    "       'checkin_week',\n",
    "#         'checkout_weekday',\n",
    "       'city_id_count', 'affiliate_id_count',\n",
    "                 'hotel_country_count', \n",
    "#                  'checkin_week_count',\n",
    "                 'city_id_rank_by_hotel_country',\n",
    "#        'city_id_rank_by_booker_country', ## there are only 5 booker countries\n",
    "                 'city_id_rank_by_affiliate',\n",
    "#        'affiliate_id_rank_by_hotel_country',\n",
    "#        'affiliate_id_rank_by_booker_country', \n",
    "                ]\n",
    "\n",
    "CAT_FEAT_NAMES = [\"booker_country\", \"device_class\",\"affiliate_id\",\n",
    "                  \"city_id\",\"hotel_country\",\n",
    "#                   \"utrip_id\", ## changed\n",
    "#                   \"user_id\", ## ? could use lower dim - depends on train/test overlap\n",
    "                  \"checkin_week\",#\"checkout_week\",\n",
    "#                     \"checkin_weekday\",\n",
    "    \"lag1_city_id\",\"lag1_hotel_country\",\"lag1_affiliate_id\", #\"lag1_device_class\",\n",
    "     \"lag2_city_id\",\"lag2_hotel_country\",#\"lag2_affiliate_id\",#\"lag2_device_class\",\n",
    "       \"lag3_city_id\",\"lag3_hotel_country\", #\"lag3_booker_country\",\"lag3_affiliate_id\",\"lag3_device_class\",\n",
    "                  \"first_hotel_country\",\"first_city_id\"\n",
    "                 ]\n",
    "\n",
    "TRAIN_FILE_PATH = \"booking_train_set.csv\" #\"/content/drive/MyDrive/booking_wisdom/booking_train_set.csv\" #\"booking_train_set.csv\"\n",
    "\n",
    "LIST_FILE_PATH =   \"all_candidates_list.parquet\"#\"predictions_list_proc_v2.csv.gz\"#\"predictions_list_proc_v2.csv.gz\"\n",
    "\n",
    "TEST_FILE_PATH = \"booking_test_set.csv\" #\"/content/drive/MyDrive/booking_wisdom/booking_train_set.csv\" #\"booking_train_set.csv\"\n",
    "# TEST_LIST_FILE_PATH =  \"list_booking_train.csv.gz\"#\"/content/drive/MyDrive/booking_wisdom/list_booking_train.csv.gz\" #\"list_booking_train.csv.gz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/33907537/groupby-and-lag-all-columns-of-a-dataframe\n",
    "# https://stackoverflow.com/questions/62924987/lag-multiple-variables-grouped-by-columns\n",
    "## lag features with groupby over many columns: \n",
    "def groupbyLagFeatures(df:pd.DataFrame,lag:[]=[1,2],group=\"utrip_id\",lag_feature_cols=[]):\n",
    "    \"\"\"\n",
    "    lag features with groupby over many columns\n",
    "    https://stackoverflow.com/questions/62924987/lag-multiple-variables-grouped-by-columns\"\"\"\n",
    "    if len(lag_feature_cols)>0:\n",
    "        df=pd.concat([df]+[df.groupby(group)[lag_feature_cols].shift(x).add_prefix('lag'+str(x)+\"_\") for x in lag],axis=1)\n",
    "    else:\n",
    "         df=pd.concat([df]+[df.groupby(group).shift(x).add_prefix('lag'+str(x)+\"_\") for x in lag],axis=1)\n",
    "    return df\n",
    "\n",
    "def groupbyFirstLagFeatures(df:pd.DataFrame,group=\"user_id\",lag_feature_cols=[]):\n",
    "    \"\"\"\n",
    "    Get  first/head value lag-like of features with groupby over columns. Assumes sorted data!\n",
    "    \"\"\"\n",
    "    if len(lag_feature_cols)>0:\n",
    "        df=pd.concat([df]+[df.groupby(group)[lag_feature_cols].transform(\"first\").add_prefix(\"first_\")],axis=1)\n",
    "    else:\n",
    "#          df=pd.concat([df]+[df.groupby(group).first().add_prefix(\"first_\")],axis=1)\n",
    "        df=pd.concat([df]+[df.groupby(group).transform(\"first\").add_prefix(\"first_\")],axis=1)\n",
    "    return df\n",
    "\n",
    "######## Get n most popular items, per group\n",
    "def most_popular(group, n_max=4):\n",
    "    \"\"\"Find most popular hotel clusters by destination\n",
    "    Define a function to get most popular hotels for a destination group.\n",
    "\n",
    "    Previous version used nlargest() Series method to get indices of largest elements. But the method is rather slow.\n",
    "    Source: https://www.kaggle.com/dvasyukova/predict-hotel-type-with-pandas\n",
    "    \"\"\"\n",
    "    relevance = group['relevance'].values\n",
    "    hotel_cluster = group['hotel_cluster'].values\n",
    "    most_popular = hotel_cluster[np.argsort(relevance)[::-1]][:n_max]\n",
    "    return np.array_str(most_popular)[1:-1] # remove square brackets\n",
    "\n",
    "from tensorflow.keras.metrics import top_k_categorical_accuracy\n",
    "def top_4_accuracy(y_true, y_pred):\n",
    "    \"\"\"will only work if doing multiclass predictions\"\"\"\n",
    "    return top_k_categorical_accuracy(y_true, y_pred, k=4)\n",
    "\n",
    "## https://codereview.stackexchange.com/questions/149306/select-the-n-most-frequent-items-from-a-pandas-groupby-dataframe\n",
    "# https://stackoverflow.com/questions/52073054/group-by-a-column-to-find-the-most-frequent-value-in-another-column\n",
    "## can get modes (sorted)\n",
    "# https://stackoverflow.com/questions/50592762/finding-most-common-values-with-pandas-groupby-and-value-counts\n",
    "## df.groupby('tag')['category'].agg(lambda x: x.value_counts().index[0])\n",
    "# https://stackoverflow.com/questions/15222754/groupby-pandas-dataframe-and-select-most-common-value\n",
    "# source2.groupby(['Country','City'])['Short name'].agg(pd.Series.mode)\n",
    "\n",
    "\n",
    "# def fit_cbr_model(loss_function, additional_params=None, train_pool=train_pool, test_pool=test_pool,\n",
    "#                   CV=True,return_model=False):\n",
    "#     \"\"\"\n",
    "#     Fit catboost ranking model, or CV (cross validate) and return scores ; with a given loss function\n",
    "#     https://colab.research.google.com/github/catboost/tutorials/blob/master/ranking/ranking_tutorial.ipynb#scrollTo=RVNW0nowbtxH\n",
    "#     >>>model = fit_model('PairLogit', {'custom_metric': ['PrecisionAt:top=10', 'RecallAt:top=4']})\n",
    "    \n",
    "#     \"\"\"\n",
    "#     parameters = deepcopy(default_parameters)\n",
    "#     parameters['loss_function'] = loss_function\n",
    "#     parameters['train_dir'] = loss_function\n",
    "    \n",
    "#     if additional_params is not None:\n",
    "#         parameters.update(additional_params)\n",
    "         \n",
    "#     if CV:\n",
    "#         scores = cv(train_pool,params=parameters, plot=True,fold_count=2)\n",
    "#         scores = scores[scores.columns[~scores.columns.str.contains(\"std|pair\",case=False)]]\n",
    "#         display(scores.tail())\n",
    "        \n",
    "#     ## train model to get hacky top4 accuracy\n",
    "#     model = CatBoost(parameters)\n",
    "#     model.fit(train_pool, eval_set=test_pool, plot=False)        \n",
    "#     y_test_pred = model.predict(X_test,prediction_type=\"Probability\")[:,1]\n",
    "#     print(\"Test:\")\n",
    "#     top4_acc = top4_accuracy(model,y_test_pred)\n",
    "#     auc_score = roc_auc_score(y_true=y_test,y_score=y_test_pred)\n",
    "#     print(\"Roc-AUC (test): {0:.4f}\".format(100*auc_score))\n",
    " \n",
    "#     if return_model:\n",
    "#         return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes_dict = {\n",
    "#     \"utrip_id\":\"category\",\n",
    "#     \"city_id\":\"int32\",\n",
    "#     \"rank\":\"int16\",\n",
    "    \"label\": \"int8\",#\"uint8\",\n",
    "    \"affiliate_id\":\"int32\",\n",
    "#     \"device_class\":\"category\",\n",
    "#     \"booker_country\":\"string\", ## category would save more memory but intereferes with feature engineering\n",
    "#     \"hotel_country\":\"string\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv(TRAIN_FILE_PATH,usecols=[\"city_id\"]).min() ## lowest normal num in train city_id is 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>checkin</th>\n",
       "      <th>checkout</th>\n",
       "      <th>city_id</th>\n",
       "      <th>device_class</th>\n",
       "      <th>affiliate_id</th>\n",
       "      <th>booker_country</th>\n",
       "      <th>hotel_country</th>\n",
       "      <th>utrip_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1004862</th>\n",
       "      <td>29</td>\n",
       "      <td>2016-07-09</td>\n",
       "      <td>2016-07-11</td>\n",
       "      <td>47054</td>\n",
       "      <td>desktop</td>\n",
       "      <td>1601</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>29_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004863</th>\n",
       "      <td>29</td>\n",
       "      <td>2016-07-11</td>\n",
       "      <td>2016-07-13</td>\n",
       "      <td>34444</td>\n",
       "      <td>desktop</td>\n",
       "      <td>1601</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>29_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004864</th>\n",
       "      <td>29</td>\n",
       "      <td>2016-07-13</td>\n",
       "      <td>2016-07-16</td>\n",
       "      <td>12291</td>\n",
       "      <td>desktop</td>\n",
       "      <td>1601</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>29_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004865</th>\n",
       "      <td>29</td>\n",
       "      <td>2016-07-16</td>\n",
       "      <td>2016-07-18</td>\n",
       "      <td>16386</td>\n",
       "      <td>desktop</td>\n",
       "      <td>8132</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>29_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897811</th>\n",
       "      <td>81</td>\n",
       "      <td>2016-05-15</td>\n",
       "      <td>2016-05-16</td>\n",
       "      <td>33665</td>\n",
       "      <td>desktop</td>\n",
       "      <td>9924</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>81_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1072993</th>\n",
       "      <td>6258065</td>\n",
       "      <td>2016-04-19</td>\n",
       "      <td>2016-04-20</td>\n",
       "      <td>55044</td>\n",
       "      <td>mobile</td>\n",
       "      <td>9452</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Pullamawang</td>\n",
       "      <td>6258065_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420479</th>\n",
       "      <td>6258087</td>\n",
       "      <td>2016-08-03</td>\n",
       "      <td>2016-08-04</td>\n",
       "      <td>17754</td>\n",
       "      <td>desktop</td>\n",
       "      <td>2436</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>6258087_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420480</th>\n",
       "      <td>6258087</td>\n",
       "      <td>2016-08-04</td>\n",
       "      <td>2016-08-05</td>\n",
       "      <td>50073</td>\n",
       "      <td>desktop</td>\n",
       "      <td>2436</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>6258087_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420481</th>\n",
       "      <td>6258087</td>\n",
       "      <td>2016-08-05</td>\n",
       "      <td>2016-08-06</td>\n",
       "      <td>11662</td>\n",
       "      <td>desktop</td>\n",
       "      <td>2436</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>6258087_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420482</th>\n",
       "      <td>6258087</td>\n",
       "      <td>2016-08-06</td>\n",
       "      <td>2016-08-07</td>\n",
       "      <td>41484</td>\n",
       "      <td>desktop</td>\n",
       "      <td>2436</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>6258087_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1166835 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id    checkin   checkout  city_id device_class  affiliate_id  \\\n",
       "1004862       29 2016-07-09 2016-07-11    47054      desktop          1601   \n",
       "1004863       29 2016-07-11 2016-07-13    34444      desktop          1601   \n",
       "1004864       29 2016-07-13 2016-07-16    12291      desktop          1601   \n",
       "1004865       29 2016-07-16 2016-07-18    16386      desktop          8132   \n",
       "897811        81 2016-05-15 2016-05-16    33665      desktop          9924   \n",
       "...          ...        ...        ...      ...          ...           ...   \n",
       "1072993  6258065 2016-04-19 2016-04-20    55044       mobile          9452   \n",
       "420479   6258087 2016-08-03 2016-08-04    17754      desktop          2436   \n",
       "420480   6258087 2016-08-04 2016-08-05    50073      desktop          2436   \n",
       "420481   6258087 2016-08-05 2016-08-06    11662      desktop          2436   \n",
       "420482   6258087 2016-08-06 2016-08-07    41484      desktop          2436   \n",
       "\n",
       "        booker_country hotel_country   utrip_id  \n",
       "1004862        Elbonia       Elbonia       29_1  \n",
       "1004863        Elbonia       Elbonia       29_1  \n",
       "1004864        Elbonia       Elbonia       29_1  \n",
       "1004865        Elbonia       Elbonia       29_1  \n",
       "897811         Elbonia       Elbonia       81_1  \n",
       "...                ...           ...        ...  \n",
       "1072993         Gondal   Pullamawang  6258065_1  \n",
       "420479          Gondal        Gondal  6258087_1  \n",
       "420480          Gondal        Gondal  6258087_1  \n",
       "420481          Gondal        Gondal  6258087_1  \n",
       "420482          Gondal        Gondal  6258087_1  \n",
       "\n",
       "[1166835 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if FASTRUN:\n",
    "    df = pd.read_csv(TRAIN_FILE_PATH,\n",
    "                     nrows=500_280,\n",
    "                     index_col=[0],\n",
    "                     parse_dates=[\"checkin\",\"checkout\"],infer_datetime_format=True, dtype=dtypes_dict)\n",
    "else:\n",
    "    df = pd.read_csv(TRAIN_FILE_PATH,\n",
    "                     index_col=[0],\n",
    "                     parse_dates=[\"checkin\",\"checkout\"],infer_datetime_format=True, dtype=dtypes_dict)\n",
    "\n",
    "# ## list of candidate predictions (100 per utrip_id)\n",
    "# df_list = pd.read_parquet(LIST_FILE_PATH,\n",
    "# #                         usecols=[0,2,3], # skip city_id col\n",
    "# #                         dtype=dtypes_dict\n",
    "#                      )\n",
    "\n",
    "# df_list = df_list.loc[df_list[\"rank\"]<50]\n",
    "# print(\"rank of labels (target =1):\")\n",
    "# # print(df_list.loc[df_list[\"label\"]==1][\"rank\"].describe(percentiles=[.25,.5,.75,.9,.95,.99]))\n",
    "\n",
    "# print(df_list.shape[0])\n",
    "\n",
    "# assert df_list.drop_duplicates([\"utrip_id\",\"city_id\"]).shape[0] == df_list.shape[0]\n",
    "\n",
    "# if FASTRUN:\n",
    "#     df_list = df_list.loc[df_list[\"rank\"]<=25]\n",
    "# #     df_list = df_list.loc[(df_list[\"rank\"]<=12) | (df_list[\"label\"]==1)]  \n",
    "# else:\n",
    "#     #### downsampole train data, less so for test data/candidates\n",
    "#     # # ## HACK due to running out of mempory ! sample of data \n",
    "    \n",
    "#     train_ids_all = list(df[\"utrip_id\"].unique())\n",
    "#     print(\"len(train_ids_all)\",len(train_ids_all))\n",
    "#     df_list_test = df_list.loc[~df_list[\"utrip_id\"].isin(train_ids_all)]\n",
    "    \n",
    "#     ## temp: for faster iterations\n",
    "#     df_list_test = df_list_test.loc[df_list_test[\"rank\"]<= 100]\n",
    "    \n",
    "#     print(\"list test shape\",df_list_test.shape)\n",
    "#     df_list_train = df_list.loc[df_list[\"utrip_id\"].isin(train_ids_all)]\n",
    "#     print(\"train test shape\",df_list_train.shape)\n",
    "    \n",
    "#     df_list_1 = df_list_train.loc[(df_list_train[\"rank\"]<=60) | (df_list_train[\"label\"]==1)]    \n",
    "#     df_list_2 = df_list_train.loc[(df_list_train[\"rank\"]>60) & (df_list_train[\"label\"]==0)].groupby([\"utrip_id\"]).sample(n=15,random_state=42)\n",
    "#     df_list = pd.concat([df_list_1,df_list_2,df_list_test])\n",
    "#     del df_list_1,df_list_2,df_list_test,df_list_train\n",
    "#     gc.collect()\n",
    "\n",
    "# print(\"df list shape\",df_list.shape[0])\n",
    "\n",
    "# ### df_test has \"missing\" cities, hotel countries\n",
    "#  ## replace missing cities with nans or -9\n",
    "# ### df_test[\"city_id\"] = df_test[\"city_id\"].replace(0,np.nan)\n",
    "\n",
    "# # df_list = df_list.loc[df_list[\"city_id\"] >0]\n",
    "\n",
    "# df_list.set_index(\"utrip_id\",inplace=True)\n",
    "                     \n",
    "df.sort_values([\"user_id\",\"checkin\"],inplace=True)\n",
    "# df_list.set_index(\"utrip_id\",inplace=True)\n",
    "\n",
    "\n",
    "## add feature - score relative to rank (could add vs utrip_id - equiv to rank)\n",
    "# df_list[\"score_norm\"] = df_list[\"score\"].div(df_list.groupby(\"rank\")[\"score\"].transform(\"mean\"))#.round(3)\n",
    "\n",
    "\n",
    "display(df)\n",
    "# display(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = pd.read_csv(TEST_FILE_PATH, parse_dates=[\"checkin\",\"checkout\"],infer_datetime_format=True, dtype=dtypes_dict)\n",
    "# ### note the missing city id in last rows\n",
    "# print(df_test.shape)\n",
    "# test_num_ids = df_test[\"utrip_id\"].nunique()\n",
    "# print(\"test_num_ids\",test_num_ids)\n",
    "# df_test[\"train\"] = 0\n",
    "\n",
    "# ### df_test has \"missing\" cities, hotel countries\n",
    "# # df_list[\"city_id\"] = df_list[\"city_id\"].replace(0,-9)# np.nan ## replace missing cities with nans or -9\n",
    "# df_test[\"city_id\"] = df_test[\"city_id\"].replace(0,np.nan) ## 0 are missing cities here\n",
    "\n",
    "# df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.166835e+06\n",
       "mean     6.117143e+00\n",
       "std      2.796383e+00\n",
       "min      1.000000e+00\n",
       "25%      4.000000e+00\n",
       "50%      5.000000e+00\n",
       "75%      7.000000e+00\n",
       "max      4.800000e+01\n",
       "Name: total_rows, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### add features to be consistent with test set of row in trip, and total trips in trip\n",
    "df[\"row_num\"] = df.groupby(\"utrip_id\")[\"checkin\"].rank(ascending=True,pct=False).astype(int)\n",
    "utrip_counts = df[\"utrip_id\"].value_counts()\n",
    "df[\"total_rows\"] = df[\"utrip_id\"].map(utrip_counts)\n",
    "\n",
    "df[\"train\"] = 1\n",
    "\n",
    "df[\"total_rows\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166835\n"
     ]
    }
   ],
   "source": [
    "# ### stack train, test for now (easier feat eng)\n",
    "# df = pd.concat([df,df_test])\n",
    "print(df.shape[0])\n",
    "\n",
    "df[\"last\"] = (df[\"row_num\"] ==df[\"total_rows\"]).astype(int)\n",
    "assert 0 == df[[\"row_num\",\"total_rows\",\"last\"]].isna().sum().max()\n",
    "# df[[\"row_num\",\"total_rows\",\"last\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1102338\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diff_total_rows</th>\n",
       "      <th>total_rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.102338e+06</td>\n",
       "      <td>1.102338e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.314553e-01</td>\n",
       "      <td>6.101682e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.809714e-01</td>\n",
       "      <td>2.728969e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.200000e+01</td>\n",
       "      <td>4.800000e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       diff_total_rows    total_rows\n",
       "count     1.102338e+06  1.102338e+06\n",
       "mean      2.314553e-01  6.101682e+00\n",
       "std       6.809714e-01  2.728969e+00\n",
       "min       0.000000e+00  1.000000e+00\n",
       "25%       0.000000e+00  4.000000e+00\n",
       "50%       0.000000e+00  5.000000e+00\n",
       "75%       0.000000e+00  7.000000e+00\n",
       "max       4.200000e+01  4.800000e+01"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Drop duplicate, consecutive rows\n",
    "df = df.loc[(df[[\"city_id\",\"utrip_id\",\"booker_country\",\"last\"]].shift() != df[[\"city_id\",\"utrip_id\",\"booker_country\",\"last\"]]).max(axis=1)]\n",
    "print(df.shape[0])\n",
    "\n",
    "## add alt total rows as a diff, for marking cases where we dropped dupes.\n",
    "\n",
    "utrip_counts_2 = df[\"utrip_id\"].value_counts()\n",
    "df[\"diff_total_rows\"] = df[\"utrip_id\"].map(utrip_counts_2)\n",
    "df[\"diff_total_rows\"] =df[\"total_rows\"] -  df[\"diff_total_rows\"]\n",
    "\n",
    "df[[\"diff_total_rows\",\"total_rows\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### todo : add \"time since start (min date in data / \"duration\")\"  \n",
    "### todo : \"time since start of trip (min date per trip \"duration\")\"  \n",
    "df[\"duration\"] = ((df[\"checkout\"] - df[\"checkin\"]).dt.days).astype(int)\n",
    "df[\"same_country\"] = (df[\"booker_country\"]==df[\"hotel_country\"]).astype(int)\n",
    "\n",
    "df[\"checkin_day\"] = df[\"checkin\"].dt.day\n",
    "df[\"checkin_weekday\"] = df[\"checkin\"].dt.weekday\n",
    "df[\"checkin_week\"] = df[\"checkin\"].dt.isocalendar().week.astype(int) ## week of year\n",
    "df[\"checkin_month\"] = df[\"checkin\"].dt.month\n",
    "df[\"checkin_year\"] = df[\"checkin\"].dt.year-2015\n",
    "df[\"checkin_day_of_year\"] = df[\"checkin\"].dt.dayofyear  # day of year - may overfit , but will allow learning holidays and other checkins on that date \n",
    "\n",
    "# df[\"checkin_quarter\"] = df[\"checkin\"].dt.quarter # relatively redundant but may be used for \"id\"\n",
    "\n",
    "# df[\"checkout_weekday\"] = df[\"checkout\"].dt.weekday\n",
    "df[\"checkout_week\"] = df[\"checkout\"].dt.isocalendar().week.astype(int) ## week of year\n",
    "df[\"checkout_day\"] = df[\"checkout\"].dt.day ## day of month\n",
    "\n",
    "## cyclical datetime embeddings\n",
    "## drop originakl variables? \n",
    "## TODO:L add for other variables, +- those that we'll embed (week?)\n",
    "\n",
    "# df['checkin_weekday_sin'] = np.sin(df[\"checkin_weekday\"]*(2.*np.pi/7))\n",
    "# df['checkin_weekday_cos'] = np.cos(df[\"checkin_weekday\"]*(2.*np.pi/7))\n",
    "\n",
    "# df['checkin_month_sin'] = np.sin((df[\"checkin_month\"]-1)*(2.*np.pi/12))\n",
    "# df['checkin_month_cos'] = np.cos((df[\"checkin_month\"]-1)*(2.*np.pi/12))\n",
    "\n",
    "df['checkin_week_sin'] = np.sin((df[\"checkin_week\"]-1)*(2.*np.pi/53))\n",
    "df['checkin_week_cos'] = np.cos((df[\"checkin_week\"]-1)*(2.*np.pi/53))\n",
    "\n",
    "df.drop([\"checkin_month\"],axis=1,errors=\"ignore\")\n",
    "\n",
    "### time since first date in dataset (i.,e index proxy)\n",
    "min_date = df[\"checkin\"].min()\n",
    "df[\"time_elapsed_global\"] = (df[\"checkin\"] - min_date).dt.days.astype(int)\n",
    "\n",
    "# #############\n",
    "# # last number in utrip id - probably which trip number it is:\n",
    "# df[\"utrip_number\"] = df[\"utrip_id\"].str.split(\"_\",expand=True)[1].astype(int)\n",
    "\n",
    "### encode string columns - must be consistent with test data \n",
    "### IF we can concat test with train, we can just do a single transformation  for the NON TARGET cols\n",
    "# obj_cols_list = df.select_dtypes(\"O\").columns.values\n",
    "obj_cols_list = ['device_class','booker_country','hotel_country'] # we could also define when loading data, dtype\n",
    "\n",
    "for c in obj_cols_list:\n",
    "    df[c] = df[c].astype(\"category\")\n",
    "    df[c] = df[c].cat.codes.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_rare_feats(df,col,thresh=2,replace_val=-1):\n",
    "    ### replace rare variables (under 2 occurrences) with \"-1\" dummy\n",
    "    affiliates_counts = df[col].value_counts()\n",
    "#     print(\"before:\", affiliates_counts)\n",
    "    print(\"uniques\",df[col].nunique())\n",
    "    affiliates_counts = affiliates_counts.to_dict()\n",
    "    # df[col] = df[col].where(df[col].apply(lambda x: x.map(x.value_counts()))>=3, -1)\n",
    "    df[col] = df[col].where(df[col].map(affiliates_counts)>thresh, replace_val)\n",
    "    print(\"afterwards uniques\",df[col].nunique())\n",
    "#     df[col] = df[col].astype(int)\n",
    "    return(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uniques 39901\n",
      "afterwards uniques 27714\n",
      "before: #trips 217686\n",
      "after: #trips 215508\n"
     ]
    }
   ],
   "source": [
    "### replace rare cities - note that we must take this into account with target! e.g. drop and account for missings as mistake\n",
    "df[\"city_id\"] = map_rare_feats(df,\"city_id\",thresh=1,replace_val=-1)\n",
    "print(\"before: #trips\",df[\"utrip_id\"].nunique())\n",
    "print(\"after: #trips\",df.loc[(df[\"last\"]==1) & (df[\"city_id\"]>=0)][\"utrip_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1102338\n",
      "trips without dropping rare last targets 217686\n",
      "trips after dropping rare last targets 215508\n",
      "1100160\n"
     ]
    }
   ],
   "source": [
    "print(df.shape[0])\n",
    "print(\"trips without dropping rare last targets\",df.loc[df[\"last\"]==1][\"utrip_id\"].nunique())\n",
    "print(\"trips after dropping rare last targets\",df.loc[(df[\"last\"]==1) & (df[\"city_id\"]>0)][\"utrip_id\"].nunique())\n",
    "df = df.loc[~((df[\"last\"]==1) & (df[\"city_id\"]<0))]\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ### replace rare variables (under 2 occurrences) with \"-1\" dummy\n",
    "# affiliates_counts = df[\"affiliate_id\"].value_counts()\n",
    "# print(\"before:\", affiliates_counts)\n",
    "# print(\"uniques\",df[\"affiliate_id\"].nunique())\n",
    "# affiliates_counts = affiliates_counts.to_dict()\n",
    "# # df[\"affiliate_id\"] = df[\"affiliate_id\"].where(df[\"affiliate_id\"].apply(lambda x: x.map(x.value_counts()))>=3, -1)\n",
    "# df[\"affiliate_id\"] = df[\"affiliate_id\"].where(df[\"affiliate_id\"].map(affiliates_counts)>4, -1)\n",
    "# df[\"affiliate_id\"] = df[\"affiliate_id\"].astype(int)\n",
    "\n",
    "# # print(\"after\\n\",df[\"affiliate_id\"].value_counts())\n",
    "# print(\"affiliate_id nunique\",df[\"affiliate_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.city_id.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### duplicate of city, country id columns, as number (may be redundant - we already have count encoding ! )\n",
    "# ### replace rare variables (under 2 occurrences) with \"-1\" dummy\n",
    "# ## city id:\n",
    "# affiliates_counts = df[\"city_id\"].value_counts()\n",
    "# # print(\"before:\", affiliates_counts)\n",
    "# print(\"uniques\",df[\"city_id\"].nunique())\n",
    "# affiliates_counts = affiliates_counts.to_dict()\n",
    "# df[\"numeric_city_id\"] = df[\"city_id\"].where(df[\"city_id\"].map(affiliates_counts)>3, -1)\n",
    "# df[\"numeric_city_id\"] = df[\"numeric_city_id\"].astype(int)\n",
    "\n",
    "# ## hotel_country: (+ordinal encoding)\n",
    "# affiliates_counts = df[\"hotel_country\"].value_counts()\n",
    "# # print(\"before:\", affiliates_counts)\n",
    "# print(\"uniques\",df[\"hotel_country\"].nunique())\n",
    "# affiliates_counts = affiliates_counts.to_dict()\n",
    "# df[\"numeric_hotel_country\"] = df[\"hotel_country\"].where(df[\"hotel_country\"].map(affiliates_counts)>3, \"\")\n",
    "# df[\"numeric_hotel_country\"] = df[\"numeric_hotel_country\"].astype(\"category\").cat.codes.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Add first country, city visited in a trip. \n",
    "* Drop first row of a trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n## add the \"first\" place visited/values\\n### nopte - will need to drop first row in trip, or impute nans when using this feature \\n\\n### first by user results in too much sparsity/rareness for our IDs purposes\\ndf = groupbyFirstLagFeatures(df,group=\"utrip_id\",lag_feature_cols=[\"hotel_country\",\"city_id\",\"duration\",\"same_country\",\\n                                                                   \"affiliate_id\", \"checkin\"]) \\n## + first checkin date -> use for time delta feature\\n# df = df.loc[df[\"row_num\"]>1] ## can\\'t do yet, needed for lag features\\nprint(df[[\"first_hotel_country\",\"hotel_country\",\"city_id\",\"first_same_country\", \"first_affiliate_id\"]].nunique())\\ndf\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "## add the \"first\" place visited/values\n",
    "### nopte - will need to drop first row in trip, or impute nans when using this feature \n",
    "\n",
    "### first by user results in too much sparsity/rareness for our IDs purposes\n",
    "df = groupbyFirstLagFeatures(df,group=\"utrip_id\",lag_feature_cols=[\"hotel_country\",\"city_id\",\"duration\",\"same_country\",\n",
    "                                                                   \"affiliate_id\", \"checkin\"]) \n",
    "## + first checkin date -> use for time delta feature\n",
    "# df = df.loc[df[\"row_num\"]>1] ## can't do yet, needed for lag features\n",
    "print(df[[\"first_hotel_country\",\"hotel_country\",\"city_id\",\"first_same_country\", \"first_affiliate_id\"]].nunique())\n",
    "df\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add relative date/duration feature(s)\n",
    "* Time since start of trip (first_checkin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf[\"total_trip_duration\"] = ((df[\"checkin\"] -df[\"first_checkin\"]).dt.days).astype(int)\\n\\ndf[\"first_checkin_week\"] = df[\"first_checkin\"].dt.isocalendar().week.astype(int)\\n# df[[\"total_trip_duration\",\"first_checkin\",\"checkin\"]]\\n\\n### change type of date cols to more learnable format\\n\\ndf[\"checkin\"] = (df[\"checkin\"]-d1).dt.days.astype(int16)\\ndf[\"checkout\"] = (df[\"checkout\"]-d1).dt.days.astype(int16)\\ndf[\"first_checkin\"] = (df[\"first_checkin\"]-d1).dt.days.astype(int16)\\n## # (df[\"checkin\"]-d1).astype(\\'int64\\')//1e9\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "df[\"total_trip_duration\"] = ((df[\"checkin\"] -df[\"first_checkin\"]).dt.days).astype(int)\n",
    "\n",
    "df[\"first_checkin_week\"] = df[\"first_checkin\"].dt.isocalendar().week.astype(int)\n",
    "# df[[\"total_trip_duration\",\"first_checkin\",\"checkin\"]]\n",
    "\n",
    "### change type of date cols to more learnable format\n",
    "\n",
    "df[\"checkin\"] = (df[\"checkin\"]-d1).dt.days.astype(int16)\n",
    "df[\"checkout\"] = (df[\"checkout\"]-d1).dt.days.astype(int16)\n",
    "df[\"first_checkin\"] = (df[\"first_checkin\"]-d1).dt.days.astype(int16)\n",
    "## # (df[\"checkin\"]-d1).astype('int64')//1e9\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### Following aggregation features - would be best to use time window (sort data) to generate, otherwise they will LEAK! (e.g. nunique countries visited)\n",
    "\n",
    "### count features (can also later add rank inside groups).\n",
    "### Some may be leaks (# visits in a trip should use time window?) , and do users repeat? \n",
    "### can add more counts of group X time period (e.g. affiliate X month of year)\n",
    "## alt way to get counts/freq :\n",
    "# freq = df[\"city_id\"].value_counts()\n",
    "# df[\"city_id_count\"] = df[\"city_id\"].map(freq)\n",
    "# print(df[\"city_id_count\"].describe())\n",
    "\n",
    "count_cols = [ 'city_id', 'hotel_country', \n",
    "             'user_id', \n",
    "#              \"checkin_month\", 'affiliate_id', \"checkin_week\",\n",
    "#               \"checkin_day_of_year\"\n",
    "             ]\n",
    "\n",
    "for c in count_cols:\n",
    "    df[f\"{c}_count\"] = df.groupby([c])[\"duration\"].transform(\"size\")\n",
    "    \n",
    "# ########################################################\n",
    "# ## nunique per trip\n",
    "# ### https://stackoverflow.com/questions/46470743/how-to-efficiently-compute-a-rolling-unique-count-in-a-pandas-time-series\n",
    "\n",
    "# nunique_cols = [ 'city_id', 'hotel_country','affiliate_id', \n",
    "#                 'booker_country',\"duration\",\"checkin_weekday\"]\n",
    "# # df[\"nunique_booker_countries\"] = df.groupby(\"utrip_id\")[\"booker_country\"].nunique()\n",
    "# # df[\"nunique_hotel_country\"] = df.groupby(\"utrip_id\")[\"hotel_country\"].nunique()\n",
    "# for c in nunique_cols:\n",
    "#     df[f\"{c}_nunique\"] = df.groupby([\"utrip_id\"])[c].transform(\"nunique\")\n",
    "\n",
    "# ### trips taken by user\n",
    "# df[\"user_nunique_trips\"] = df.groupby(\"user_id\")[\"utrip_id\"].transform(\"nunique\")\n",
    "# print(df.nunique())\n",
    "\n",
    "# ########################################################\n",
    "# ## get frequency/count feature's rank within a group - e.g. within a country (or affiliate) \n",
    "# ## add \"_count\" to column name to get count col name, then add rank col \n",
    "\n",
    "# ### ALT/ duplicate feat - add percent rank (instead or in addition)\n",
    "\n",
    "# rank_cols = ['city_id','affiliate_id', 'hotel_country',  # 'booker_country',\n",
    "# #  \"checkin_month\",\n",
    "#              \"checkin_week\"\n",
    "#             ]\n",
    "# ### what is meaning of groupby and rank of smae variable by same var? Surely should be 1 / unary? \n",
    "# for c in rank_cols:\n",
    "#     df[f\"{c}_rank_by_hotel_country\"] = df.groupby(['hotel_country'])[f\"{c}_count\"].transform(\"rank\")\n",
    "#     df[f\"{c}_rank_by_affiliate\"] = df.groupby(['affiliate_id'])[f\"{c}_count\"].transform(\"rank\")\n",
    "#     ### rank by month: potentially strong feature, but must avoid leaks + must be joined back by month\n",
    "# #     df[f\"{c}_rank_by_month\"] = df.groupby(['checkin_month'])[f\"{c}_count\"].transform(\"rank\") ## \"city_id_rank_by_month\" may leak\n",
    "# ## there are just 5 booker countries.. \n",
    "# df[\"city_id_rank_by_booker_country\"] = df.groupby(['booker_country'])[f\"city_id_count\"].transform(\"rank\")\n",
    "    \n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12173"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df[\"city_id_count\"]>=7][\"city_id\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert df.isna().sum().max() ==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum top4 total percentage: 5.800000000000001\n",
      "sum top200 total percentage: 49.0\n",
      "sum top500 total percentage: 62.4\n",
      "sum top1000 total percentage: 72.1\n"
     ]
    }
   ],
   "source": [
    "## check coverage of top k classes\n",
    "y_last = df.loc[(df[\"last\"]==1) & (df[\"train\"]==1)][\"city_id\"].dropna().astype(int)\n",
    "print(\"sum top4 total percentage:\",100*y_last.value_counts(normalize=True)[0:4].sum().round(3)) ##5.7%\n",
    "print(\"sum top200 total percentage:\",100*y_last.value_counts(normalize=True)[0:200].sum().round(3)) ## 48%\n",
    "print(\"sum top500 total percentage:\",100*y_last.value_counts(normalize=True)[0:500].sum().round(3)) ### 61%\n",
    "print(\"sum top1000 total percentage:\",100*y_last.value_counts(normalize=True)[0:1000].sum().round(3)) ### 71 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add lag features + Train/test/data split\n",
    "* Lag feats (remember for categorical)\n",
    "* Drop leak features (target values - country, city)\n",
    "\n",
    "* drop instances  that lack history (e.g. at least 3d step and onwards) - by dropna in lag feat\n",
    "* fill nans\n",
    "* Split train/test by `user id` / split could maybe be by `utrip ID` ? ? \n",
    "    * Test - only last trip\n",
    "    *  stratified train/test split by class - then drop any train rows with overlap with tests' IDs.  \n",
    "        * Could also stratify by users, but risks some classes being non present in test\n",
    "\n",
    "* TODO: aggregate features o ntime series - e.g. mean, max, min duration\n",
    "    * We already have nunique\n",
    "###### WE may not need to drop these features anymore - but we may want to join them by city id\n",
    "*We  join city id features after adding the list of caNDIDATes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### features to drop - not usable, or leaks (e.g. aggregations on target) , or hotel/city specific (should be added from per city features)\n",
    "\n",
    "DROP_FEATS = [ #'user_id', ## drop seperately\n",
    "#     'checkin',  # is ok if turned into number\n",
    "#     'checkout',\n",
    "              'hotel_country',\n",
    "              'city_id_count','same_country',\n",
    "#               'utrip_id',\n",
    "             'city_id_count','hotel_country_count',\n",
    "              'city_id_nunique', 'hotel_country_nunique',\n",
    "              'city_id_rank_by_hotel_country','city_id_rank_by_booker_country', 'city_id_rank_by_affiliate',\n",
    "              'affiliate_id_rank_by_hotel_country','affiliate_id_rank_by_booker_country', 'affiliate_id_rank_by_affiliate',\n",
    "              'hotel_country_rank_by_hotel_country',\n",
    "       'hotel_country_rank_by_booker_country','hotel_country_rank_by_affiliate',\n",
    "              'booker_country_rank_by_hotel_country',\n",
    "              'booker_country_rank_by_booker_country',\n",
    "              'checkin_month_rank_by_hotel_country',\n",
    "             ]\n",
    "\n",
    "# df2.drop(DROP_FEATS,axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    27714.000000\n",
       "mean        39.696904\n",
       "std        245.509379\n",
       "min          2.000000\n",
       "25%          3.000000\n",
       "50%          5.000000\n",
       "75%         15.000000\n",
       "max      10010.000000\n",
       "Name: city_id, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"city_id\"].value_counts().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    17721.000000\n",
       "mean        12.161165\n",
       "std         82.974868\n",
       "min          1.000000\n",
       "25%          1.000000\n",
       "50%          2.000000\n",
       "75%          5.000000\n",
       "max       3792.000000\n",
       "Name: city_id, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df[\"last\"]==1][\"city_id\"].value_counts().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quick filter rare cities\n",
    "* Doesn't necessarily filter target cities only\n",
    "* should maybe use utrip id, not user id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before # trips ['29_1' '81_1' '136_2' ... '6258041_1' '6258065_1' '6258087_1']\n",
      "1100160\n",
      "rows after dropping rare targets 1100160\n",
      "count    17266.000000\n",
      "mean        11.195413\n",
      "std         73.359163\n",
      "min          1.000000\n",
      "25%          1.000000\n",
      "50%          2.000000\n",
      "75%          4.750000\n",
      "max       3083.000000\n",
      "Name: counts, dtype: float64\n",
      "prefiltering # 17266\n",
      "cities after filtering: 6951\n",
      "% kept 40.25831113170393\n",
      "% dropped 59.74168886829607\n",
      "% trips kept 100.0\n",
      "% trips dropped 27.581806707871635\n",
      "813326\n",
      "utrip_id    156067\n",
      "city_id       6938\n",
      "dtype: int64\n",
      "       user_id_count  city_id_count\n",
      "count  156067.000000  156067.000000\n",
      "mean        5.211390    2142.505930\n",
      "std         1.763559    2560.729087\n",
      "min         4.000000       4.000000\n",
      "25%         4.000000     251.000000\n",
      "50%         5.000000    1030.000000\n",
      "75%         6.000000    3028.000000\n",
      "max        36.000000    9959.000000\n",
      "After # trips ['81_1' '136_2' '136_4' ... '6257933_3' '6257973_1' '6258017_1']\n"
     ]
    }
   ],
   "source": [
    "# df = df.loc[df[\"city_id_count\"]>=2]\n",
    "print(\"before # trips\",df[\"utrip_id\"].unique())\n",
    "s1 = df.shape[0]\n",
    "print(s1)\n",
    "df = df.loc[~((df[\"last\"]==1) & (df[\"city_id\"]<0))]\n",
    "print(\"rows after dropping rare targets\",df.shape[0])\n",
    "\n",
    "keep_users = df.loc[df[\"last\"]==1][\"utrip_id\"].unique() # list  of users who still have a last address\n",
    "df = df.loc[df[\"utrip_id\"].isin(keep_users)] ## keep only users who have last destination (After our filtering)\n",
    "\n",
    "df[\"user_id_count\"] = df.groupby([\"utrip_id\"])[\"city_id\"].transform(\"size\")\n",
    "df = df.loc[df[\"user_id_count\"]>=4]\n",
    "##########\n",
    "#filter by frequency of last city target\n",
    "pre_trips_amount = len(keep_users)\n",
    "last_targets_freq = df.loc[df[\"last\"]==1][\"city_id\"].value_counts().reset_index().rename(columns={\"index\":\"city_id\",\"city_id\":\"counts\"})\n",
    "print(last_targets_freq[\"counts\"].describe())\n",
    "### min target frequency : \n",
    "raw_count = last_targets_freq.shape[0]\n",
    "print(\"prefiltering #\",raw_count)\n",
    "last_targets_freq = last_targets_freq.loc[last_targets_freq[\"counts\"]>= 3]\n",
    "print(\"cities after filtering:\",last_targets_freq.shape[0])\n",
    "print(\"% kept\",100*(last_targets_freq.shape[0]/raw_count))\n",
    "print(\"% dropped\",100*(1-last_targets_freq.shape[0]/raw_count))\n",
    "\n",
    "df = df.loc[df[\"city_id\"].isin(last_targets_freq[\"city_id\"])]\n",
    "#######\n",
    "df[\"user_id_count\"] = df.groupby([\"utrip_id\"])[\"city_id\"].transform(\"size\") ## update count\n",
    "df = df.loc[df[\"user_id_count\"]>=4]\n",
    "#######\n",
    "post_trips_amount = df[\"utrip_id\"].nunique()\n",
    "print(\"% trips kept\",100*(post_trips_amount/post_trips_amount))\n",
    "print(\"% trips dropped\",100*(1-post_trips_amount/pre_trips_amount))\n",
    "#######\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df[[\"utrip_id\",\"city_id\"]].nunique())\n",
    "print(df.drop_duplicates(\"utrip_id\")[[\"user_id_count\",\"city_id_count\"]].describe() )\n",
    "\n",
    "print(\"After # trips\",df[\"utrip_id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% trips kept 72.41819329212836\n",
      "% trips dropped 27.581806707871635\n"
     ]
    }
   ],
   "source": [
    "post_trips_amount = df[\"utrip_id\"].nunique()\n",
    "print(\"% trips kept\",100*(post_trips_amount/pre_trips_amount))\n",
    "print(\"% trips dropped\",100*(1-post_trips_amount/pre_trips_amount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lbl encode city, users \n",
    "* reencoding is wasteful, but saves on bugs ? (e.g. for embedding, which assumes start from zero)\n",
    "* We add 1 , due to using zero padding for empty lag features! \n",
    "\n",
    "\n",
    "## I later reencode the target city_id, to save on space/# layers\n",
    "* this loses the original values for now! \n",
    "* this is due to  how the sparse categoircal acc works - and we had encoded city id by the full vocab before, which is more cities than exist as targets in the final dest (and that defines the softmax layer size) \n",
    "    * final_target_encoder = LabelEncoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    156067.000000\n",
       "mean          5.211390\n",
       "std           1.763559\n",
       "min           4.000000\n",
       "25%           4.000000\n",
       "50%           5.000000\n",
       "75%           6.000000\n",
       "max          36.000000\n",
       "Name: utrip_id, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"utrip_id\"].value_counts().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min utrip_id    1000027_1\n",
      "city_id             6\n",
      "dtype: object max utrip_id    999855_1\n",
      "city_id        67560\n",
      "dtype: object\n",
      "min utrip_id    0.0\n",
      "city_id     1.0\n",
      "dtype: float64 max utrip_id    156066.0\n",
      "city_id       6938.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "ord_encoder = OrdinalEncoder()\n",
    "print(\"min\",df[[\"utrip_id\",\"city_id\"]].min(),\"max\", df[[\"utrip_id\",\"city_id\"]].max())\n",
    "df[[\"utrip_id\",\"city_id\"]] = ord_encoder.fit_transform(df[[\"utrip_id\",\"city_id\"]] )\n",
    "df[\"city_id\"] =  df[\"city_id\"] + 1 ## start values at 1\n",
    "print(\"min\",df[[\"utrip_id\",\"city_id\"]].min(),\"max\", df[[\"utrip_id\",\"city_id\"]].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max city id 6939\n"
     ]
    }
   ],
   "source": [
    "vocab_size = int(df[\"city_id\"].max()+1)\n",
    "print(\"max city id\",vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### resume pipeline - lag features for city id only + fillna 0  (sequence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape (813326, 30)\n",
      "df_feat (152488, 33)\n"
     ]
    }
   ],
   "source": [
    "print(\"df.shape\",df.shape)\n",
    "# ### lag features - last n visits\n",
    "### orig: \n",
    "# df_feat = groupbyLagFeatures(df=df.copy(), \n",
    "#                    lag=[1,2],group=\"utrip_id\",lag_feature_cols=LAG_FEAT_COLS)\n",
    "\n",
    "df_feat = groupbyLagFeatures(df=df.copy(), \n",
    "                   lag=[1,2,3,4,5],group=\"utrip_id\",lag_feature_cols=[\"city_id\"])\n",
    "\n",
    "# df_feat = df_feat.dropna(subset=[\"lag2_city_id\"]).sample(frac=1)\n",
    "df_feat = df_feat.loc[df_feat[\"last\"]==1].drop([\"last\",\"row_num\"],axis=1,errors=\"ignore\")\n",
    "# df_feat = df_feat.drop(DROP_FEATS,axis=1,errors=\"ignore\")\n",
    "print(\"df_feat\",df_feat.shape)\n",
    "# assert df_feat[\"utrip_id\"].nunique() == df[\"utrip_id\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_id</th>\n",
       "      <th>lag1_city_id</th>\n",
       "      <th>lag2_city_id</th>\n",
       "      <th>lag3_city_id</th>\n",
       "      <th>lag4_city_id</th>\n",
       "      <th>utrip_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>897814</th>\n",
       "      <td>2001</td>\n",
       "      <td>2318</td>\n",
       "      <td>1606</td>\n",
       "      <td>3474</td>\n",
       "      <td>0</td>\n",
       "      <td>151007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770550</th>\n",
       "      <td>4657</td>\n",
       "      <td>6222</td>\n",
       "      <td>2951</td>\n",
       "      <td>4006</td>\n",
       "      <td>0</td>\n",
       "      <td>10214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117283</th>\n",
       "      <td>4258</td>\n",
       "      <td>4684</td>\n",
       "      <td>4787</td>\n",
       "      <td>5770</td>\n",
       "      <td>4462</td>\n",
       "      <td>10215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426973</th>\n",
       "      <td>5946</td>\n",
       "      <td>2001</td>\n",
       "      <td>4345</td>\n",
       "      <td>1721</td>\n",
       "      <td>253</td>\n",
       "      <td>15810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818963</th>\n",
       "      <td>1766</td>\n",
       "      <td>4011</td>\n",
       "      <td>481</td>\n",
       "      <td>6218</td>\n",
       "      <td>1766</td>\n",
       "      <td>40553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990966</th>\n",
       "      <td>6303</td>\n",
       "      <td>6526</td>\n",
       "      <td>2243</td>\n",
       "      <td>5323</td>\n",
       "      <td>5242</td>\n",
       "      <td>145594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379898</th>\n",
       "      <td>3999</td>\n",
       "      <td>2231</td>\n",
       "      <td>390</td>\n",
       "      <td>3354</td>\n",
       "      <td>0</td>\n",
       "      <td>145596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1078894</th>\n",
       "      <td>1621</td>\n",
       "      <td>116</td>\n",
       "      <td>1621</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>145597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64260</th>\n",
       "      <td>1605</td>\n",
       "      <td>625</td>\n",
       "      <td>1354</td>\n",
       "      <td>5483</td>\n",
       "      <td>0</td>\n",
       "      <td>145598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368021</th>\n",
       "      <td>5300</td>\n",
       "      <td>5300</td>\n",
       "      <td>729</td>\n",
       "      <td>5814</td>\n",
       "      <td>2579</td>\n",
       "      <td>145600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>152488 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         city_id  lag1_city_id  lag2_city_id  lag3_city_id  lag4_city_id  \\\n",
       "897814      2001          2318          1606          3474             0   \n",
       "770550      4657          6222          2951          4006             0   \n",
       "117283      4258          4684          4787          5770          4462   \n",
       "426973      5946          2001          4345          1721           253   \n",
       "818963      1766          4011           481          6218          1766   \n",
       "...          ...           ...           ...           ...           ...   \n",
       "990966      6303          6526          2243          5323          5242   \n",
       "379898      3999          2231           390          3354             0   \n",
       "1078894     1621           116          1621           116             0   \n",
       "64260       1605           625          1354          5483             0   \n",
       "368021      5300          5300           729          5814          2579   \n",
       "\n",
       "         utrip_id  \n",
       "897814     151007  \n",
       "770550      10214  \n",
       "117283      10215  \n",
       "426973      15810  \n",
       "818963      40553  \n",
       "...           ...  \n",
       "990966     145594  \n",
       "379898     145596  \n",
       "1078894    145597  \n",
       "64260      145598  \n",
       "368021     145600  \n",
       "\n",
       "[152488 rows x 6 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### keep just lag cols for basic model\n",
    "MINIMAL_COLS = ['city_id','lag1_city_id', 'lag2_city_id',\n",
    "       'lag3_city_id', 'lag4_city_id', \n",
    "#                 'lag5_city_id',\n",
    "               \"utrip_id\"]\n",
    "df_feat = df_feat[MINIMAL_COLS]\n",
    "df_feat = df_feat.fillna(0)\n",
    "df_feat = df_feat.apply(pd.to_numeric, errors='ignore',downcast=\"integer\")\n",
    "df_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6748\n"
     ]
    }
   ],
   "source": [
    "## reorder/encode target city id - sincei t has less values than full vocab\n",
    "final_target_encoder = LabelEncoder()\n",
    "df_feat[\"city_id\"] = final_target_encoder.fit_transform(df_feat[\"city_id\"])\n",
    "\n",
    "\n",
    "# num_classes = df_feat[\"city_id\"].nunique()+1 ## target outputs\n",
    "num_classes = int(df_feat[\"city_id\"].max())\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save intermediate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if SAVE_OUTPUTS:\n",
    "#     df_feat.to_parquet(path=\"df_merged_train.parquet\",index=False)\n",
    "#     df_candidates.to_parquet(path=\"df_merged_candidates.parquet\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# classes 6571\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lag1_city_id</th>\n",
       "      <th>lag2_city_id</th>\n",
       "      <th>lag3_city_id</th>\n",
       "      <th>lag4_city_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>897814</th>\n",
       "      <td>2318</td>\n",
       "      <td>1606</td>\n",
       "      <td>3474</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770550</th>\n",
       "      <td>6222</td>\n",
       "      <td>2951</td>\n",
       "      <td>4006</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230216</th>\n",
       "      <td>4657</td>\n",
       "      <td>5674</td>\n",
       "      <td>960</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411617</th>\n",
       "      <td>3086</td>\n",
       "      <td>218</td>\n",
       "      <td>1478</td>\n",
       "      <td>2092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720067</th>\n",
       "      <td>6717</td>\n",
       "      <td>4111</td>\n",
       "      <td>5467</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909369</th>\n",
       "      <td>6652</td>\n",
       "      <td>1618</td>\n",
       "      <td>861</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990966</th>\n",
       "      <td>6526</td>\n",
       "      <td>2243</td>\n",
       "      <td>5323</td>\n",
       "      <td>5242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1078894</th>\n",
       "      <td>116</td>\n",
       "      <td>1621</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64260</th>\n",
       "      <td>625</td>\n",
       "      <td>1354</td>\n",
       "      <td>5483</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368021</th>\n",
       "      <td>5300</td>\n",
       "      <td>729</td>\n",
       "      <td>5814</td>\n",
       "      <td>2579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>121990 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         lag1_city_id  lag2_city_id  lag3_city_id  lag4_city_id\n",
       "897814           2318          1606          3474             0\n",
       "770550           6222          2951          4006             0\n",
       "230216           4657          5674           960             0\n",
       "411617           3086           218          1478          2092\n",
       "720067           6717          4111          5467             0\n",
       "...               ...           ...           ...           ...\n",
       "909369           6652          1618           861             0\n",
       "990966           6526          2243          5323          5242\n",
       "1078894           116          1621           116             0\n",
       "64260             625          1354          5483             0\n",
       "368021           5300           729          5814          2579\n",
       "\n",
       "[121990 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################\n",
    "## Groupwise train/test split by user or utrip_id\n",
    "TARGET_COL = \"city_id\" ## overwrite\n",
    "train_inds, test_inds = next(GroupShuffleSplit(test_size=.2,\n",
    "                                               n_splits=2,\n",
    "                                               random_state = 12).split(df_feat,groups=df_feat['utrip_id']))\n",
    "\n",
    "X_train = df_feat.iloc[train_inds].drop(DROP_FEATS+[\"utrip_id\"],axis=1,errors=\"ignore\")#.merge(df_city_features,on=\"city_id\",how=\"left\")\n",
    "X_test = df_feat.iloc[test_inds].drop(DROP_FEATS +[\"utrip_id\"],axis=1,errors=\"ignore\")#.merge(df_city_features,on=\"city_id\",how=\"left\")\n",
    "\n",
    "y_train = X_train.pop(TARGET_COL)\n",
    "y_test = X_test.pop(TARGET_COL)\n",
    "\n",
    "print(\"# classes\",y_train.nunique())\n",
    "\n",
    "# ## check that same classes in train and test - \n",
    "# assert (set(y_train.unique()) == set(y_test.unique()))\n",
    "assert y_train.isna().max() == y_test.isna().max() == 0\n",
    "\n",
    "display(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding + multiclass model\n",
    "* apply self attention\n",
    "* Shared embedding - apply same embedding weights to all entities - https://stackoverflow.com/questions/42461673/apply-a-shared-embedding-layer-on-a-set-of-documents-in-keras\n",
    "\n",
    "* see also previous booking modelsL:\n",
    "    * https://www.kaggle.com/parlin987p/booking-sample-cnn#Bidirectional-+-masking \n",
    "    * https://www.kaggle.com/danofer/booking-sample-lstm-fork-1\n",
    "    * https://www.kaggle.com/danofer/booking-model-dan\n",
    "    \n",
    "Note - we could also add triplet/ ranking loss\n",
    "    * https://github.com/maciejkula/triplet_recommendations_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_recommenders as tfrs\n",
    "from tensorflow_recommenders.layers.dcn import Cross\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization\n",
    "\n",
    "callback =[ tf.keras.callbacks.EarlyStopping(patience=3),  #,monitor=â€œval_maeâ€,baseline=5\n",
    "tf.keras.callbacks.ReduceLROnPlateau(factor=0.25, patience=2) ## we should reset the LR when finetuning ?\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### basic embedding + maxpooling model - trivial , very fast\n",
    "def pooling_model():\n",
    "    x = Input(shape=(X_train.shape[1],1,),name=\"city_lags\") ## dim set by # of lag features/cols\n",
    "    ## apply same embedding to all city - time distributed \n",
    "    emb = Embedding(input_dim=vocab_size+1, output_dim=300, mask_zero=True)\n",
    "    att = TimeDistributed(emb)(x)\n",
    "    att = TimeDistributed(GlobalMaxPooling1D())(att)\n",
    "    att = Flatten()(att)\n",
    "    out = Dense(num_classes+1,activation=\"softmax\")(att)\n",
    "    model = Model(x, out)\n",
    "    model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\", metrics=[tf.keras.metrics.SparseTopKCategoricalAccuracy(k=4)])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def self_attention_model_flat():\n",
    "    inp = Input(shape=(X_train.shape[1],1,),name=\"city_lags\") ## dim set by # of lag features/cols\n",
    "    ## apply same embedding to all city - time distributed \n",
    "    emb = Embedding(input_dim=vocab_size+1, output_dim=300, mask_zero=True)\n",
    "\n",
    "    x = TimeDistributed(emb)(inp)\n",
    "#     x = Flatten()(x) # orig\n",
    "    x = TimeDistributed(Flatten())(x) ## alt     (seemingly no difference)\n",
    "    att = Cross()(x)\n",
    "    att = Cross()(x,att)\n",
    "    att = Flatten()(att)\n",
    "    out = Dense(num_classes+1,activation=\"softmax\")(att)\n",
    "    model = Model(inp, out)\n",
    "    model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\", metrics=[tf.keras.metrics.SparseTopKCategoricalAccuracy(k=4)])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "def self_cross_model_dense():\n",
    "    inp = Input(shape=(X_train.shape[1],1,),name=\"city_lags\") ## dim set by # of lag features/cols\n",
    "    ## apply same embedding to all city - time distributed \n",
    "    emb = Embedding(input_dim=vocab_size+1, output_dim=300, mask_zero=True)\n",
    "\n",
    "    x = TimeDistributed(emb)(inp)\n",
    "#     x = Flatten()(x) # orig\n",
    "    x = TimeDistributed(Flatten())(x) ## alt     \n",
    "    att = Cross()(x)\n",
    "    att = Cross()(x,att)\n",
    "    att = Flatten()(att)\n",
    "    \n",
    "#     C = concatenate([att, x ### add raw inputs +- embedding - semi residual/skip. \n",
    "#                  ])\n",
    "    x = Dense(512, activation=\"relu\")(att)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(512, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    out = Dense(num_classes+1,activation=\"softmax\")(x)\n",
    "    model = Model(inp, out)\n",
    "    model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\", metrics=[tf.keras.metrics.SparseTopKCategoricalAccuracy(k=4)])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "def self_attention_flat():\n",
    "    inp = Input(shape=(X_train.shape[1],1,),name=\"city_lags\") ## dim set by # of lag features/cols\n",
    "    ## apply same embedding to all city - time distributed \n",
    "    emb = Embedding(input_dim=vocab_size+1, output_dim=300, mask_zero=True)\n",
    "\n",
    "    x = TimeDistributed(emb)(inp)\n",
    "#     x = Flatten()(x) # orig\n",
    "    x = TimeDistributed(Flatten())(x) ## alt     \n",
    "    att = MultiHeadAttention(num_heads=4, key_dim=4)(x,x) # return_attention_scores=True\n",
    "#     att = MultiHeadAttention(num_heads=4, key_dim=4)(att,att)\n",
    "\n",
    "    att = Flatten()(att)\n",
    "    out = Dense(num_classes+1,activation=\"softmax\")(att)\n",
    "    model = Model(inp, out)\n",
    "    model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\", metrics=[tf.keras.metrics.SparseTopKCategoricalAccuracy(k=4)])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def self_attention_model_dense():\n",
    "    inp = Input(shape=(X_train.shape[1],1,),name=\"city_lags\") ## dim set by # of lag features/cols\n",
    "    ## apply same embedding to all city - time distributed \n",
    "    emb = Embedding(input_dim=vocab_size+1, output_dim=300, mask_zero=True)\n",
    "\n",
    "    x = TimeDistributed(emb)(inp)\n",
    "#     x = Flatten()(x) # orig\n",
    "    x = TimeDistributed(Flatten())(x) ## alt     \n",
    "    att = MultiHeadAttention(num_heads=4, key_dim=4)(x,x)\n",
    "    att = MultiHeadAttention(num_heads=4, key_dim=4)(att,x)\n",
    "\n",
    "    att = Flatten()(att)\n",
    "    att = LayerNormalization()(att)\n",
    "#     C = concatenate([att, x ### add raw inputs +- embedding - semi residual/skip. \n",
    "#                  ])\n",
    "    \n",
    "    x = Dense(1024, activation=\"relu\")(att)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dense(1024, activation=\"relu\")(x)\n",
    "    att = LayerNormalization()(x)\n",
    "    out = Dense(num_classes+1,activation=\"softmax\")(att)\n",
    "    model = Model(inp, out)\n",
    "    model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\", metrics=[tf.keras.metrics.SparseTopKCategoricalAccuracy(k=4)])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def self_attention_model_2():\n",
    "    inp = Input(shape=(X_train.shape[1],1,),name=\"city_lags\") ## dim set by # of lag features/cols\n",
    "    ## apply same embedding to all city - time distributed \n",
    "    emb = Embedding(input_dim=vocab_size+1, output_dim=300, mask_zero=True)\n",
    "\n",
    "    x = TimeDistributed(emb)(inp)\n",
    "#     x = Flatten()(x) # orig\n",
    "    x = TimeDistributed(Flatten())(x) ## alt     \n",
    "    att = Attention(use_scale=True,dropout=0.25)([x,x])\n",
    "    att = Attention(use_scale=True)([att,att])\n",
    "\n",
    "    att = Flatten()(att)\n",
    "    out = Dense(num_classes+1,activation=\"softmax\")(att)\n",
    "    model = Model(inp, out)\n",
    "    model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\", metrics=[tf.keras.metrics.SparseTopKCategoricalAccuracy(k=4)])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def self_attention_model_dense_2():\n",
    "    inp = Input(shape=(X_train.shape[1],1,),name=\"city_lags\") ## dim set by # of lag features/cols\n",
    "    ## apply same embedding to all city - time distributed \n",
    "    emb = Embedding(input_dim=vocab_size+1, output_dim=300, mask_zero=True)\n",
    "\n",
    "    x1 = TimeDistributed(emb)(inp)\n",
    "#     x = Flatten()(x) # orig\n",
    "    max_pooled = Flatten()(TimeDistributed(GlobalMaxPooling1D())(x1))\n",
    "    mean_pooled = Flatten()(TimeDistributed(GlobalAveragePooling1D())(x1))\n",
    "    x = TimeDistributed(Flatten())(x1) ## alt     \n",
    "    att = Attention(use_scale=True,dropout=0.25)([x,x])\n",
    "    att = Attention(use_scale=True)([att,att])\n",
    "    att = Flatten()(att)\n",
    "#     att = LayerNormalization()(att)\n",
    "    C = concatenate([att, max_pooled,mean_pooled ### add raw inputs +- embedding - semi residual/skip. \n",
    "                 ])\n",
    "#     C = Flatten()(C)\n",
    "    x = Dense(1024, activation=\"relu\")(C)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dense(1024, activation=\"relu\")(x)\n",
    "#     x = LayerNormalization()(x)\n",
    "    out = Dense(num_classes+1,activation=\"softmax\")(att)\n",
    "    model = Model(inp, out)\n",
    "    model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\", metrics=[tf.keras.metrics.SparseTopKCategoricalAccuracy(k=4)])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = self_attention_model_dense_2()\n",
    "\n",
    "model.fit(X_train,y_train,\n",
    "          epochs=15,\n",
    "          callbacks=callback, ## lrplateau, early stopping\n",
    "          validation_data=(X_test,y_test),\n",
    "          batch_size = 64,\n",
    "         )\n",
    "### 54  val topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "city_lags (InputLayer)          [(None, 4, 1)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 4, 1, 300)    2082000     city_lags[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, 4, 300)       0           time_distributed_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 4, 300)       1           time_distributed_5[0][0]         \n",
      "                                                                 time_distributed_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 4, 300)       1           attention_2[0][0]                \n",
      "                                                                 attention_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 1200)         0           attention_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 6749)         8105549     flatten_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 10,187,551\n",
      "Trainable params: 10,187,551\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/15\n",
      "1907/1907 [==============================] - 130s 68ms/step - loss: 5.8712 - sparse_top_k_categorical_accuracy: 0.3549 - val_loss: 4.0625 - val_sparse_top_k_categorical_accuracy: 0.4887\n",
      "Epoch 2/15\n",
      "1907/1907 [==============================] - 127s 66ms/step - loss: 3.7851 - sparse_top_k_categorical_accuracy: 0.4932 - val_loss: 3.8616 - val_sparse_top_k_categorical_accuracy: 0.4973\n",
      "Epoch 3/15\n",
      "1907/1907 [==============================] - 128s 67ms/step - loss: 3.4266 - sparse_top_k_categorical_accuracy: 0.5213 - val_loss: 3.8101 - val_sparse_top_k_categorical_accuracy: 0.4998\n",
      "Epoch 4/15\n",
      "1907/1907 [==============================] - 126s 66ms/step - loss: 3.1420 - sparse_top_k_categorical_accuracy: 0.5550 - val_loss: 3.7822 - val_sparse_top_k_categorical_accuracy: 0.5043\n",
      "Epoch 5/15\n",
      "1907/1907 [==============================] - 123s 65ms/step - loss: 2.8673 - sparse_top_k_categorical_accuracy: 0.5979 - val_loss: 3.7831 - val_sparse_top_k_categorical_accuracy: 0.5116\n",
      "Epoch 6/15\n",
      "1907/1907 [==============================] - 131s 69ms/step - loss: 2.6146 - sparse_top_k_categorical_accuracy: 0.6453 - val_loss: 3.8122 - val_sparse_top_k_categorical_accuracy: 0.5107\n",
      "Epoch 7/15\n",
      "1907/1907 [==============================] - 122s 64ms/step - loss: 2.2604 - sparse_top_k_categorical_accuracy: 0.7144 - val_loss: 3.7375 - val_sparse_top_k_categorical_accuracy: 0.5252\n",
      "Epoch 8/15\n",
      "1907/1907 [==============================] - 121s 64ms/step - loss: 2.1513 - sparse_top_k_categorical_accuracy: 0.7351 - val_loss: 3.7500 - val_sparse_top_k_categorical_accuracy: 0.5260\n",
      "Epoch 9/15\n",
      "1907/1907 [==============================] - 121s 64ms/step - loss: 2.0836 - sparse_top_k_categorical_accuracy: 0.7477 - val_loss: 3.7599 - val_sparse_top_k_categorical_accuracy: 0.5268\n",
      "Epoch 10/15\n",
      "1907/1907 [==============================] - 121s 63ms/step - loss: 2.0050 - sparse_top_k_categorical_accuracy: 0.7628 - val_loss: 3.7551 - val_sparse_top_k_categorical_accuracy: 0.5265\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1de897a4f08>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = self_attention_model_2()\n",
    "\n",
    "model.fit(X_train,y_train,\n",
    "          epochs=15,\n",
    "          callbacks=callback, ## lrplateau, early stopping\n",
    "          validation_data=(X_test,y_test),\n",
    "          batch_size = 64,\n",
    "         )\n",
    "### 54  val topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = self_attention_flat()\n",
    "\n",
    "# model.fit(X_train,y_train,\n",
    "#           epochs=15,\n",
    "#           callbacks=callback, ## lrplateau, early stopping\n",
    "#           validation_data=(X_test,y_test),\n",
    "#           batch_size = 64,\n",
    "#          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "city_lags (InputLayer)          [(None, 4, 1)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, 4, 1, 300)    2082000     city_lags[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (None, 4, 300)       0           time_distributed_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention (MultiHead (None, 4, 300)       19548       time_distributed_7[0][0]         \n",
      "                                                                 time_distributed_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_1 (MultiHe (None, 4, 300)       19548       multi_head_attention[0][0]       \n",
      "                                                                 time_distributed_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 1200)         0           multi_head_attention_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 1200)         2400        flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1024)         1229824     layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (None, 1024)         2048        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1024)         1049600     layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 1024)         2048        dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 6749)         6917725     layer_normalization_3[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 11,324,741\n",
      "Trainable params: 11,324,741\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/15\n",
      "1907/1907 [==============================] - 147s 76ms/step - loss: 5.4227 - sparse_top_k_categorical_accuracy: 0.3394 - val_loss: 4.0440 - val_sparse_top_k_categorical_accuracy: 0.4650\n",
      "Epoch 2/15\n",
      "1907/1907 [==============================] - 146s 76ms/step - loss: 3.7741 - sparse_top_k_categorical_accuracy: 0.4733 - val_loss: 3.9057 - val_sparse_top_k_categorical_accuracy: 0.4813\n",
      "Epoch 3/15\n",
      "1907/1907 [==============================] - 147s 77ms/step - loss: 3.4798 - sparse_top_k_categorical_accuracy: 0.4980 - val_loss: 3.8888 - val_sparse_top_k_categorical_accuracy: 0.4879\n",
      "Epoch 4/15\n",
      "1907/1907 [==============================] - 156s 82ms/step - loss: 3.2692 - sparse_top_k_categorical_accuracy: 0.5195 - val_loss: 3.9502 - val_sparse_top_k_categorical_accuracy: 0.4881\n",
      "Epoch 5/15\n",
      "1907/1907 [==============================] - 160s 84ms/step - loss: 3.0536 - sparse_top_k_categorical_accuracy: 0.5473 - val_loss: 4.0895 - val_sparse_top_k_categorical_accuracy: 0.4852\n",
      "Epoch 6/15\n",
      "1907/1907 [==============================] - 156s 82ms/step - loss: 2.6778 - sparse_top_k_categorical_accuracy: 0.6145 - val_loss: 4.3901 - val_sparse_top_k_categorical_accuracy: 0.4844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1de8a255448>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = self_attention_model_dense()\n",
    "\n",
    "model.fit(X_train,y_train,\n",
    "          epochs=15,\n",
    "          callbacks=callback, ## lrplateau, early stopping\n",
    "          validation_data=(X_test,y_test),\n",
    "          batch_size = 64,\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "city_lags (InputLayer)          [(None, 4, 1)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_8 (TimeDistrib (None, 4, 1, 300)    2082000     city_lags[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_9 (TimeDistrib (None, 4, 300)       0           time_distributed_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "cross (Cross)                   (None, 4, 300)       90300       time_distributed_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "cross_1 (Cross)                 (None, 4, 300)       90300       time_distributed_9[0][0]         \n",
      "                                                                 cross[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 1200)         0           cross_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 512)          614912      flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 512)          2048        dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 512)          262656      batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 512)          2048        dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 6749)         3462237     batch_normalization_1[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 6,606,501\n",
      "Trainable params: 6,604,453\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/15\n",
      "1907/1907 [==============================] - 101s 52ms/step - loss: 5.2858 - sparse_top_k_categorical_accuracy: 0.3981 - val_loss: 3.8032 - val_sparse_top_k_categorical_accuracy: 0.5143\n",
      "Epoch 2/15\n",
      "1907/1907 [==============================] - 98s 51ms/step - loss: 3.2559 - sparse_top_k_categorical_accuracy: 0.5646 - val_loss: 3.7726 - val_sparse_top_k_categorical_accuracy: 0.5185\n",
      "Epoch 3/15\n",
      "1907/1907 [==============================] - 90s 47ms/step - loss: 2.5714 - sparse_top_k_categorical_accuracy: 0.6761 - val_loss: 4.0401 - val_sparse_top_k_categorical_accuracy: 0.5128\n",
      "Epoch 4/15\n",
      "1907/1907 [==============================] - 90s 47ms/step - loss: 1.9447 - sparse_top_k_categorical_accuracy: 0.7852 - val_loss: 4.4359 - val_sparse_top_k_categorical_accuracy: 0.4964\n",
      "Epoch 5/15\n",
      "1907/1907 [==============================] - 90s 47ms/step - loss: 1.3016 - sparse_top_k_categorical_accuracy: 0.8833 - val_loss: 4.5707 - val_sparse_top_k_categorical_accuracy: 0.5059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1de98ab0c88>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = self_cross_model_dense()\n",
    "\n",
    "model.fit(X_train,y_train,\n",
    "          epochs=15,\n",
    "          callbacks=callback, ## lrplateau, early stopping\n",
    "          validation_data=(X_test,y_test),\n",
    "          batch_size = 64,\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "city_lags (InputLayer)       [(None, 4, 1)]            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_10 (TimeDis (None, 4, 1, 300)         2082000   \n",
      "_________________________________________________________________\n",
      "time_distributed_11 (TimeDis (None, 4, 300)            0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 6749)              8105549   \n",
      "=================================================================\n",
      "Total params: 10,187,549\n",
      "Trainable params: 10,187,549\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/15\n",
      "1907/1907 [==============================] - 118s 61ms/step - loss: 6.0224 - sparse_top_k_categorical_accuracy: 0.3512 - val_loss: 3.8491 - val_sparse_top_k_categorical_accuracy: 0.5219\n",
      "Epoch 2/15\n",
      "1907/1907 [==============================] - 117s 62ms/step - loss: 3.3327 - sparse_top_k_categorical_accuracy: 0.5656 - val_loss: 3.6324 - val_sparse_top_k_categorical_accuracy: 0.5382\n",
      "Epoch 3/15\n",
      "1907/1907 [==============================] - 117s 62ms/step - loss: 2.6335 - sparse_top_k_categorical_accuracy: 0.6582 - val_loss: 3.7096 - val_sparse_top_k_categorical_accuracy: 0.5297\n",
      "Epoch 4/15\n",
      "1907/1907 [==============================] - 117s 61ms/step - loss: 2.0783 - sparse_top_k_categorical_accuracy: 0.7611 - val_loss: 3.8875 - val_sparse_top_k_categorical_accuracy: 0.5228\n",
      "Epoch 5/15\n",
      "1907/1907 [==============================] - 117s 62ms/step - loss: 1.5721 - sparse_top_k_categorical_accuracy: 0.8498 - val_loss: 3.8784 - val_sparse_top_k_categorical_accuracy: 0.5260\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1de981d3388>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = pooling_model()\n",
    "\n",
    "model.fit(X_train,y_train,\n",
    "          epochs=15,\n",
    "          callbacks=callback, ## lrplateau, early stopping\n",
    "          validation_data=(X_test,y_test),\n",
    "          batch_size = 64,\n",
    "         )\n",
    "\n",
    "# # Epoch 7/10 - 20s - loss: 1.907 - accuracy: 0.479 - sparse_top_k_categorical_accuracy: 0.789 -\n",
    "# #             val_loss: 3.091 - val_accuracy: 0.3453 - val_sparse_top_k_categorical_accuracy: 0.5922"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = self_attention_model_flat_1()\n",
    "\n",
    "# model.fit(X_train,y_train,\n",
    "#           epochs=15,\n",
    "#           callbacks=callback, ## lrplateau, early stopping\n",
    "#           validation_data=(X_test,y_test),\n",
    "#           batch_size = 64,\n",
    "#          )\n",
    "\n",
    "# # Epoch 2/10\n",
    "# # 1527/1527 [==============================] - 23s 15ms/step - loss: 2.7761 - accuracy: 0.3593 - sparse_top_k_categorical_accuracy: 0.6269 - val_loss: 3.0526 - val_accuracy: 0.3387 - val_sparse_top_k_categorical_accuracy: 0.5939\n",
    "# # Epoch 6/10\n",
    "# # 1527/1527 [==============================] - 23s 15ms/step - loss: 1.3743 - accuracy: 0.5895 - sparse_top_k_categorical_accuracy: 0.8710 - val_loss: 3.8441 - val_accuracy: 0.3205 - val_sparse_top_k_categorical_accuracy: 0.5692"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pooling, 2 epochs: \n",
    "# 5s 8ms/step - loss: 2.9698 - accuracy: 0.3515 - sparse_top_k_categorical_accuracy: 0.6059 - val_loss: 2.8055 - val_accuracy: 0.3643 - val_sparse_top_k_categorical_accuracy: 0.6204"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top4_accuracy(model,y_test_pred=None):\n",
    "    \"\"\"warning - very hacky! - set to use global X_test, y_test variables\"\"\"\n",
    "    if y_test_pred is None:\n",
    "        y_test_pred = model.predict(X_test,prediction_type=\"Probability\")[:,1]\n",
    "\n",
    "    test_preds = pd.DataFrame({\"y_test_pred\":y_test_pred,\n",
    "#                                \"candidate_city_id\":X_test[\"city_id\"],\n",
    "                               \"utrip_id\":X_test[\"utrip_id\"],\n",
    "                               \"rank\":X_test[\"rank\"],\n",
    "#                                \"city_id_count\":X_test[\"city_id_count\"],\n",
    "                               \"label\":y_test\n",
    "                              })    \n",
    "    ## most frequent in overall data (not as target)\n",
    "    n_users = test_preds[\"utrip_id\"].nunique()\n",
    "\n",
    "#     test_preds.sort_values([\"utrip_id\",\"city_id_count\"],inplace=True,ascending=False)\n",
    "#     res_count = 100*test_preds.groupby(\"utrip_id\")[\"label\"].head(4).sum() / n_users\n",
    "#     print(\"top4 acc baseline: by most frequent city_id_count {0:.4f}\".format(res_count))\n",
    "\n",
    "    test_preds.sort_values([\"utrip_id\",\"rank\"],inplace=True,ascending=True)\n",
    "    res_rank = 100*test_preds.groupby(\"utrip_id\")[\"label\"].head(4).sum() / n_users\n",
    "    print(\"top4 acc baseline: by rank {0:.4f}\".format(res_rank))\n",
    "\n",
    "    test_preds.sort_values([\"utrip_id\",\"y_test_pred\"],inplace=True,ascending=False)\n",
    "    res = 100*test_preds.groupby(\"utrip_id\")[\"label\"].head(4).sum() / n_users\n",
    "    print(\"top4 acc: by model {0:.4f}\".format(res))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get predictions on competition/candidate data\n",
    "* `df_candidates`\n",
    "* There appear to be duplicate city_ids in the predictions ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if SAVE_OUTPUTS:\n",
    "    candidates_pool = Pool(data=df_candidates,cat_features=CAT_FEAT_NAMES\n",
    "#                      ,group_id=df_candidates[\"utrip_id\"]\n",
    "                     )\n",
    "    print(\"candidate pool done\")\n",
    "    preds = model.predict(candidates_pool,prediction_type=\"Probability\")[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # errors if using raw data - data type disparities . unclear why\n",
    "# ## thread_count\n",
    "# preds = model.predict(df_candidates,prediction_type=\"Probability\",verbose=True,thread_count=4)[:,1]\n",
    "# print(\"done\")\n",
    "# print(len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_OUTPUTS:\n",
    "    submission = pd.DataFrame({\n",
    "                               \"utrip_id\":df_candidates[\"utrip_id\"],\n",
    "                           \"city_id\":df_candidates[\"city_id\"],\n",
    "                           \"pred\":preds,\n",
    "    #                        \"rank\":df_candidates[\"rank\"],\n",
    "                          })   \n",
    "\n",
    "    submission.sort_values([\"pred\"],ascending=False,inplace=True)\n",
    "\n",
    "    ### unknonw bug - we get multiple cities per ID ???\n",
    "\n",
    "    print(\"duplicate cities suggested:\",submission.shape[0] - submission.drop_duplicates(['utrip_id','city_id']).shape[0])\n",
    "\n",
    "    #  if SAVE_OUTPUTS:\n",
    "    submission = submission.drop_duplicates(['utrip_id','city_id'],keep=\"first\")\n",
    "    print(\"submission.nunique()\",submission.nunique())\n",
    "    display(submission)\n",
    "    ## drop -1s:\n",
    "    submission = submission.loc[submission['city_id']>= 0]\n",
    "    submission = submission.groupby([\"utrip_id\"]).head(4).drop(\"pred\",axis=1)\n",
    "\n",
    "    submission[\"r_num\"] = submission.groupby(\"utrip_id\")[\"city_id\"].rank(ascending=True,pct=False)\n",
    "\n",
    "    submission = submission.pivot(index='utrip_id', columns='r_num', values='city_id').reset_index() ## , values='score'\n",
    "    submission.columns = [\"utrip_id\",'city_id_1','city_id_2','city_id_3','city_id_4']\n",
    "\n",
    "    assert submission[\"utrip_id\"].nunique() == test_num_ids\n",
    "    display(submission)       \n",
    "\n",
    "    submission.to_csv(\"submission.csv\",index=False)\n",
    "    print(submission.nunique())\n",
    "    print(submission.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## gets stuck with many rows\n",
    "\n",
    "# y_test_preds = model.predict(test_pool,prediction_type=\"Probability\")[:,1]\n",
    "# print(top4_accuracy(model,y_test_preds))\n",
    "\n",
    "### from run on sample, with old train-train lightfm ranking: \n",
    "# top4 acc baseline: by most frequent city_id_count 39.6007\n",
    "# top4 acc baseline: by rank 41.2646\n",
    "# top4 acc: by model 77.0383\n",
    "# 77.03826955074875\n",
    "\n",
    "# ## still ~ 160k sample , model without rank/score: \n",
    "# top4 acc baseline: by most frequent city_id_count 32.1688\n",
    "# top4 acc baseline: by rank 41.6647\n",
    "# top4 acc: by model 49.9179"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance\n",
    "* if usingSHAP: We don't want to wait too long/to calculate it over 40 million rows, so we could look at a sample. e.g. top ranked (rank <= 10 | label ==1)  , or first few rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict() got an unexpected keyword argument 'prediction_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-3cc54f8ad7ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtop4_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtop4_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# # print(top4_accuracy(model))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# ## without sqrt balance, top4 acc: by model 56.9870\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-51-0b084219cc8f>\u001b[0m in \u001b[0;36mtop4_accuracy\u001b[1;34m(model, y_test_pred)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;34m\"\"\"warning - very hacky! - set to use global X_test, y_test variables\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_test_pred\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0my_test_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprediction_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Probability\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     test_preds = pd.DataFrame({\"y_test_pred\":y_test_pred,\n",
      "\u001b[1;31mTypeError\u001b[0m: predict() got an unexpected keyword argument 'prediction_type'"
     ]
    }
   ],
   "source": [
    "top4_acc = top4_accuracy(model)\n",
    "# # print(top4_accuracy(model))\n",
    "\n",
    "# ## without sqrt balance, top4 acc: by model 56.9870"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_imp_cb = model.get_feature_importance(data=test_pool,\n",
    "#                        prettified=True,shap_calc_type =\"Approximate\")\n",
    "\n",
    "# feat_imp_cb = feat_imp_cb.loc[feat_imp_cb[\"Importances\"]>0].round(2)\n",
    "# display(feat_imp_cb.head(32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Evaluate model on test data\n",
    "    * function may have memory leakage issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using 30% of data:\n",
    "    * \n",
    "        top4 acc baseline: by most frequent city_id_count 23.99\n",
    "        \n",
    "        top4 acc baseline: by rank 41.87\n",
    "        \n",
    "        top4 acc: by model 55.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
