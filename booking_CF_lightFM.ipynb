{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collab filtering/MF model V1.1\n",
    "\n",
    "* try intermediate-simple collaborative filtering/MF/implicit recc embedding model\n",
    "* create new \"id\" - semi distinct tuple of distinguishing variables\n",
    "    * Could consider adding previous city/country visited to tuple - would reduce data but greatly improve this model. For that to make sense, we would also need to do a shared embedding so that city id lag1 and city id will have the same embedding weights learned. \n",
    "    * Could also add discretized number of locations in a trip `total_rows` (1-3,4-5,6+?) as feature , +- step/stage in a trip - https://pbpython.com/natural-breaks.html\n",
    "* Start with stupid approach: ~9k most popular items. highly unbalanced. Dot product .\n",
    "    * Alt simple baseline - SVD (but that loses on embeddings per variable. Instead we'd use just the id)\n",
    "    \n",
    "    \n",
    "    \n",
    " - Possible negatives sampling code : \n",
    "     * https://curiousily.com/posts/build-a-recommender-system-using-keras-and-tensorflow2-in-python/\n",
    "     * tf.datasets - https://stackoverflow.com/questions/58520594/tf-data-dataset-on-each-epoch-only-train-with-a-sub-sample-of-the-full-datase\n",
    "     \n",
    "     \n",
    "     \n",
    " * ALT - get all combinations (itertools) - https://stackoverflow.com/questions/43800390/how-to-create-all-combinations-column-wise-for-multiple-variables-in-pandas\n",
    " \n",
    " * Get all combinations: `pd.crosstab(df[\"city_id\"],df[\"ID\"]).stack().reset_index()`\n",
    " * Keras + sparse generator - https://stackoverflow.com/questions/41538692/using-sparse-matrices-with-keras-and-tensorflow\n",
    "     * https://www.kaggle.com/luisgarcia/keras-nn-with-parallelized-batch-training\n",
    " \n",
    " ###### V1.1: Add lag feature - hotel_country\n",
    " * +- first hotel country, and lag1 hotel country ?\n",
    " \n",
    " * Would be interesting to combine with \"next country\" prediction task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://www.bookingchallenge.com/\n",
    "\n",
    "* Predict `city_id`\n",
    "        * Metric: P@4\n",
    "\n",
    "##### Dataset\n",
    "The training dataset consists of over a million of anonymized hotel reservations, based on real data, with the following features:\n",
    "*    user_id - User ID\n",
    "*    check-in - Reservation check-in date\n",
    "*    checkout - Reservation check-out date\n",
    "*    affiliate_id - An anonymized ID of affiliate channels where the booker came from (e.g. direct, some third party referrals, paid search engine, etc.)\n",
    "*    device_class - desktop/mobile\n",
    "*    booker_country - Country from which the reservation was made (anonymized)\n",
    "*    hotel_country - Country of the hotel (anonymized)\n",
    "*    city_id - city_id of the hotel’s city (anonymized)\n",
    "*    utrip_id - Unique identification of user’s trip (a group of multi-destinations bookings within the same trip)\n",
    "\n",
    "\n",
    "* Each reservation is a part of a customer’s trip (identified by utrip_id) which includes at least 4 consecutive reservations. The check-out date of a reservation is the check-in date of the following reservation in their trip.\n",
    "\n",
    "* The evaluation dataset is constructed similarly, however the city_id of the final reservation of each trip is concealed and requires a prediction.\n",
    "\n",
    " \n",
    "###### Evaluation criteria\n",
    "The goal of the challenge is to predict (and recommend) the final city (city_id) of each trip (utrip_id). We will evaluate the quality of the predictions based on the top four recommended cities for each trip by using Precision@4 metric (4 representing the four suggestion slots at Booking.com website). When the true city is one of the top 4 suggestions (regardless of the order), it is considered correct.\n",
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "* If we are given  the country in question, then this problem is maybe more of a _learning to rank_ problem. (Rather than massively multiclass). \n",
    "    * CatBoost learning to rank on ms dataset (0/1):  https://colab.research.google.com/github/catboost/tutorials/blob/master/ranking/ranking_tutorial.ipynb\n",
    "        * https://catboost.ai/docs/concepts/loss-functions-ranking.html\n",
    "        * for CB ranking,  all objects in dataset must be grouped by group_id - this would be user/trip id X country, in our case. (Still need to add negatives, within each such subgroup/group/\"query\"). \n",
    "\n",
    "    * lightFM - ranking (implicit interactions)\n",
    "        * https://github.com/qqwjq/lightFM\n",
    "\n",
    "    * lstm/w2v - next item recomendation\n",
    "    * dot product between different factors as features (recc.)\n",
    "    * xgboost ap - https://www.kaggle.com/anokas/xgboost-2\n",
    "* Relevant: Kaggle expedia hotel prediction: https://www.kaggle.com/c/expedia-hotel-recommendations/discussion  \n",
    "\n",
    "* ALSO: `implicit interaction` - reccommendation problem (We have only positive feedback, no ranked/negative explicit feedback)'\n",
    "\n",
    "\n",
    "* __BASELINE__ to beat: 4 most popular by country ; 4 most popular by affiliate_id X booker_country X hotel_country (X month?)\n",
    "    * Ignore/auto answer the 4 most popular for countries with less than 4 unique cities in data\n",
    " \n",
    " \n",
    "* Likely approach : build a model (and targets/negatives) per country.\n",
    "\n",
    "-----------\n",
    "#### Data notes:\n",
    "* Long tail of cities and countries\n",
    "* Some (31%) countries have 4 or less unique cities - for those return fixed answer/prediction ?  -\n",
    "    * CAN'T! In test set, we will not have the country ID :(\n",
    "    \n",
    "    \n",
    "----------------------\n",
    "MF - embedding model\n",
    "\n",
    "* https://blog.tensorflow.org/2020/09/introducing-tensorflow-recommenders.html\n",
    "* Implicit recommendations - needs negs\n",
    "    * example of explicit (simple): https://petamind.com/build-a-simple-recommender-system-with-matrix-factorization/\n",
    "* sample negatives - how ? TFRS requires tf.dataset overhead (And confuses me with what user id should be )\n",
    "    * https://www.kaggle.com/skihikingkevin/some-recommender-system-implementations\n",
    "    \n",
    "    \n",
    "Simple keras example of multiple inputs : \n",
    "* keras topologies\n",
    "* https://stackoverflow.com/questions/61722973/why-keras-embedding-not-learning-for-recommendation-system\n",
    "\n",
    "\n",
    "*Tensorflow ranking (seems in beta) : https://colab.research.google.com/github/tensorflow/ranking/blob/master/tensorflow_ranking/examples/handling_sparse_features.ipynb#scrollTo=HfDMGnZY9eVO\n",
    "\n",
    "\n",
    "Negative pairs training with generator - https://towardsdatascience.com/building-a-recommendation-system-using-neural-network-embeddings-1ef92e5c80c9\n",
    "* See code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommenders embedding - fit generator\n",
    "# https://towardsdatascience.com/building-a-recommendation-system-using-neural-network-embeddings-1ef92e5c80c9\n",
    "# Also has code for generator to generate positive, negative pairs per batch - good for siamese/triplets/metric! \n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "np.random.seed(100)\n",
    "random.seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible approahc + negatives - https://github.com/zhangruiskyline/DeepLearning/blob/master/doc/Recommendation.md#ranking \n",
    "\n",
    "\n",
    "* Negative sampling from the sparse user-item cooccurrence matrix\n",
    "    * https://stackoverflow.com/questions/49971318/how-to-generate-negative-samples-in-tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from catboost import CatBoostClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, GroupShuffleSplit\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.metrics import TopKCategoricalAccuracy, Precision, SparseTopKCategoricalAccuracy # @4\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## https://www.tensorflow.org/guide/mixed_precision ## TF mixed precision - pytorch requires other setup\n",
    "# from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "\n",
    "# policy = mixed_precision.Policy('mixed_float16')\n",
    "# mixed_precision.set_policy(policy)\n",
    "# ## will need to correct in places, e.g.: \n",
    "# ## outputs = layers.Activation('softmax', dtype='float32', name='predictions')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features to add:\n",
    "* Lag \n",
    "* Rank (popularity) of city, country (in general, +- given booker country)\n",
    "* Count of hotel; user, trip size ? (may be leaky )\n",
    "* Seasonal features - Holidays? , datetime\n",
    "\n",
    "Aggregate feats:\n",
    "* user changed country? last booking (lag 1) country change? \n",
    "* max/min/avg popularity rank of previous locations visited\n",
    "\n",
    "\n",
    "\n",
    "We should create a dictionary of the rank, count, city/country etc' feats, so we can easily merge them when making more \"negative\" samples/feats for ranking.\n",
    "\n",
    "\n",
    "* Consider using a df2 of df without dates + drop_duplicates, +- without user/trip id (After calcing that) .\n",
    "\n",
    "\n",
    "Leaky or potentially leaky (Dependso n test set): \n",
    "* Target freq features - frequency of target city, given source county +- affiliate +- month of year +- given country (and interactions of target freq). \n",
    "    * Risk of leaks - depends of test data has temporal split or not. \n",
    "    * cartboost can do target encode, but this lets us do it for interactions, e.g. target city freq given the 2 countries and affiliate.\n",
    "    * beware overfitting! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_TARGET_FREQ = 38 # drop target/city_id values that appear less than this many times, as final step's target \n",
    "KEEP_TOP_K_TARGETS = 0 # keep K most frequent city ID targets (redundnat with the above, )\n",
    "\n",
    "## (some) categorical variables that appear less than this many times will be replaced with a placeholder value!\n",
    "## Includes CITY id (but done after target filtering, to avoid creating a \"rare class\" target:\n",
    "LOW_COUNT_THRESH = 10\n",
    "\n",
    "RUN_TABNET = False\n",
    "max_epochs = 5\n",
    "\n",
    "GET_COUNT_AGG_FEATS = False ## disable getting count, rank etc' groupby features , for speedup\n",
    "\n",
    "DROP_FIRST_ROW = True\n",
    "\n",
    "## for matrix factorization/CF:\n",
    "### morte possible ID_cols :  # last (last step in trip) - would double data per user incorrectly\n",
    "### hotel_country_lag1 , city_id_lag1  (very relevant - needs shared embeddingm and would increase cardinality a lot.. ) \n",
    "ID_COLS = [\n",
    "#     'device_class',\n",
    "#            'affiliate_id',\n",
    "           'booker_country',\n",
    "           'checkin_quarter',\n",
    "#            \"last\",\n",
    "          \"first_hotel_country\"] \n",
    "MF_KEEP_COLS = [\"ID\"]+ID_COLS+['city_id',\"hotel_country\"]\n",
    "\n",
    "SAVE_TO_DISK = False\n",
    "\n",
    "TARGET_COL =  'city_id' #\"city_id\"#'hotel_country' \n",
    "USER_ID_COL = \"utrip_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most basic categorical columns , without 'user_id', , 'utrip_id' ordevice_class - used for count encoding/filtering\n",
    "BASE_CAT_COLS = ['city_id',  'affiliate_id', 'booker_country', 'hotel_country']\n",
    "\n",
    "# ### features to get lags for. Not very robust. May want different feats for lags before -1\n",
    "# LAG_FEAT_COLS = ['city_id', 'device_class',\n",
    "#        'affiliate_id', 'booker_country', 'hotel_country', \n",
    "#        'duration', 'same_country', 'checkin_day', 'checkin_weekday',\n",
    "#        'checkin_week',\n",
    "#         'checkout_weekday','checkout_week',\n",
    "#        'city_id_count', 'affiliate_id_count',\n",
    "#        'booker_country_count', 'hotel_country_count', \n",
    "#        'checkin_month_count', 'checkin_week_count', 'city_id_nunique',\n",
    "#        'affiliate_id_nunique', 'booker_country_nunique',\n",
    "#        'hotel_country_nunique', 'city_id_rank_by_hotel_country',\n",
    "#        'city_id_rank_by_booker_country', 'city_id_rank_by_affiliate',\n",
    "#        'affiliate_id_rank_by_hotel_country',\n",
    "#        'affiliate_id_rank_by_booker_country', 'affiliate_id_rank_by_affiliate',\n",
    "#        'booker_country_rank_by_hotel_country',\n",
    "#        'booker_country_rank_by_booker_country',\n",
    "#        'booker_country_rank_by_affiliate',\n",
    "#        'hotel_country_rank_by_hotel_country',\n",
    "#        'hotel_country_rank_by_booker_country',\n",
    "#        'hotel_country_rank_by_affiliate',\n",
    "#        'checkin_month_rank_by_hotel_country',\n",
    "#        'checkin_month_rank_by_booker_country',\n",
    "#        'checkin_month_rank_by_affiliate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/33907537/groupby-and-lag-all-columns-of-a-dataframe\n",
    "# https://stackoverflow.com/questions/62924987/lag-multiple-variables-grouped-by-columns\n",
    "## lag features with groupby over many columns: \n",
    "def groupbyLagFeatures(df:pd.DataFrame,lag:[]=[1,2],group=\"utrip_id\",lag_feature_cols=[]):\n",
    "    \"\"\"\n",
    "    lag features with groupby over many columns.\n",
    "    Assumes sorted data!\n",
    "    https://stackoverflow.com/questions/62924987/lag-multiple-variables-grouped-by-columns\"\"\"\n",
    "    if len(lag_feature_cols)>0:\n",
    "        df=pd.concat([df]+[df.groupby(group)[lag_feature_cols].shift(x).add_prefix('lag'+str(x)+\"_\") for x in lag],axis=1)\n",
    "    else:\n",
    "         df=pd.concat([df]+[df.groupby(group).shift(x).add_prefix('lag'+str(x)+\"_\") for x in lag],axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def groupbyFirstLagFeatures(df:pd.DataFrame,group=\"user_id\",lag_feature_cols=[]):\n",
    "    \"\"\"\n",
    "    Get  first/head value lag-like of features with groupby over columns. Assumes sorted data!\n",
    "    \"\"\"\n",
    "    if len(lag_feature_cols)>0:\n",
    "        df=pd.concat([df]+[df.groupby(group)[lag_feature_cols].transform(\"first\").add_prefix(\"first_\")],axis=1)\n",
    "    else:\n",
    "#          df=pd.concat([df]+[df.groupby(group).first().add_prefix(\"first_\")],axis=1)\n",
    "        df=pd.concat([df]+[df.groupby(group).transform(\"first\").add_prefix(\"first_\")],axis=1)\n",
    "    return df\n",
    "\n",
    "######## Get n most popular items, per group\n",
    "def most_popular(group, n_max=4):\n",
    "    \"\"\"Find most popular hotel clusters by destination\n",
    "    Define a function to get most popular hotels for a destination group.\n",
    "\n",
    "    Previous version used nlargest() Series method to get indices of largest elements. But the method is rather slow.\n",
    "    Source: https://www.kaggle.com/dvasyukova/predict-hotel-type-with-pandas\n",
    "    \"\"\"\n",
    "    relevance = group['relevance'].values\n",
    "    hotel_cluster = group['hotel_cluster'].values\n",
    "    most_popular = hotel_cluster[np.argsort(relevance)[::-1]][:n_max]\n",
    "    return np.array_str(most_popular)[1:-1] # remove square brackets\n",
    "\n",
    "\n",
    "## https://codereview.stackexchange.com/questions/149306/select-the-n-most-frequent-items-from-a-pandas-groupby-dataframe\n",
    "# https://stackoverflow.com/questions/52073054/group-by-a-column-to-find-the-most-frequent-value-in-another-column\n",
    "## can get modes (sorted)\n",
    "# https://stackoverflow.com/questions/50592762/finding-most-common-values-with-pandas-groupby-and-value-counts\n",
    "## df.groupby('tag')['category'].agg(lambda x: x.value_counts().index[0])\n",
    "# https://stackoverflow.com/questions/15222754/groupby-pandas-dataframe-and-select-most-common-value\n",
    "# source2.groupby(['Country','City'])['Short name'].agg(pd.Series.mode)\n",
    "\n",
    "\n",
    "\n",
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=64,target_col=\"target\"):\n",
    "    \"\"\"\n",
    "    Wrap dataframes with tf.data. \n",
    "    This will enable us to use feature columns as a bridge to map from the columns in a dataframe to features used to train the model.\n",
    "    https://www.tensorflow.org/tutorials/structured_data/feature_columns#create_an_input_pipeline_using_tfdata\n",
    "    \"\"\"\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop(target_col)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id           200153\n",
      "checkin              425\n",
      "checkout             425\n",
      "city_id            39901\n",
      "device_class           3\n",
      "affiliate_id        3254\n",
      "booker_country         5\n",
      "hotel_country        195\n",
      "utrip_id          217686\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>checkin</th>\n",
       "      <th>checkout</th>\n",
       "      <th>city_id</th>\n",
       "      <th>device_class</th>\n",
       "      <th>affiliate_id</th>\n",
       "      <th>booker_country</th>\n",
       "      <th>hotel_country</th>\n",
       "      <th>utrip_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>519017</th>\n",
       "      <td>727105</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>18820</td>\n",
       "      <td>mobile</td>\n",
       "      <td>359</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>727105_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986596</th>\n",
       "      <td>2000964</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>63341</td>\n",
       "      <td>mobile</td>\n",
       "      <td>8151</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>2000964_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7504</th>\n",
       "      <td>2595109</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>27404</td>\n",
       "      <td>mobile</td>\n",
       "      <td>359</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>2595109_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52884</th>\n",
       "      <td>110418</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>3763</td>\n",
       "      <td>desktop</td>\n",
       "      <td>9924</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Glubbdubdrib</td>\n",
       "      <td>110418_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068508</th>\n",
       "      <td>221863</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>45597</td>\n",
       "      <td>mobile</td>\n",
       "      <td>7774</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>221863_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908844</th>\n",
       "      <td>5755992</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>52860</td>\n",
       "      <td>desktop</td>\n",
       "      <td>4568</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Axphain</td>\n",
       "      <td>5755992_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154024</th>\n",
       "      <td>5842454</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>35850</td>\n",
       "      <td>desktop</td>\n",
       "      <td>384</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Rook Islands</td>\n",
       "      <td>5842454_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160065</th>\n",
       "      <td>5936647</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>20199</td>\n",
       "      <td>desktop</td>\n",
       "      <td>4541</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Kasnia</td>\n",
       "      <td>5936647_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244942</th>\n",
       "      <td>5955565</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>54384</td>\n",
       "      <td>desktop</td>\n",
       "      <td>8132</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>Patusan</td>\n",
       "      <td>5955565_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787249</th>\n",
       "      <td>6172320</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>6005</td>\n",
       "      <td>mobile</td>\n",
       "      <td>9452</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Patusan</td>\n",
       "      <td>6172320_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1166835 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id    checkin   checkout  city_id device_class  affiliate_id  \\\n",
       "519017    727105 2015-12-31 2016-01-01    18820       mobile           359   \n",
       "986596   2000964 2015-12-31 2016-01-01    63341       mobile          8151   \n",
       "7504     2595109 2015-12-31 2016-01-01    27404       mobile           359   \n",
       "52884     110418 2016-01-01 2016-01-02     3763      desktop          9924   \n",
       "1068508   221863 2016-01-01 2016-01-02    45597       mobile          7774   \n",
       "...          ...        ...        ...      ...          ...           ...   \n",
       "908844   5755992 2017-02-27 2017-02-28    52860      desktop          4568   \n",
       "1154024  5842454 2017-02-27 2017-02-28    35850      desktop           384   \n",
       "1160065  5936647 2017-02-27 2017-02-28    20199      desktop          4541   \n",
       "244942   5955565 2017-02-27 2017-02-28    54384      desktop          8132   \n",
       "787249   6172320 2017-02-27 2017-02-28     6005       mobile          9452   \n",
       "\n",
       "               booker_country hotel_country   utrip_id  \n",
       "519017   The Devilfire Empire  Cobra Island   727105_1  \n",
       "986596   The Devilfire Empire  Cobra Island  2000964_1  \n",
       "7504     The Devilfire Empire  Cobra Island  2595109_1  \n",
       "52884    The Devilfire Empire  Glubbdubdrib   110418_1  \n",
       "1068508                Gondal        Gondal   221863_1  \n",
       "...                       ...           ...        ...  \n",
       "908844                 Gondal       Axphain  5755992_4  \n",
       "1154024                Gondal  Rook Islands  5842454_6  \n",
       "1160065                Gondal        Kasnia  5936647_2  \n",
       "244942                Elbonia       Patusan  5955565_2  \n",
       "787249   The Devilfire Empire       Patusan  6172320_1  \n",
       "\n",
       "[1166835 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"booking_train_set.csv\",\n",
    "#                  nrows=523456,\n",
    "                 index_col=[0],\n",
    "                 parse_dates=[\"checkin\",\"checkout\"],infer_datetime_format=True)\n",
    "\n",
    "df.sort_values([ \"checkin\",\n",
    "                \"user_id\"],inplace=True)\n",
    "print(df.nunique())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### i disabled most of thefeature eztraction here for simplicity\n",
    "\n",
    "# df[\"duration\"] = (df[\"checkout\"] - df[\"checkin\"]).dt.days\n",
    "# df[\"same_country\"] = (df[\"booker_country\"]==df[\"hotel_country\"]).astype(int)\n",
    "\n",
    "# df[\"checkin_day\"] = df[\"checkin\"].dt.day\n",
    "# df[\"checkin_weekday\"] = df[\"checkin\"].dt.weekday\n",
    "df[\"checkin_week\"] = df[\"checkin\"].dt.isocalendar().week.astype(int) ## week of year\n",
    "df[\"checkin_month\"] = df[\"checkin\"].dt.month\n",
    "# df[\"checkin_year\"] = df[\"checkin\"].dt.year-2016\n",
    "\n",
    "df[\"checkin_quarter\"] = df[\"checkin\"].dt.quarter # relatively redundant but may be used for \"id\"\n",
    "\n",
    "# df[\"checkin_quarter\"] = df[\"checkin_quarter\"]/4 # scale. could also do cos, sin extraction. makesi t a float instead of int/embedding\n",
    "\n",
    "\n",
    "# df[\"checkout_weekday\"] = df[\"checkout\"].dt.weekday\n",
    "# df[\"checkout_week\"] = df[\"checkout\"].dt.isocalendar().week.astype(int) ## week of year\n",
    "# df[\"checkout_day\"] = df[\"checkout\"].dt.day ## day of month\n",
    "\n",
    "## cyclical datetime embeddings\n",
    "## drop originakl variables? \n",
    "## TODO:L add for other variables, +- those that we'll embed (week?)\n",
    "\n",
    "# df['checkin_weekday_sin'] = np.sin(df[\"checkin_weekday\"]*(2.*np.pi/7))\n",
    "# df['checkin_weekday_cos'] = np.cos(df[\"checkin_weekday\"]*(2.*np.pi/7))\n",
    "# df['checkin_month_sin'] = np.sin((df[\"checkin_month\"]-1)*(2.*np.pi/12))\n",
    "# df['checkin_month_cos'] = np.cos((df[\"checkin_month\"]-1)*(2.*np.pi/12))\n",
    "\n",
    "# #############\n",
    "# # last number in utrip id - probably which trip number it is:\n",
    "# df[\"utrip_number\"] = df[\"utrip_id\"].str.split(\"_\",expand=True)[1].astype(int)\n",
    "\n",
    "### encode string columns - must be consistent with test data \n",
    "### IF we can concat test with train, we can just do a single transformation  for the NON TARGET cols\n",
    "# obj_cols_list = df.select_dtypes(\"O\").columns.values\n",
    "obj_cols_list = ['device_class','booker_country','hotel_country',\n",
    "#                 \"city_id\"\n",
    "                ] # we could also define when loading data, dtype\n",
    "\n",
    "for c in obj_cols_list:\n",
    "    df[c] = df[c].astype(\"category\")\n",
    "#     df[c] = df[c].cat.codes.astype(int)\n",
    "#     print(\"min\",df[c].min()) min is 0 - which is what the embedding (indices) will expect\n",
    "\n",
    "## view steps of a trip per user & trip, in order. ## last step == 1.\n",
    "## count #/pct step in a trip (utrip_id) per user. Useful to get the \"final\" step per trip - for prediction\n",
    "## note that the order is ascending, so we would need to select by \"last\" . (i.e \"1\" is the first step, 2 the second, etc') , or we could use pct .rank(ascending=True,pct=True)\n",
    "#### this feature overlaps with the count of each trip id (for the final row)\n",
    "##  = df.sort_values([\"checkin\",\"checkout\"])... - df already sorted above\n",
    "df[\"utrip_steps_from_end\"] = df.groupby(\"utrip_id\")[\"checkin\"].rank(ascending=True,pct=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### add features to be consistent with test set of row in trip, and total trips in trip\n",
    "\n",
    "df[\"row_num\"] = df.groupby(\"utrip_id\")[\"checkin\"].rank(ascending=True,pct=False).astype(int)\n",
    "utrip_counts = df[\"utrip_id\"].value_counts()\n",
    "df[\"total_rows\"] = df[\"utrip_id\"].map(utrip_counts)\n",
    "\n",
    "### last step in trip\n",
    "df[\"last\"] = (df[\"total_rows\"]==df[\"row_num\"]).astype(int)\n",
    "\n",
    "df[\"total_rows\"].describe();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### replace rare categorical variable(s) - affiliates\n",
    "* replace rare variables (under 2 occurrences) with \"-1\" dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: 9924     277775\n",
      "359      171385\n",
      "384       88137\n",
      "9452      85476\n",
      "4541      41504\n",
      "          ...  \n",
      "8351          1\n",
      "8464          1\n",
      "2202          1\n",
      "10513         1\n",
      "2047          1\n",
      "Name: affiliate_id, Length: 3254, dtype: int64\n",
      "uniques 3254\n",
      "after\n",
      " 9924    277775\n",
      "359     171385\n",
      "384      88137\n",
      "9452     85476\n",
      "4541     41504\n",
      "         ...  \n",
      "2615         3\n",
      "5963         3\n",
      "2618         3\n",
      "838          3\n",
      "176          3\n",
      "Name: affiliate_id, Length: 2152, dtype: int64\n",
      "uniques 2152\n"
     ]
    }
   ],
   "source": [
    "### replace rare variables (under 2 occurrences) with \"-1\" dummy\n",
    "\n",
    "affiliates_counts = df[\"affiliate_id\"].value_counts()\n",
    "print(\"before:\", affiliates_counts)\n",
    "print(\"uniques\",df[\"affiliate_id\"].nunique())\n",
    "affiliates_counts = affiliates_counts.to_dict()\n",
    "# df[\"affiliate_id\"] = df[\"affiliate_id\"].where(df[\"affiliate_id\"].apply(lambda x: x.map(x.value_counts()))>=3, -1)\n",
    "df[\"affiliate_id\"] = df[\"affiliate_id\"].where(df[\"affiliate_id\"].map(affiliates_counts)>=3, -2)\n",
    "df[\"affiliate_id\"] = df[\"affiliate_id\"].astype(int)\n",
    "\n",
    "print(\"after\\n\",df[\"affiliate_id\"].value_counts())\n",
    "print(\"uniques\",df[\"affiliate_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"total_rows\"].map(pd.cut(df[\"total_rows\"],bins=3,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add LAG feature(s)  + DROP First visited location rows\n",
    "* \"first\" hotel country (vs most recent country visited)\n",
    "    * `groupbyFirstLagFeatures(df,group=\"user_id\",lag_feature_cols=[\"hotel_country\",\"city_id\"])`\n",
    "* Can consider: lag1 hotel_country, hotel_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_hotel_country      144\n",
      "hotel_country            195\n",
      "city_id                39901\n",
      "first_city_id           7863\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>checkin</th>\n",
       "      <th>checkout</th>\n",
       "      <th>city_id</th>\n",
       "      <th>device_class</th>\n",
       "      <th>affiliate_id</th>\n",
       "      <th>booker_country</th>\n",
       "      <th>hotel_country</th>\n",
       "      <th>utrip_id</th>\n",
       "      <th>checkin_week</th>\n",
       "      <th>checkin_month</th>\n",
       "      <th>checkin_quarter</th>\n",
       "      <th>utrip_steps_from_end</th>\n",
       "      <th>row_num</th>\n",
       "      <th>total_rows</th>\n",
       "      <th>last</th>\n",
       "      <th>first_hotel_country</th>\n",
       "      <th>first_city_id</th>\n",
       "      <th>first_device_class</th>\n",
       "      <th>first_affiliate_id</th>\n",
       "      <th>first_checkin_quarter</th>\n",
       "      <th>first_checkin_month</th>\n",
       "      <th>first_booker_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>519017</th>\n",
       "      <td>727105</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>18820</td>\n",
       "      <td>mobile</td>\n",
       "      <td>359</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>727105_1</td>\n",
       "      <td>53</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0.250</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>18820</td>\n",
       "      <td>mobile</td>\n",
       "      <td>359</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986596</th>\n",
       "      <td>2000964</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>63341</td>\n",
       "      <td>mobile</td>\n",
       "      <td>8151</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>2000964_1</td>\n",
       "      <td>53</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0.200</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>63341</td>\n",
       "      <td>mobile</td>\n",
       "      <td>8151</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7504</th>\n",
       "      <td>2595109</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>27404</td>\n",
       "      <td>mobile</td>\n",
       "      <td>359</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>2595109_1</td>\n",
       "      <td>53</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0.250</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>18820</td>\n",
       "      <td>mobile</td>\n",
       "      <td>359</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52884</th>\n",
       "      <td>110418</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>3763</td>\n",
       "      <td>desktop</td>\n",
       "      <td>9924</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Glubbdubdrib</td>\n",
       "      <td>110418_1</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>Glubbdubdrib</td>\n",
       "      <td>3763</td>\n",
       "      <td>desktop</td>\n",
       "      <td>9924</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068508</th>\n",
       "      <td>221863</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>45597</td>\n",
       "      <td>mobile</td>\n",
       "      <td>7774</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>221863_1</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.125</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>45597</td>\n",
       "      <td>mobile</td>\n",
       "      <td>7774</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Gondal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908844</th>\n",
       "      <td>5755992</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>52860</td>\n",
       "      <td>desktop</td>\n",
       "      <td>4568</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Axphain</td>\n",
       "      <td>5755992_4</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>29394</td>\n",
       "      <td>desktop</td>\n",
       "      <td>4568</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Gondal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154024</th>\n",
       "      <td>5842454</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>35850</td>\n",
       "      <td>desktop</td>\n",
       "      <td>384</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Rook Islands</td>\n",
       "      <td>5842454_6</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Sylvania</td>\n",
       "      <td>14342</td>\n",
       "      <td>desktop</td>\n",
       "      <td>384</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Gondal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160065</th>\n",
       "      <td>5936647</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>20199</td>\n",
       "      <td>desktop</td>\n",
       "      <td>4541</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Kasnia</td>\n",
       "      <td>5936647_2</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>Rolisica</td>\n",
       "      <td>8462</td>\n",
       "      <td>desktop</td>\n",
       "      <td>4541</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Gondal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244942</th>\n",
       "      <td>5955565</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>54384</td>\n",
       "      <td>desktop</td>\n",
       "      <td>8132</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>Patusan</td>\n",
       "      <td>5955565_2</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>52327</td>\n",
       "      <td>desktop</td>\n",
       "      <td>8132</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Elbonia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787249</th>\n",
       "      <td>6172320</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>6005</td>\n",
       "      <td>mobile</td>\n",
       "      <td>9452</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Patusan</td>\n",
       "      <td>6172320_1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Sokovia</td>\n",
       "      <td>34342</td>\n",
       "      <td>mobile</td>\n",
       "      <td>9452</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1166835 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id    checkin   checkout  city_id device_class  affiliate_id  \\\n",
       "519017    727105 2015-12-31 2016-01-01    18820       mobile           359   \n",
       "986596   2000964 2015-12-31 2016-01-01    63341       mobile          8151   \n",
       "7504     2595109 2015-12-31 2016-01-01    27404       mobile           359   \n",
       "52884     110418 2016-01-01 2016-01-02     3763      desktop          9924   \n",
       "1068508   221863 2016-01-01 2016-01-02    45597       mobile          7774   \n",
       "...          ...        ...        ...      ...          ...           ...   \n",
       "908844   5755992 2017-02-27 2017-02-28    52860      desktop          4568   \n",
       "1154024  5842454 2017-02-27 2017-02-28    35850      desktop           384   \n",
       "1160065  5936647 2017-02-27 2017-02-28    20199      desktop          4541   \n",
       "244942   5955565 2017-02-27 2017-02-28    54384      desktop          8132   \n",
       "787249   6172320 2017-02-27 2017-02-28     6005       mobile          9452   \n",
       "\n",
       "               booker_country hotel_country   utrip_id  checkin_week  \\\n",
       "519017   The Devilfire Empire  Cobra Island   727105_1            53   \n",
       "986596   The Devilfire Empire  Cobra Island  2000964_1            53   \n",
       "7504     The Devilfire Empire  Cobra Island  2595109_1            53   \n",
       "52884    The Devilfire Empire  Glubbdubdrib   110418_1            53   \n",
       "1068508                Gondal        Gondal   221863_1            53   \n",
       "...                       ...           ...        ...           ...   \n",
       "908844                 Gondal       Axphain  5755992_4             9   \n",
       "1154024                Gondal  Rook Islands  5842454_6             9   \n",
       "1160065                Gondal        Kasnia  5936647_2             9   \n",
       "244942                Elbonia       Patusan  5955565_2             9   \n",
       "787249   The Devilfire Empire       Patusan  6172320_1             9   \n",
       "\n",
       "         checkin_month  checkin_quarter  utrip_steps_from_end  row_num  \\\n",
       "519017              12                4                 0.250        1   \n",
       "986596              12                4                 0.200        1   \n",
       "7504                12                4                 0.250        1   \n",
       "52884                1                1                 0.100        1   \n",
       "1068508              1                1                 0.125        1   \n",
       "...                ...              ...                   ...      ...   \n",
       "908844               2                1                 1.000        5   \n",
       "1154024              2                1                 1.000        9   \n",
       "1160065              2                1                 1.000       12   \n",
       "244942               2                1                 1.000        5   \n",
       "787249               2                1                 1.000       10   \n",
       "\n",
       "         total_rows  last first_hotel_country  first_city_id  \\\n",
       "519017            4     0        Cobra Island          18820   \n",
       "986596            5     0        Cobra Island          63341   \n",
       "7504              4     0        Cobra Island          18820   \n",
       "52884            10     0        Glubbdubdrib           3763   \n",
       "1068508           8     0              Gondal          45597   \n",
       "...             ...   ...                 ...            ...   \n",
       "908844            5     1              Gondal          29394   \n",
       "1154024           9     1            Sylvania          14342   \n",
       "1160065          12     1            Rolisica           8462   \n",
       "244942            5     1             Elbonia          52327   \n",
       "787249           10     1             Sokovia          34342   \n",
       "\n",
       "        first_device_class  first_affiliate_id  first_checkin_quarter  \\\n",
       "519017              mobile                 359                      4   \n",
       "986596              mobile                8151                      4   \n",
       "7504                mobile                 359                      4   \n",
       "52884              desktop                9924                      1   \n",
       "1068508             mobile                7774                      1   \n",
       "...                    ...                 ...                    ...   \n",
       "908844             desktop                4568                      1   \n",
       "1154024            desktop                 384                      1   \n",
       "1160065            desktop                4541                      1   \n",
       "244942             desktop                8132                      1   \n",
       "787249              mobile                9452                      1   \n",
       "\n",
       "         first_checkin_month  first_booker_country  \n",
       "519017                    12  The Devilfire Empire  \n",
       "986596                    12  The Devilfire Empire  \n",
       "7504                      12  The Devilfire Empire  \n",
       "52884                      1  The Devilfire Empire  \n",
       "1068508                    1                Gondal  \n",
       "...                      ...                   ...  \n",
       "908844                     2                Gondal  \n",
       "1154024                    2                Gondal  \n",
       "1160065                    2                Gondal  \n",
       "244942                     2               Elbonia  \n",
       "787249                     2  The Devilfire Empire  \n",
       "\n",
       "[1166835 rows x 23 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## add the \"first\" place visited/values\n",
    "### nopte - will need to drop first row in trip, or impute nans when using this feature \n",
    "\n",
    "### first by user results in too much sparsity/rareness for our IDs purposes\n",
    "# df = groupbyFirstLagFeatures(df,group=\"user_id\",lag_feature_cols=[\"hotel_country\"]) # [\"hotel_country\",\"city_id\"]\n",
    "\n",
    "## alt - messy, but maybe good enough : \n",
    "df = groupbyFirstLagFeatures(df,group=['device_class', 'affiliate_id',\n",
    "                                       'booker_country','checkin_month',\"last\"],\n",
    "                             lag_feature_cols=[\"hotel_country\",\"city_id\",'device_class', 'affiliate_id',\n",
    "                                              \"checkin_quarter\",\"checkin_month\",\"booker_country\"])\n",
    "\n",
    "\n",
    "print(df[[\"first_hotel_country\",\"hotel_country\",\"city_id\",\"first_city_id\"]].nunique())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id                  200153\n",
       "checkin                     425\n",
       "checkout                    425\n",
       "city_id                   39901\n",
       "device_class                  3\n",
       "affiliate_id               2152\n",
       "booker_country                5\n",
       "hotel_country               195\n",
       "utrip_id                 217686\n",
       "checkin_week                 53\n",
       "checkin_month                12\n",
       "checkin_quarter               4\n",
       "utrip_steps_from_end        520\n",
       "row_num                      48\n",
       "total_rows                   41\n",
       "last                          2\n",
       "first_hotel_country         144\n",
       "first_city_id              7863\n",
       "first_device_class            3\n",
       "first_affiliate_id         2152\n",
       "first_checkin_quarter         4\n",
       "first_checkin_month          12\n",
       "first_booker_country          5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create pseudo ID for use with CF - composed of semi-distinct tuple of variables\n",
    "* should have moderate uniqueness. Main purpose is to get good embeddings for those variables \n",
    "\n",
    "* `'device_class','affiliate_id', 'booker_country','checkin_quarter'` - 14K \"uniques\"\n",
    "* `'device_class','affiliate_id', 'booker_country'` - 7.5 K \"uniques\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "booker_country           5\n",
      "checkin_quarter          4\n",
      "first_hotel_country    144\n",
      "checkin_month           12\n",
      "total_rows              41\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "booker_country        checkin_quarter  first_hotel_country           \n",
       "Bartovia              1                Absurdistan                         0\n",
       "                                       Aldorria                            0\n",
       "                                       Aldovia                            16\n",
       "                                       Almaigne                            0\n",
       "                                       Altis and Stratis, Republic of      3\n",
       "                                                                        ... \n",
       "The Devilfire Empire  4                Yellow Empire                       0\n",
       "                                       Yerba                               2\n",
       "                                       Yudonia                           309\n",
       "                                       Zekistan                            1\n",
       "                                       Zephyria                            0\n",
       "Length: 3900, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### for possible \"user id\" embedding/ID : How many unique values are there for these source tuple? :\n",
    "### Could also maybe add previous location/lag1 country/city ? \n",
    "## 'device_class','affiliate_id', 'booker_country' - 7.5 K \"uniques\"\n",
    "## 'device_class','affiliate_id', 'booker_country','checkin_month' - 24 K \"uniques\"\n",
    "## 'device_class','affiliate_id', 'booker_country','checkin_quarter' 14K \"uniques\"\n",
    "## ,\"total_rows\" \n",
    "\n",
    "print(df[ID_COLS + ['checkin_month',\"total_rows\"]].nunique(axis=0))\n",
    "df.groupby(ID_COLS).size()\n",
    "\n",
    "# id2 = [item for item in ID_COLS if item != \"last\"]\n",
    "# df.groupby(id2).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['booker_country', 'checkin_quarter', 'first_hotel_country']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ID_COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     1411.000000\n",
      "mean       826.956060\n",
      "std       2937.318129\n",
      "min          1.000000\n",
      "25%          7.000000\n",
      "50%         42.000000\n",
      "75%        349.500000\n",
      "max      55205.000000\n",
      "Name: ID, dtype: float64\n",
      "1160065                 Gondal1Rolisica\n",
      "244942                  Elbonia1Elbonia\n",
      "787249     The Devilfire Empire1Sokovia\n",
      "Name: ID, dtype: object\n",
      "1411\n"
     ]
    }
   ],
   "source": [
    "df[\"ID\"] = df[ID_COLS].astype(str).sum(1)#.astype(\"category\")\n",
    "\n",
    "print(df[\"ID\"].value_counts().describe())\n",
    "print(df[\"ID\"].tail(3))\n",
    "print(df[\"ID\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keep seperate DF of IDs and component features - will make merging id feats simpler later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>booker_country</th>\n",
       "      <th>checkin_quarter</th>\n",
       "      <th>first_hotel_country</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>The Devilfire Empire4Cobra Island</th>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>4</td>\n",
       "      <td>Cobra Island</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Devilfire Empire1Glubbdubdrib</th>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>1</td>\n",
       "      <td>Glubbdubdrib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gondal1Gondal</th>\n",
       "      <td>Gondal</td>\n",
       "      <td>1</td>\n",
       "      <td>Gondal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Elbonia1Patusan</th>\n",
       "      <td>Elbonia</td>\n",
       "      <td>1</td>\n",
       "      <td>Patusan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Devilfire Empire1Nova Africa</th>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>1</td>\n",
       "      <td>Nova Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Devilfire Empire1Laurania</th>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>1</td>\n",
       "      <td>Laurania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gondal1San Theodoros</th>\n",
       "      <td>Gondal</td>\n",
       "      <td>1</td>\n",
       "      <td>San Theodoros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bartovia1Buranda</th>\n",
       "      <td>Bartovia</td>\n",
       "      <td>1</td>\n",
       "      <td>Buranda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Devilfire Empire1Yerba</th>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>1</td>\n",
       "      <td>Yerba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gondal1Yudonia</th>\n",
       "      <td>Gondal</td>\n",
       "      <td>1</td>\n",
       "      <td>Yudonia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1411 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         booker_country  checkin_quarter  \\\n",
       "ID                                                                         \n",
       "The Devilfire Empire4Cobra Island  The Devilfire Empire                4   \n",
       "The Devilfire Empire1Glubbdubdrib  The Devilfire Empire                1   \n",
       "Gondal1Gondal                                    Gondal                1   \n",
       "Elbonia1Patusan                                 Elbonia                1   \n",
       "The Devilfire Empire1Nova Africa   The Devilfire Empire                1   \n",
       "...                                                 ...              ...   \n",
       "The Devilfire Empire1Laurania      The Devilfire Empire                1   \n",
       "Gondal1San Theodoros                             Gondal                1   \n",
       "Bartovia1Buranda                               Bartovia                1   \n",
       "The Devilfire Empire1Yerba         The Devilfire Empire                1   \n",
       "Gondal1Yudonia                                   Gondal                1   \n",
       "\n",
       "                                  first_hotel_country  \n",
       "ID                                                     \n",
       "The Devilfire Empire4Cobra Island        Cobra Island  \n",
       "The Devilfire Empire1Glubbdubdrib        Glubbdubdrib  \n",
       "Gondal1Gondal                                  Gondal  \n",
       "Elbonia1Patusan                               Patusan  \n",
       "The Devilfire Empire1Nova Africa          Nova Africa  \n",
       "...                                               ...  \n",
       "The Devilfire Empire1Laurania                Laurania  \n",
       "Gondal1San Theodoros                    San Theodoros  \n",
       "Bartovia1Buranda                              Buranda  \n",
       "The Devilfire Empire1Yerba                      Yerba  \n",
       "Gondal1Yudonia                                Yudonia  \n",
       "\n",
       "[1411 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ID = df[[\"ID\"]+ID_COLS].drop_duplicates().set_index(\"ID\")\n",
    "df_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    39901.000000\n",
      "mean        29.243252\n",
      "std        218.801654\n",
      "min          1.000000\n",
      "25%          1.000000\n",
      "50%          3.000000\n",
      "75%          9.000000\n",
      "max      11242.000000\n",
      "Name: city_id, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>checkin</th>\n",
       "      <th>checkout</th>\n",
       "      <th>city_id</th>\n",
       "      <th>device_class</th>\n",
       "      <th>affiliate_id</th>\n",
       "      <th>booker_country</th>\n",
       "      <th>hotel_country</th>\n",
       "      <th>utrip_id</th>\n",
       "      <th>checkin_week</th>\n",
       "      <th>checkin_month</th>\n",
       "      <th>checkin_quarter</th>\n",
       "      <th>utrip_steps_from_end</th>\n",
       "      <th>row_num</th>\n",
       "      <th>total_rows</th>\n",
       "      <th>last</th>\n",
       "      <th>first_hotel_country</th>\n",
       "      <th>first_city_id</th>\n",
       "      <th>first_device_class</th>\n",
       "      <th>first_affiliate_id</th>\n",
       "      <th>first_checkin_quarter</th>\n",
       "      <th>first_checkin_month</th>\n",
       "      <th>first_booker_country</th>\n",
       "      <th>ID</th>\n",
       "      <th>city_id_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>519017</th>\n",
       "      <td>727105</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>18820</td>\n",
       "      <td>mobile</td>\n",
       "      <td>359</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>727105_1</td>\n",
       "      <td>53</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0.250</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>18820</td>\n",
       "      <td>mobile</td>\n",
       "      <td>359</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>The Devilfire Empire4Cobra Island</td>\n",
       "      <td>535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986596</th>\n",
       "      <td>2000964</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>63341</td>\n",
       "      <td>mobile</td>\n",
       "      <td>8151</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>2000964_1</td>\n",
       "      <td>53</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0.200</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>63341</td>\n",
       "      <td>mobile</td>\n",
       "      <td>8151</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>The Devilfire Empire4Cobra Island</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7504</th>\n",
       "      <td>2595109</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>27404</td>\n",
       "      <td>mobile</td>\n",
       "      <td>359</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>2595109_1</td>\n",
       "      <td>53</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0.250</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>18820</td>\n",
       "      <td>mobile</td>\n",
       "      <td>359</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>The Devilfire Empire4Cobra Island</td>\n",
       "      <td>3614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52884</th>\n",
       "      <td>110418</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>3763</td>\n",
       "      <td>desktop</td>\n",
       "      <td>9924</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Glubbdubdrib</td>\n",
       "      <td>110418_1</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>Glubbdubdrib</td>\n",
       "      <td>3763</td>\n",
       "      <td>desktop</td>\n",
       "      <td>9924</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>The Devilfire Empire1Glubbdubdrib</td>\n",
       "      <td>5544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068508</th>\n",
       "      <td>221863</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>45597</td>\n",
       "      <td>mobile</td>\n",
       "      <td>7774</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>221863_1</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.125</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>45597</td>\n",
       "      <td>mobile</td>\n",
       "      <td>7774</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Gondal1Gondal</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908844</th>\n",
       "      <td>5755992</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>52860</td>\n",
       "      <td>desktop</td>\n",
       "      <td>4568</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Axphain</td>\n",
       "      <td>5755992_4</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>29394</td>\n",
       "      <td>desktop</td>\n",
       "      <td>4568</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Gondal1Gondal</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154024</th>\n",
       "      <td>5842454</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>35850</td>\n",
       "      <td>desktop</td>\n",
       "      <td>384</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Rook Islands</td>\n",
       "      <td>5842454_6</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Sylvania</td>\n",
       "      <td>14342</td>\n",
       "      <td>desktop</td>\n",
       "      <td>384</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Gondal1Sylvania</td>\n",
       "      <td>692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160065</th>\n",
       "      <td>5936647</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>20199</td>\n",
       "      <td>desktop</td>\n",
       "      <td>4541</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Kasnia</td>\n",
       "      <td>5936647_2</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>Rolisica</td>\n",
       "      <td>8462</td>\n",
       "      <td>desktop</td>\n",
       "      <td>4541</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Gondal1Rolisica</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244942</th>\n",
       "      <td>5955565</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>54384</td>\n",
       "      <td>desktop</td>\n",
       "      <td>8132</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>Patusan</td>\n",
       "      <td>5955565_2</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>52327</td>\n",
       "      <td>desktop</td>\n",
       "      <td>8132</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Elbonia</td>\n",
       "      <td>Elbonia1Elbonia</td>\n",
       "      <td>526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787249</th>\n",
       "      <td>6172320</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>6005</td>\n",
       "      <td>mobile</td>\n",
       "      <td>9452</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Patusan</td>\n",
       "      <td>6172320_1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Sokovia</td>\n",
       "      <td>34342</td>\n",
       "      <td>mobile</td>\n",
       "      <td>9452</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>The Devilfire Empire1Sokovia</td>\n",
       "      <td>367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1166835 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id    checkin   checkout  city_id device_class  affiliate_id  \\\n",
       "519017    727105 2015-12-31 2016-01-01    18820       mobile           359   \n",
       "986596   2000964 2015-12-31 2016-01-01    63341       mobile          8151   \n",
       "7504     2595109 2015-12-31 2016-01-01    27404       mobile           359   \n",
       "52884     110418 2016-01-01 2016-01-02     3763      desktop          9924   \n",
       "1068508   221863 2016-01-01 2016-01-02    45597       mobile          7774   \n",
       "...          ...        ...        ...      ...          ...           ...   \n",
       "908844   5755992 2017-02-27 2017-02-28    52860      desktop          4568   \n",
       "1154024  5842454 2017-02-27 2017-02-28    35850      desktop           384   \n",
       "1160065  5936647 2017-02-27 2017-02-28    20199      desktop          4541   \n",
       "244942   5955565 2017-02-27 2017-02-28    54384      desktop          8132   \n",
       "787249   6172320 2017-02-27 2017-02-28     6005       mobile          9452   \n",
       "\n",
       "               booker_country hotel_country   utrip_id  checkin_week  \\\n",
       "519017   The Devilfire Empire  Cobra Island   727105_1            53   \n",
       "986596   The Devilfire Empire  Cobra Island  2000964_1            53   \n",
       "7504     The Devilfire Empire  Cobra Island  2595109_1            53   \n",
       "52884    The Devilfire Empire  Glubbdubdrib   110418_1            53   \n",
       "1068508                Gondal        Gondal   221863_1            53   \n",
       "...                       ...           ...        ...           ...   \n",
       "908844                 Gondal       Axphain  5755992_4             9   \n",
       "1154024                Gondal  Rook Islands  5842454_6             9   \n",
       "1160065                Gondal        Kasnia  5936647_2             9   \n",
       "244942                Elbonia       Patusan  5955565_2             9   \n",
       "787249   The Devilfire Empire       Patusan  6172320_1             9   \n",
       "\n",
       "         checkin_month  checkin_quarter  utrip_steps_from_end  row_num  \\\n",
       "519017              12                4                 0.250        1   \n",
       "986596              12                4                 0.200        1   \n",
       "7504                12                4                 0.250        1   \n",
       "52884                1                1                 0.100        1   \n",
       "1068508              1                1                 0.125        1   \n",
       "...                ...              ...                   ...      ...   \n",
       "908844               2                1                 1.000        5   \n",
       "1154024              2                1                 1.000        9   \n",
       "1160065              2                1                 1.000       12   \n",
       "244942               2                1                 1.000        5   \n",
       "787249               2                1                 1.000       10   \n",
       "\n",
       "         total_rows  last first_hotel_country  first_city_id  \\\n",
       "519017            4     0        Cobra Island          18820   \n",
       "986596            5     0        Cobra Island          63341   \n",
       "7504              4     0        Cobra Island          18820   \n",
       "52884            10     0        Glubbdubdrib           3763   \n",
       "1068508           8     0              Gondal          45597   \n",
       "...             ...   ...                 ...            ...   \n",
       "908844            5     1              Gondal          29394   \n",
       "1154024           9     1            Sylvania          14342   \n",
       "1160065          12     1            Rolisica           8462   \n",
       "244942            5     1             Elbonia          52327   \n",
       "787249           10     1             Sokovia          34342   \n",
       "\n",
       "        first_device_class  first_affiliate_id  first_checkin_quarter  \\\n",
       "519017              mobile                 359                      4   \n",
       "986596              mobile                8151                      4   \n",
       "7504                mobile                 359                      4   \n",
       "52884              desktop                9924                      1   \n",
       "1068508             mobile                7774                      1   \n",
       "...                    ...                 ...                    ...   \n",
       "908844             desktop                4568                      1   \n",
       "1154024            desktop                 384                      1   \n",
       "1160065            desktop                4541                      1   \n",
       "244942             desktop                8132                      1   \n",
       "787249              mobile                9452                      1   \n",
       "\n",
       "         first_checkin_month  first_booker_country  \\\n",
       "519017                    12  The Devilfire Empire   \n",
       "986596                    12  The Devilfire Empire   \n",
       "7504                      12  The Devilfire Empire   \n",
       "52884                      1  The Devilfire Empire   \n",
       "1068508                    1                Gondal   \n",
       "...                      ...                   ...   \n",
       "908844                     2                Gondal   \n",
       "1154024                    2                Gondal   \n",
       "1160065                    2                Gondal   \n",
       "244942                     2               Elbonia   \n",
       "787249                     2  The Devilfire Empire   \n",
       "\n",
       "                                        ID  city_id_count  \n",
       "519017   The Devilfire Empire4Cobra Island            535  \n",
       "986596   The Devilfire Empire4Cobra Island            196  \n",
       "7504     The Devilfire Empire4Cobra Island           3614  \n",
       "52884    The Devilfire Empire1Glubbdubdrib           5544  \n",
       "1068508                      Gondal1Gondal            101  \n",
       "...                                    ...            ...  \n",
       "908844                       Gondal1Gondal             12  \n",
       "1154024                    Gondal1Sylvania            692  \n",
       "1160065                    Gondal1Rolisica             12  \n",
       "244942                     Elbonia1Elbonia            526  \n",
       "787249        The Devilfire Empire1Sokovia            367  \n",
       "\n",
       "[1166835 rows x 25 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Following aggregation features - would be best to use time window (sort data) to generate, otherwise they will LEAK! (e.g. nunique countries visited)\n",
    "\n",
    "### count features (can also later add rank inside groups).\n",
    "### Some may be leaks (# visits in a trip should use time window?) , and do users repeat? \n",
    "### can add more counts of group X time period (e.g. affiliate X month of year)\n",
    "\n",
    "## alt way to get counts/freq :\n",
    "\n",
    "if GET_COUNT_AGG_FEATS:\n",
    "    count_cols = [ 'city_id','affiliate_id', 'booker_country', 'hotel_country', \n",
    "    #               'utrip_id','user_id', \n",
    "     \"checkin_month\",\"checkin_week\"]\n",
    "    for c in count_cols:\n",
    "        df[f\"{c}_count\"] = df.groupby([c])[\"duration\"].transform(\"size\")\n",
    "\n",
    "    ########################################################\n",
    "    ## nunique per trip\n",
    "    ### https://stackoverflow.com/questions/46470743/how-to-efficiently-compute-a-rolling-unique-count-in-a-pandas-time-series\n",
    "\n",
    "    nunique_cols = [ 'city_id','affiliate_id', 'booker_country', 'hotel_country']\n",
    "    # df[\"nunique_booker_countries\"] = df.groupby(\"utrip_id\")[\"booker_country\"].nunique()\n",
    "    # df[\"nunique_hotel_country\"] = df.groupby(\"utrip_id\")[\"hotel_country\"].nunique()\n",
    "    for c in nunique_cols:\n",
    "        df[f\"{c}_nunique\"] = df.groupby([\"utrip_id\"])[c].transform(\"nunique\")\n",
    "    print(df.nunique())\n",
    "\n",
    "    ########################################################\n",
    "    ## get frequency/count feature's rank within a group - e.g. within a country (or affiliate) \n",
    "    ## add \"_count\" to column name to get count col name, then add rank col \n",
    "\n",
    "    ### ALT/ duplicate feat - add percent rank (instead or in addition)\n",
    "\n",
    "    rank_cols = ['city_id','affiliate_id', 'booker_country','hotel_country',\n",
    "     \"checkin_month\"]\n",
    "    ### what is meaning of groupby and rank of smae variable by same var? Surely should be 1 / unary? \n",
    "    for c in rank_cols:\n",
    "        df[f\"{c}_rank_by_hotel_country\"] = df.groupby(['hotel_country'])[f\"{c}_count\"].transform(\"rank\")\n",
    "        df[f\"{c}_rank_by_booker_country\"] = df.groupby(['booker_country'])[f\"{c}_count\"].transform(\"rank\")\n",
    "        df[f\"{c}_rank_by_affiliate\"] = df.groupby(['affiliate_id'])[f\"{c}_count\"].transform(\"rank\")     \n",
    "else:\n",
    "    freq = df[\"city_id\"].value_counts()\n",
    "    df[\"city_id_count\"] = df[\"city_id\"].map(freq)\n",
    "    print(freq.describe())\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cities with more than 7 occurences: 12345\n",
      "cities with more than MIN_TARGET_FREQ (38) occurences: 3498\n",
      "rows left if filtering by MIN_TARGET_FREQ : 970972\n"
     ]
    }
   ],
   "source": [
    "print(\"cities with more than 7 occurences:\",df.loc[df[\"city_id_count\"]>=7][\"city_id\"].nunique())\n",
    "print(f\"cities with more than MIN_TARGET_FREQ ({MIN_TARGET_FREQ}) occurences:\",df.loc[df[\"city_id_count\"]>=MIN_TARGET_FREQ][\"city_id\"].nunique())\n",
    "print(f\"rows left if filtering by MIN_TARGET_FREQ :\",df.loc[df[\"city_id_count\"]>=MIN_TARGET_FREQ].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df.isna().sum().max() ==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1029804\n",
      "df2 nunique (cities without duplicate user visits) 39901\n",
      "city counts\n",
      "23921    8137\n",
      "55128    7197\n",
      "47499    7188\n",
      "64876    6724\n",
      "29319    6361\n",
      "         ... \n",
      "50916       1\n",
      "57063       1\n",
      "46826       1\n",
      "38638       1\n",
      "2049        1\n",
      "Name: city_id, Length: 39901, dtype: int64\n",
      "count    39901.000000\n",
      "mean        25.808977\n",
      "std        173.750203\n",
      "min          1.000000\n",
      "25%          1.000000\n",
      "50%          3.000000\n",
      "75%          9.000000\n",
      "max       8137.000000\n",
      "Name: city_id, dtype: float64\n",
      "cities with at least 2: 27533\n",
      "cities with at least 3: 21337\n",
      "cities with at least 5: 15248\n",
      "cities with at least 10: 9281\n",
      "cities with at least 15: 6836\n",
      "cities with at least 20: 5499\n",
      "cities with at least 30: 4006\n",
      "cities with at least 50: 2673\n",
      "cities with at least 100: 1550\n",
      "top 4 sum coverage (normalized):  0.028\n",
      "top 50 sum coverage (normalized):  0.183\n",
      "top 100 sum coverage (normalized):  0.263\n",
      "top 400 sum coverage (normalized):  0.481\n",
      "top 1,000 sum coverage (normalized):  0.64\n",
      "top 5,000 sum coverage (normalized):  0.858\n",
      "top 8,000 sum coverage (normalized):  0.905\n",
      "top 15,000 sum coverage (normalized):  0.955\n"
     ]
    }
   ],
   "source": [
    "# df2 = df[[\"user_id\",\"city_id\"]].drop_duplicates().copy()\n",
    "df2 = df.drop_duplicates(subset=[\"user_id\",\"city_id\"])[\"city_id\"].copy()\n",
    "print(df2.shape[0])\n",
    "print(\"df2 nunique (cities without duplicate user visits)\",df2.nunique())\n",
    "\n",
    "# c2_counts = df2[\"city_id\"].value_counts()\n",
    "c2_counts = df2.value_counts()\n",
    "# df2[\"new_counts\"] = df2[\"city_id\"].map(c2_counts)\n",
    "# df2[\"new_counts\"] = df2.map(c2_counts)\n",
    "print(\"city counts\")\n",
    "print(c2_counts)\n",
    "print(c2_counts.describe())\n",
    "print(\"cities with at least 2:\",(c2_counts>=2).sum())\n",
    "print(\"cities with at least 3:\",(c2_counts>=3).sum())\n",
    "print(\"cities with at least 5:\",(c2_counts>=5).sum())\n",
    "print(\"cities with at least 10:\",(c2_counts>=10).sum())\n",
    "print(\"cities with at least 15:\",(c2_counts>=15).sum())\n",
    "print(\"cities with at least 20:\",(c2_counts>=20).sum())\n",
    "print(\"cities with at least 30:\",(c2_counts>=30).sum())\n",
    "print(\"cities with at least 50:\",(c2_counts>=50).sum())\n",
    "print(\"cities with at least 100:\",(c2_counts>=100).sum())\n",
    "\n",
    "c2_freq = df2.value_counts(normalize=True)\n",
    "print(\"top 4 sum coverage (normalized): \",c2_freq[0:4].sum().round(3))\n",
    "print(\"top 50 sum coverage (normalized): \",c2_freq[0:50].sum().round(3))\n",
    "print(\"top 100 sum coverage (normalized): \",c2_freq[0:100].sum().round(3))\n",
    "print(\"top 400 sum coverage (normalized): \",c2_freq[0:400].sum().round(3))\n",
    "print(\"top 1,000 sum coverage (normalized): \",c2_freq[0:1000].sum().round(3))\n",
    "print(\"top 5,000 sum coverage (normalized): \",c2_freq[0:5000].sum().round(3))\n",
    "print(\"top 8,000 sum coverage (normalized): \",c2_freq[0:8000].sum().round(3))\n",
    "print(\"top 15,000 sum coverage (normalized): \",c2_freq[0:15000].sum().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent city target List + City count encoding\n",
    "* Get the K most frequent target city IDs - selected based on frequency as final destination (not just overall)\n",
    "* +- Also after this, replace rare city IDs categorical features with count encoding to reduce dimensionality\n",
    "    * Keep them as count, or aggregate all of them as \"under_K\"?\n",
    "\n",
    "##### Output  : `TOP_TARGETS` - filter data by this *after* creation of lag features ! \n",
    "\n",
    "* Drop duplicates by the same user (reduce possible bias of frequent users? Only relevant if test is seperater from \"frequent travellers\") \n",
    "    * results in 216,633 , vs 217,686 without dropping duplicates by users\n",
    "    * ~19.9k unique cities\n",
    "    \n",
    "* Could do other encodings - https://contrib.scikit-learn.org/category_encoders/count.html\n",
    "\n",
    "* Note that all this is after we've added rank, count features beforehand, so that information won't be lost for these variables, despite these transforms\n",
    "\n",
    "\n",
    "\n",
    "* **NOTE** he most frequent final destinations are NOT the same as the most popular overall destinations +- first location ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if KEEP_TOP_K_TARGETS > 0 :\n",
    "    df_end = df.loc[df[\"utrip_steps_from_end\"]==1].drop_duplicates(subset=[\"city_id\",\"hotel_country\",\"user_id\"])[[\"city_id\",\"hotel_country\"]].copy()\n",
    "    print(df_end.shape[0])\n",
    "    end_city_counts = df_end.city_id.value_counts()\n",
    "    print(end_city_counts)\n",
    "    \n",
    "    TOP_TARGETS = end_city_counts.head(KEEP_TOP_K_TARGETS).index.values\n",
    "    print(f\"top {KEEP_TOP_K_TARGETS} targets \\n\",TOP_TARGETS)\n",
    "    \n",
    "#     assert df.loc[df[\"city_id\"].isin(TOP_TARGETS)][\"city_id\"].nunique() == KEEP_TOP_K_TARGETS\n",
    "\n",
    "####        \n",
    "# replace low frequency categoircal features    \n",
    "\n",
    "# ##replace with count encoding if have at least k, group rarest as \"-1\":# df[BASE_CAT_COLS] = df[BASE_CAT_COLS].where(df[BASE_CAT_COLS].apply(lambda x: x.map(x.value_counts()))>=LOW_COUNT_THRESH, -1)   \n",
    "# ## replace/group only the rare variables : \n",
    "# df[BASE_CAT_COLS] = df[BASE_CAT_COLS].where(df[BASE_CAT_COLS].apply(lambda x: x.map(x.value_counts()))>=LOW_COUNT_THRESH, -1)\n",
    "# df[BASE_CAT_COLS].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Long tail of targets warning!\n",
    "* 75% of cities appear less than 4 times in the data (as a final destination!) \n",
    "    * Dropping them will mean a maximum accuracy of 25% at best!!\n",
    "    * training on intermediates may help overcome improve this. \n",
    "* Using ~2d step+ , still leaves us with 75% appearing less than 7 times\n",
    "\n",
    "* Top 4,000 cities (just for those as final trip destination) - offers 89% coverage - \n",
    "\n",
    "* Unsure how to handle this - too amny targets to learn, and no auxiliary data to help learn it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df_end.city_id.value_counts(normalize=True)[0:4000].sum().round(3)# 4k: 89% coverage  (note, this is just for the end count cities, not all cities overall)\n",
    "\n",
    "# df_end.city_id.value_counts(normalize=True)[0:2000].sum().round(3) #7k: 97% coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter data by city id frequency\n",
    "#### df2 - smaller df (may not ben ecessary to make\n",
    "\n",
    "* drop rows if it's city id appears less than X times - this is prior to CF\n",
    "* We could also add inclusion/exclusion based on target appearing/frequency as target in final stage of rtip - optional\n",
    "* maybe also drop (end exclude in freq counting) thefirst point in a trip ? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropping users with less than 4 trips\n",
      "dropping cities with less than 38 occurences:\n",
      "968939\n",
      "dropping users with less than 4 instances, after previous filters:\n",
      "rows left: 788514\n",
      "nunique cities after freq filt city_id            2530\n",
      "utrip_id         150653\n",
      "user_id          141625\n",
      "hotel_country        96\n",
      "dtype: int64\n",
      "nunique city_id per hotel_country:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    195.000000\n",
       "mean      12.974359\n",
       "std       36.190139\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.000000\n",
       "75%        7.000000\n",
       "max      303.000000\n",
       "Name: city_id, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### unsure about this filtering - depends if data points are real or mistake\n",
    "print(\"dropping users with less than 4 trips\")\n",
    "df2 = df.loc[df[\"total_rows\"]>=4]#.copy()\n",
    "# print(\"abnormal users dropped\",df.shape[0]-df2.shape[0])\n",
    "\n",
    "print(f\"dropping cities with less than {MIN_TARGET_FREQ} occurences:\")\n",
    "df2 = df2.loc[df2.groupby([\"city_id\"])[\"hotel_country\"].transform(\"count\")>=MIN_TARGET_FREQ] ## update count\n",
    "# df2 = df2.loc[df2[\"city_id_count\"]>=MIN_TARGET_FREQ]\n",
    "print(df2.shape[0])\n",
    "\n",
    "# print(f\"dropping users with less than 4 instances, after previous city filter:\")\n",
    "df2 = df2.loc[df2.groupby([\"utrip_id\"])[TARGET_COL].transform(\"count\")>=4]\n",
    "\n",
    "# print(f\"dropping cities with less than {MIN_TARGET_FREQ} occurences:\")\n",
    "df2 = df2.loc[df2.groupby([\"city_id\"])[\"hotel_country\"].transform(\"count\")>=MIN_TARGET_FREQ]\n",
    "\n",
    "print(f\"dropping users with less than 4 instances, after previous filters:\")\n",
    "df2 = df2.loc[df2.groupby([\"utrip_id\"])[TARGET_COL].transform(\"count\")>=4]\n",
    "\n",
    "df2 = df2.loc[df2.groupby([\"city_id\"])[\"hotel_country\"].transform(\"count\")>=MIN_TARGET_FREQ]\n",
    "df2 = df2.loc[df2.groupby([\"utrip_id\"])[TARGET_COL].transform(\"count\")>=4]\n",
    "df2 = df2.loc[df2.groupby([\"city_id\"])[\"hotel_country\"].transform(\"count\")>=MIN_TARGET_FREQ]\n",
    "df2 = df2.loc[df2.groupby([\"utrip_id\"])[TARGET_COL].transform(\"count\")>=4]\n",
    "df2 = df2.loc[df2.groupby([\"city_id\"])[\"hotel_country\"].transform(\"count\")>=MIN_TARGET_FREQ]\n",
    "df2 = df2.loc[df2.groupby([\"utrip_id\"])[TARGET_COL].transform(\"count\")>=4]\n",
    "\n",
    "print(\"rows left:\",df2.shape[0])\n",
    "\n",
    "print(\"nunique cities after freq filt\",df2[[\"city_id\",\"utrip_id\",\"user_id\",\"hotel_country\"]].nunique())\n",
    "print(\"nunique city_id per hotel_country:\")\n",
    "df2.groupby([\"hotel_country\"])[\"city_id\"].nunique().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* drop the first location visited per trip, as that shares the \"first country/hotel/city id feature\" ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "645813\n"
     ]
    }
   ],
   "source": [
    "if DROP_FIRST_ROW: ## drop the first location visited per trip, (row_num = 1) from df data/interactions\n",
    "    df = df.loc[df[\"row_num\"]>1]\n",
    "    print(df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightFM basic user-item CF\n",
    "* just users and country without side features\n",
    " * todo: train/test split by last? \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/lightfm/_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from sklearn import preprocessing\n",
    "from lightfm import LightFM\n",
    "from scipy.sparse import csr_matrix \n",
    "from scipy.sparse import coo_matrix \n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "from lightfm.evaluation import auc_score\n",
    "import seaborn as sns\n",
    "\n",
    "from lightfm.data import Dataset\n",
    "from lightfm import LightFM\n",
    "from lightfm.evaluation import precision_at_k, recall_at_k, auc_score\n",
    "from lightfm.cross_validation import random_train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# df_interactions = pd.crosstab(df[USER_ID_COL],df[TARGET_COL]).stack().reset_index()\n",
    "# ## stack so that target wilk l be count/sum of interactions /weight\n",
    "# df_interactions.rename(columns={0:\"target\"},inplace=True)\n",
    "\n",
    "# print(df_interactions.describe())\n",
    "# print(df_interactions[[USER_ID_COL,TARGET_COL]].nunique())\n",
    "# df_interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num users: 150653, num_items 2530.\n"
     ]
    }
   ],
   "source": [
    "### lightFM dataset\n",
    "## https://stackoverflow.com/questions/51144061/how-to-create-a-sparsematrix-from-dataframe-in-a-specific-format\n",
    "## https://github.com/jamesdhope/recommender/blob/master/recommender.py\n",
    "## https://www.kaggle.com/niyamatalmass/lightfm-hybrid-recommendation-system#Defining-our-necessary-functions  - funcs for prepping features\n",
    "dataset = Dataset(user_identity_features=True, item_identity_features=True)\n",
    "\n",
    "# dataset.fit(df[USER_ID_COL].unique(), df[TARGET_COL].unique())\n",
    "dataset.fit(df[USER_ID_COL], df[TARGET_COL])\n",
    "\n",
    "num_users, num_items = dataset.interactions_shape()\n",
    "print('Num users: {}, num_items {}.'.format(num_users, num_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert num_users == df_interactions[USER_ID_COL].nunique()\n",
    "# assert num_items == df_interactions[TARGET_COL].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_matrix, weights_matrix = dataset.build_interactions([tuple(i) for i in df[[USER_ID_COL,TARGET_COL]].values])\n",
    "# interactions_matrix, weights_matrix = dataset.build_interactions([tuple(i) for i in df_interactions.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, test) = random_train_test_split(interactions=interactions_matrix, test_percentage=0.2,random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<150653x2530 sparse matrix of type '<class 'numpy.int32'>'\n",
      "\twith 645813 stored elements in COOrdinate format>\n"
     ]
    }
   ],
   "source": [
    "print(repr(interactions_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://www.dropbox.com/sh/o4tio3ik9bvx0ve/AACt1zuOSM0KaYXaL_AwugT-a?dl=0&fbclid=IwAR1OIebCTYv2ebi_VI5MIBVgSsQ4JwtyTYAt4jRPDlVHamB9iCBpJF7OBWw&preview=ugly_code.py\n",
    "def create_feature_list(df_,cols):\n",
    "    feature_list = []\n",
    "    for col in cols:\n",
    "        feature_list += list(set(df_[col].to_list()))\n",
    "    return feature_list\n",
    "  \n",
    "\n",
    "def build_features(df_):\n",
    "    return [(row[0],\n",
    "             [row[i] for i in range(1,len(row))]) \n",
    "            for row in df_.itertuples(index=False)]\n",
    "\n",
    "\n",
    "def sample_lightfm_recommendation(model, train,items_labels,user_ids,item_features=None,user_features=None,topk=5):\n",
    "    \"\"\"\n",
    "    Get topk recommendations per user\n",
    "    \"\"\"\n",
    "\n",
    "    #number of users and movies in training data\n",
    "    n_users, n_items = train.shape\n",
    "\n",
    "    #generate recommendations for each user we input\n",
    "    for user_id in user_ids:\n",
    "\n",
    "        #movies they already like\n",
    "        #known_positives = data['item_labels'][data['train'].tocsr()[user_id].indices]\n",
    "        known_positives = items_labels[train.tocsr()[user_id].indices]\n",
    "        \n",
    "        #movies our model predicts they will like\n",
    "        scores = model.predict(user_id, np.arange(n_items),user_features=user_features,item_features=item_features)\n",
    "        #rank them in order of most liked to least\n",
    "        top_items = items_labels[np.argsort(-scores)]\n",
    "\n",
    "        #print out the results\n",
    "        print(\"User %s\" % user_id)\n",
    "        print(\"     Known positives:\")\n",
    "\n",
    "        for x in known_positives[:3]:\n",
    "            print(\"        %s\" % x)\n",
    "\n",
    "        print(\"     Recommended:\")\n",
    "\n",
    "        for x in top_items[:3]:\n",
    "            print(\"        %s\" % x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'checkin', 'checkout', 'city_id', 'device_class',\n",
       "       'affiliate_id', 'booker_country', 'hotel_country', 'utrip_id',\n",
       "       'checkin_week', 'checkin_month', 'checkin_quarter',\n",
       "       'utrip_steps_from_end', 'row_num', 'total_rows', 'last',\n",
       "       'first_hotel_country', 'first_city_id', 'first_device_class',\n",
       "       'first_affiliate_id', 'first_checkin_quarter', 'first_checkin_month',\n",
       "       'first_booker_country', 'ID', 'city_id_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-36-c3dcab40c905>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"user_id\"] = \"us_\" + df[\"user_id\"].astype(str)\n",
      "<ipython-input-36-c3dcab40c905>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"first_affiliate_id\"] = \"a_\" + df[\"first_affiliate_id\"].astype(str)\n",
      "<ipython-input-36-c3dcab40c905>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"first_city_id\"] = \"c_\" + df[\"first_city_id\"].astype(str)\n",
      "<ipython-input-36-c3dcab40c905>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"first_checkin_month\"] = \"m_\" + df[\"first_checkin_month\"].astype(str)\n",
      "<ipython-input-36-c3dcab40c905>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"first_checkin_quarter\"] = \"q_\" + df[\"first_checkin_quarter\"].astype(str)\n"
     ]
    }
   ],
   "source": [
    "df[\"user_id\"] = \"us_\" + df[\"user_id\"].astype(str)\n",
    "\n",
    "df[\"first_affiliate_id\"] = \"a_\" + df[\"first_affiliate_id\"].astype(str)\n",
    "df[\"first_city_id\"] = \"c_\" + df[\"first_city_id\"].astype(str)\n",
    "df[\"first_checkin_month\"] = \"m_\" + df[\"first_checkin_month\"].astype(str)\n",
    "df[\"first_checkin_quarter\"] = \"q_\" + df[\"first_checkin_quarter\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utrip_id                150653\n",
      "first_hotel_country        118\n",
      "first_booker_country         5\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utrip_id</th>\n",
       "      <th>first_hotel_country</th>\n",
       "      <th>first_booker_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52885</th>\n",
       "      <td>110418_1</td>\n",
       "      <td>Glubbdubdrib</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068509</th>\n",
       "      <td>221863_1</td>\n",
       "      <td>Vadeem</td>\n",
       "      <td>Tcherkistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400623</th>\n",
       "      <td>403238_1</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>Elbonia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761855</th>\n",
       "      <td>414381_1</td>\n",
       "      <td>Nova Africa</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002561</th>\n",
       "      <td>870848_1</td>\n",
       "      <td>Bartovia</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268836</th>\n",
       "      <td>6140090_2</td>\n",
       "      <td>Fook Island</td>\n",
       "      <td>Elbonia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572120</th>\n",
       "      <td>181301_1</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Gondal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877422</th>\n",
       "      <td>3209267_5</td>\n",
       "      <td>Kasnia</td>\n",
       "      <td>Elbonia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378261</th>\n",
       "      <td>4682726_1</td>\n",
       "      <td>Samavia</td>\n",
       "      <td>Gondal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232500</th>\n",
       "      <td>5274588_3</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Gondal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150653 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          utrip_id first_hotel_country  first_booker_country\n",
       "52885     110418_1        Glubbdubdrib  The Devilfire Empire\n",
       "1068509   221863_1              Vadeem           Tcherkistan\n",
       "400623    403238_1        Cobra Island               Elbonia\n",
       "761855    414381_1         Nova Africa  The Devilfire Empire\n",
       "1002561   870848_1            Bartovia  The Devilfire Empire\n",
       "...            ...                 ...                   ...\n",
       "268836   6140090_2         Fook Island               Elbonia\n",
       "572120    181301_1              Gondal                Gondal\n",
       "877422   3209267_5              Kasnia               Elbonia\n",
       "378261   4682726_1             Samavia                Gondal\n",
       "232500   5274588_3              Gondal                Gondal\n",
       "\n",
       "[150653 rows x 3 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_feats = [ \n",
    "    'first_hotel_country', \n",
    "              \"first_booker_country\",\n",
    "#        'first_city_id', \n",
    "#               'first_device_class', \n",
    "#               'first_affiliate_id', \n",
    "#        'first_checkin_quarter', \n",
    "#             'first_checkin_month',\n",
    "#               \"user_id\"\n",
    "             ]\n",
    "\n",
    "## ALT: \n",
    "# user_feats = [\"ID\"]\n",
    "\n",
    "users = df[['utrip_id'] +  user_feats].drop_duplicates([\"utrip_id\"])\n",
    "print(users.nunique())\n",
    "users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city_id          2530\n",
      "hotel_country      96\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_id</th>\n",
       "      <th>hotel_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52885</th>\n",
       "      <td>3763</td>\n",
       "      <td>Glubbdubdrib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068509</th>\n",
       "      <td>36063</td>\n",
       "      <td>Gondal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400623</th>\n",
       "      <td>40871</td>\n",
       "      <td>Patusan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761855</th>\n",
       "      <td>2813</td>\n",
       "      <td>Nova Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002561</th>\n",
       "      <td>51128</td>\n",
       "      <td>Bartovia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205125</th>\n",
       "      <td>14117</td>\n",
       "      <td>Leutonia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742138</th>\n",
       "      <td>29672</td>\n",
       "      <td>Franchia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760325</th>\n",
       "      <td>7458</td>\n",
       "      <td>Novistrana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755388</th>\n",
       "      <td>43540</td>\n",
       "      <td>Bahavia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644368</th>\n",
       "      <td>28997</td>\n",
       "      <td>Osterlich</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2530 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         city_id hotel_country\n",
       "52885       3763  Glubbdubdrib\n",
       "1068509    36063        Gondal\n",
       "400623     40871       Patusan\n",
       "761855      2813   Nova Africa\n",
       "1002561    51128      Bartovia\n",
       "...          ...           ...\n",
       "205125     14117      Leutonia\n",
       "742138     29672      Franchia\n",
       "760325      7458    Novistrana\n",
       "755388     43540       Bahavia\n",
       "644368     28997     Osterlich\n",
       "\n",
       "[2530 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_item_features = df[[\"city_id\",\"hotel_country\"]].drop_duplicates()\n",
    "print(df_item_features.nunique())\n",
    "df_item_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_list = create_feature_list(items,cols=['section_primary','writer_name'])\n",
    "# user_features_list = create_feature_list(users,cols=['popular_section','popular_platform','popular_sources'])\n",
    "\n",
    "features_list = create_feature_list(df_item_features,cols=[\"hotel_country\"])\n",
    "user_features_list = create_feature_list(users,cols=user_feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TARGET_COL !=  'hotel_country':  ## no item level features for country\n",
    "    dataset.fit_partial(items=df_item_features.city_id,\n",
    "                   item_features=features_list,\n",
    "                     users=users[\"utrip_id\"],\n",
    "                    user_features=user_features_list, # ORIG\n",
    "#                         user_features = user_features\n",
    "                   )    \n",
    "else: \n",
    "    dataset.fit_partial(users=users[\"utrip_id\"],\n",
    "                        user_features=user_features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features = dataset.build_user_features(build_features(users), normalize=False)\n",
    "\n",
    "if TARGET_COL !=  'hotel_country':\n",
    "    item_features = dataset.build_item_features(build_features(df_item_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(user_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightFM makes certain subtle assumptions when you do or do not pass side information. \n",
    "When no user_features or item_features are explicitly included, then LightFM assumes that both feature matrices are in fact identity matrices of size (num_users X num_users) or (num_items X num_items) for user and item feature matrices, respectively. What this is effectively doing is one-hot-encoding each user and item ID as a single feature vector.\n",
    "In the case where you do pass an item_features matrix, then LightFM does not do any one-hot-encoding. Thus, each user and item ID does not get its own vector unless you explicitly define one.\n",
    "The easiest way to do this is to make your own identity matrix and stack it on the side of the item_features matrix that we already created. This way, each item is described by a single vector for its unique ID and then a set of vectors for each of its tags.\n",
    "\n",
    "*  https://www.ethanrosenthal.com/2016/11/07/implicit-mf-part-2/\n",
    "\n",
    "* **NOTE** - Apparently not needed anymore, `dataset` function creates latent vectors by default ? \n",
    "    * https://making.lyst.com/lightfm/docs/lightfm.data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### add item level features ohe - https://www.ethanrosenthal.com/2016/11/07/implicit-mf-part-2/\n",
    "# import scipy as sp\n",
    "# # Need to hstack user_features\n",
    "\n",
    "\n",
    "# eye = sp.sparse.eye(user_features.shape[0], user_features.shape[0]).tocsr() ## 212 GB if not sparse\n",
    "# user_features_concat = sp.sparse.hstack((eye, user_features))\n",
    "# print(type(user_features_concat))\n",
    "# user_features_concat = user_features_concat.tocsr().astype(np.float32)\n",
    "# print(type(user_features_concat))\n",
    "\n",
    "# ## new : \n",
    "# # eye = sp.sparse.eye(user_features.shape[0], user_features.shape[0]).tocsr()\n",
    "# # user_features = sp.hstack((eye, user_features))\n",
    "# # user_features = user_features.tocsr().astype(np.float32)\n",
    "\n",
    "# # user_features_concat = sp.sparse.csr_matrix(user_features_concat)\n",
    "# # print(type(user_features_concat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if TARGET_COL ==  'hotel_country':  ## no item level features for country ? \n",
    "#     dataset.fit_partial(users=users[\"utrip_id\"],\n",
    "#     #                     user_features=user_features_concat, ## error : TypeError: unhashable type: 'csr_matrix'\n",
    "#                         user_features=user_features_list,\n",
    "#                        )\n",
    "# else: \n",
    "#         dataset.fit_partial(items=df_item_features.city_id,\n",
    "#                        item_features=features_list,\n",
    "#                          users=users[\"utrip_id\"],\n",
    "#     #                     user_features=user_features_concat, ## error : TypeError: unhashable type: 'csr_matrix'\n",
    "#                         user_features=user_features_list,\n",
    "#                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num users: 150653, num_items 2530.\n"
     ]
    }
   ],
   "source": [
    "num_users, num_items = dataset.interactions_shape()\n",
    "print('Num users: {}, num_items {}.'.format(num_users, num_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "### - modifies and uses user/item features https://gist.github.com/kayibal/16340660d1d85b9ea1872a5d9be0f383\n",
    "# ## error: \n",
    "# item_features = dataset.build_item_features([tuple(i) for i in df_item_features.values])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 10/10 [00:11<00:00,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.3 s, sys: 54.2 ms, total: 11.3 s\n",
      "Wall time: 11.3 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightfm.lightfm.LightFM at 0x7f92145475e0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = LightFM(no_components=16,loss='warp',n=20,max_sampled=30, item_alpha = 1.6e-08, user_alpha = 1.8e-09)\n",
    "\n",
    "# (train, test) = random_train_test_split(interactions=interactions_matrix, test_percentage=0.2)\n",
    "\n",
    "model.fit(train,\n",
    "#           item_features=item_features, user_features=user_features, # ORIG\n",
    "          item_features=item_features, user_features=user_features, ## are these the right inputs ??? \n",
    "          epochs=10,\n",
    "          verbose=True,num_threads=14   \n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall @4: train 0.38, test 0.37.\n",
      "Recall @90: train 0.88, test 0.90.\n",
      "Precision @4: train 0.31, test 0.13.\n",
      "AUC: train 0.99, test 0.99.\n",
      "CPU times: user 1min 38s, sys: 98.4 ms, total: 1min 38s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_precision_4 = precision_at_k(model, train,k=4,num_threads=7,item_features=item_features,user_features=user_features).mean()\n",
    "test_precision_4 = precision_at_k(model, test,k=4,num_threads=7,item_features=item_features,user_features=user_features).mean()\n",
    "\n",
    "train_recall_4 = recall_at_k(model, train,k=4,num_threads=7,item_features=item_features,user_features=user_features).mean()\n",
    "test_recall_4 = recall_at_k(model, test,k=4,num_threads=7,item_features=item_features,user_features=user_features).mean()\n",
    "print('Recall @4: train %.2f, test %.2f.' % (train_recall_4, test_recall_4))\n",
    "\n",
    "train_recall_90 = recall_at_k(model, train,k=90,num_threads=7,item_features=item_features,user_features=user_features).mean()\n",
    "test_recall_90 = recall_at_k(model, test,k=90,num_threads=7,item_features=item_features,user_features=user_features).mean()\n",
    "print('Recall @90: train %.2f, test %.2f.' % (train_recall_90, test_recall_90))\n",
    "\n",
    "train_auc = auc_score(model, train,num_threads=7,\n",
    "                     item_features=item_features,user_features=user_features).mean()\n",
    "test_auc = auc_score(model, test,num_threads=7,\n",
    "                     item_features=item_features,user_features=user_features\n",
    "                    ).mean()\n",
    "\n",
    "# print('Precision @10: train %.2f, test %.2f.' % (train_precision_10, test_precision_10))\n",
    "print('Precision @4: train %.2f, test %.2f.' % (train_precision_4, test_precision_4))\n",
    "# print('Recall @30: test %.2f.' % ( recall_at_k(model, test,k=30,num_threads=8,item_features=item_features,user_features=user_features).mean()))\n",
    "\n",
    "print('AUC: train %.2f, test %.2f.' % (train_auc, test_auc))\n",
    "\n",
    "## 20 epochs, 64 components, min 20 target freq:\n",
    "# Precision @4: train 0.35, test 0.14.\n",
    "# Recall @4: train 0.35, test 0.37.\n",
    "# Recall @90: train 0.83, test 0.87.\n",
    "##  Recall @30:  test \n",
    "# AUC: train 0.99, test 0.99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_recall_4 = recall_at_k(model, train,train,k=4,num_threads=7).mean()\n",
    "# test_recall_4 = recall_at_k(model, test,train,k=4,num_threads=7).mean()\n",
    "# print('Recall @4: train %.2f, test %.2f.' % (train_recall_4, test_recall_4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### just item level features  (for country level - identical to just pure CF)\n",
    "# model_item = LightFM(no_components=16,loss='warp',n=20,max_sampled=30, item_alpha = 1.6e-08, user_alpha = 1.8e-09)\n",
    "# model_item.fit(train, epochs=10,item_features=item_features,verbose=True,num_threads=8)\n",
    "\n",
    "# train_precision_4 = precision_at_k(model_item, train,k=4,num_threads=7,item_features=item_features).mean()\n",
    "# test_precision_4 = precision_at_k(model_item, test,k=4,num_threads=7,item_features=item_features).mean()\n",
    "\n",
    "# train_recall_4 = recall_at_k(model_item, train,k=4,num_threads=7,item_features=item_features).mean()\n",
    "# test_recall_4 = recall_at_k(model_item, test,k=4,num_threads=7,item_features=item_features).mean()\n",
    "\n",
    "# train_recall_90 = recall_at_k(model_item, train,k=90,num_threads=7,item_features=item_features).mean()\n",
    "# test_recall_90 = recall_at_k(model_item, test,k=90,num_threads=7,item_features=item_features).mean()\n",
    "# print('Recall @90: train %.2f, test %.2f.' % (train_recall_90, test_recall_90))\n",
    "\n",
    "# train_auc = auc_score(model_item, train,num_threads=7,\n",
    "#                      item_features=item_features).mean()\n",
    "# test_auc = auc_score(model_item, test,num_threads=7,\n",
    "#                      item_features=item_features\n",
    "#                     ).mean()\n",
    "\n",
    "# # print('Precision @10: train %.2f, test %.2f.' % (train_precision_10, test_precision_10))\n",
    "# print('Precision @4: train %.2f, test %.2f.' % (train_precision_4, test_precision_4))\n",
    "# print('Recall @4: train %.2f, test %.2f.' % (train_recall_4, test_recall_4))\n",
    "# print('Recall @30: test %.2f.' % ( recall_at_k(model_item, test,k=30,num_threads=8,item_features=item_features).mean()))\n",
    "\n",
    "# print('AUC: train %.2f, test %.2f.' % (train_auc, test_auc))\n",
    "\n",
    "# ## just item feats , 20 epochs, 64 components, min 20 target freq:\n",
    "# # Precision @4: train 0.45, test 0.15.\n",
    "# # Recall @4: train 0.46, test 0.40.\n",
    "\n",
    "# # Recall @90: train 0.89, test 0.90.\n",
    "# # AUC: train 1.00, test 0.99.\n",
    "\n",
    "# ##  Recall @30: test 0.783 (not sure if with less components ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision @4: train 0.38, test 0.14.\n",
      "Recall @4: train 0.48, test 0.42.\n",
      "Recall @90: train 0.91, test 0.92.\n",
      "AUC: train 1.00, test 0.99.\n"
     ]
    }
   ],
   "source": [
    "# ## model_base performnce evaluation - without user/item features - does worse than + item level\n",
    "model_base = LightFM(no_components=16,loss='warp',n=20,max_sampled=30, item_alpha = 1.6e-08, user_alpha = 1.8e-09)\n",
    "model_base.fit(train, epochs=10,num_threads=14)\n",
    "\n",
    "train_precision_4 = precision_at_k(model_base, train,k=4,num_threads=8).mean()\n",
    "test_precision_4 = precision_at_k(model_base, test,k=4,num_threads=8).mean()\n",
    "print('Precision @4: train %.2f, test %.2f.' % (train_precision_4, test_precision_4))\n",
    "\n",
    "train_recall_4 = recall_at_k(model_base, train,k=4,num_threads=8).mean()\n",
    "test_recall_4 = recall_at_k(model_base, test,k=4,num_threads=8).mean()\n",
    "print('Recall @4: train %.2f, test %.2f.' % (train_recall_4, test_recall_4))\n",
    "\n",
    "train_recall_90 = recall_at_k(model_base, train,k=90,num_threads=7).mean()\n",
    "test_recall_90 = recall_at_k(model_base, test,k=90,num_threads=7).mean()\n",
    "print('Recall @90: train %.2f, test %.2f.' % (train_recall_90, test_recall_90))\n",
    "\n",
    "train_auc = auc_score(model_base, train,num_threads=12).mean()\n",
    "test_auc = auc_score(model_base, test,num_threads=12,).mean()\n",
    "print('AUC: train %.2f, test %.2f.' % (train_auc, test_auc))\n",
    "\n",
    "# # Precision @4: train 0.24, test 0.09.\n",
    "# # Recall @4: train 0.23, test 0.24.\n",
    "# # Recall @90: train 0.62, test 0.65.\n",
    "# # AUC: train 0.93, test 0.92."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 10/10 [00:08<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision @4: train 0.34, test 0.11.\n",
      "Recall @4: train 0.42, test 0.33.\n",
      "Recall @90: train 0.85, test 0.81.\n",
      "AUC: train 0.99, test 0.97.\n",
      "CPU times: user 1min 25s, sys: 104 ms, total: 1min 25s\n",
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### just user level side-features\n",
    "model_user = LightFM(no_components=16,loss='warp',n=20,max_sampled=30, item_alpha = 1.6e-08, user_alpha = 1.8e-09)\n",
    "model_user.fit(train, epochs=10,user_features=user_features,verbose=True,num_threads=8)\n",
    "\n",
    "train_precision_4 = precision_at_k(model_user, train,k=4,num_threads=14,user_features=user_features).mean()\n",
    "test_precision_4 = precision_at_k(model_user, test,k=4,num_threads=14,user_features=user_features).mean()\n",
    "print('Precision @4: train %.2f, test %.2f.' % (train_precision_4, test_precision_4))\n",
    "train_recall_4 = recall_at_k(model_user, train,k=4,num_threads=14,user_features=user_features).mean()\n",
    "test_recall_4 = recall_at_k(model_user, test,k=4,num_threads=14,user_features=user_features).mean()\n",
    "print('Recall @4: train %.2f, test %.2f.' % (train_recall_4, test_recall_4))\n",
    "\n",
    "train_recall_90 = recall_at_k(model_user, train,k=90,num_threads=14,user_features=user_features).mean()\n",
    "test_recall_90 = recall_at_k(model_user, test,k=90,num_threads=14,user_features=user_features).mean()\n",
    "print('Recall @90: train %.2f, test %.2f.' % (train_recall_90, test_recall_90))\n",
    "\n",
    "train_auc = auc_score(model_user, train,num_threads=14,\n",
    "                     user_features=user_features).mean()\n",
    "test_auc = auc_score(model_user, test,num_threads=14,\n",
    "                     user_features=user_features\n",
    "                    ).mean()\n",
    "print('AUC: train %.2f, test %.2f.' % (train_auc, test_auc))\n",
    "# print('Precision @10: train %.2f, test %.2f.' % (train_precision_10, test_precision_10))\n",
    "\n",
    "# print('Recall @30: test %.2f.' % ( recall_at_k(model_user, test,k=30,num_threads=8,user_features=user_features).mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* hyperparam search\n",
    "    *  https://stackoverflow.com/questions/49896816/how-do-i-optimize-the-hyperparameters-of-lightfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "### https://stackoverflow.com/questions/49896816/how-do-i-optimize-the-hyperparameters-of-lightfm\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "from lightfm import LightFM\n",
    "from lightfm.evaluation import auc_score\n",
    "\n",
    "\n",
    "def sample_hyperparameters():\n",
    "    \"\"\"\n",
    "    Yield possible hyperparameter choices.\n",
    "    \"\"\"\n",
    "\n",
    "    while True:\n",
    "        yield {\n",
    "            \"no_components\": np.random.randint(8, 64),\n",
    "            \"learning_schedule\": np.random.choice([\"adagrad\", \"adadelta\"]),\n",
    "            \"loss\": np.random.choice([\"bpr\", \"warp\", \"warp-kos\"]),\n",
    "            \"learning_rate\": np.random.exponential(0.05),\n",
    "            \"item_alpha\": np.random.exponential(1e-8),\n",
    "            \"user_alpha\": np.random.exponential(1e-8),\n",
    "#             \"max_sampled\": np.random.randint(5, 40),\n",
    "#             \"num_epochs\": np.random.randint(1, 8),\n",
    "            \"num_epochs\":6\n",
    "        }\n",
    "\n",
    "def random_search(train, test, num_samples=10, num_threads=8):\n",
    "    \"\"\"\n",
    "    Sample random hyperparameters, fit a LightFM model, and evaluate it\n",
    "    on the test set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    train: np.float32 coo_matrix of shape [n_users, n_items]\n",
    "        Training data.\n",
    "    test: np.float32 coo_matrix of shape [n_users, n_items]\n",
    "        Test data.\n",
    "    num_samples: int, optional\n",
    "        Number of hyperparameter choices to evaluate.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    generator of (auc_score, hyperparameter dict, fitted model)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    for hyperparams in itertools.islice(sample_hyperparameters(), num_samples):\n",
    "        num_epochs = hyperparams.pop(\"num_epochs\")\n",
    "        model = LightFM(**hyperparams) # ,learning_rate=.03\n",
    "        model.fit(train, epochs=num_epochs, num_threads=num_threads,\n",
    "                 item_features=item_features, user_features=user_features)\n",
    "        ### should i pass in train_interactions (when i have repeats) ? \n",
    "#         score = auc_score(model, test, train_interactions=train, num_threads=num_threads).mean() # ORIG\n",
    "#         score = precision_at_k(model, test,  num_threads=num_threads,k=4).mean()\n",
    "        score = recall_at_k(model, test,  num_threads=num_threads,k=30\n",
    "                           ,item_features=item_features,user_features=user_features).mean()\n",
    "\n",
    "        hyperparams[\"num_epochs\"] = num_epochs\n",
    "\n",
    "        yield (score, hyperparams, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# (score, hyperparams, model) = max(random_search(train, test, num_threads=8, num_samples=25), key=lambda x: x[0])\n",
    "# print(\"Best score {} at {}\".format(score, hyperparams))\n",
    "\n",
    "### Best score - Recall @30: 0.747 at {'no_components': 37, 'learning_schedule': 'adagrad', 'loss': 'warp-kos', 'learning_rate': 0.192, 'item_alpha': 1.669e-08, 'user_alpha': 1.88e-09, 'num_epochs': 6}\n",
    "\n",
    "### raw model search (no side-feats): (P@4): Best score 0.1326 at {'no_components': 60, 'learning_schedule': 'adagrad', 'loss': 'warp', 'learning_rate': 0.029032455085289472, 'num_epochs': 6}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add lag features + Train/test/data split\n",
    "* Lag feats (remember for categorical)\n",
    "* Drop leak features (target values - country, city)\n",
    "\n",
    "* drop instances  that lack history (e.g. at least 3d step and onwards) - by dropna in lag feat\n",
    "* fill nans\n",
    "* Split train/test by `user id` / split could maybe be by `utrip ID` ? ? \n",
    "    * Test - only last trip\n",
    "    *  stratified train/test split by class - then drop any train rows with overlap with tests' IDs.  \n",
    "        * Could also stratify by users, but risks some classes being non present in test\n",
    "        \n",
    "###### Big possible improvement to lag features: Have \"first location\" (starting point) \"lag\" feature\n",
    "* `groupbyFirstLagFeatures(df,group=\"user_id\",lag_feature_cols=[\"hotel_country\",\"city_id\"])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Artifical train/test split grouping \n",
    "\n",
    "* Make additional grouping for train/test split.\n",
    "WARNING: This is NOT as good/reliable as splitting by USER_ID or utripID, but we do this for now, since we don't have that variable here\n",
    "* This splitting would result in some variables not being learned ? \n",
    "\n",
    "* Ignore some of the ID columns for this groups creation\n",
    "    * warning - manua/fragile/may break if columns used change\n",
    "    \n",
    "This splitting is **not reliable**!! It is just to try to approximate something like splitting across trips\n",
    "\n",
    "* Mainly - groups should include rows with last 0/1 for same trip\n",
    "    * How bad is it that we group/exclude by city_id (for a specific grouping)  ?? \n",
    "    * Alt - could do group also with lag1_city_id ? (If using that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_feat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-a35ac5f87db4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_feat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_feat' is not defined"
     ]
    }
   ],
   "source": [
    "df_feat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat[\"Group_id\"] = df_feat[[ 'device_class', 'affiliate_id', 'booker_country',\n",
    "                                f\"{TARGET_COL}\", #\"hotel_country\",  # 'city_id',\n",
    "                               'checkin_quarter']].astype(str).sum(1)\n",
    "print(df_feat[\"Group_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inds, test_inds = next(GroupShuffleSplit(test_size=.25, n_splits=2, random_state = 7).split(df_feat, groups=df_feat['Group_id']))\n",
    "\n",
    "train = df_feat.iloc[train_inds].drop(\"Group_id\",axis=1)\n",
    "test = df_feat.iloc[test_inds]\n",
    "\n",
    "## split test into validation and final test (only last stage of trip). We could also spliut by group here as well\n",
    "\n",
    "valid_inds, test_inds  = next(GroupShuffleSplit(test_size=.6, n_splits=2, random_state = 7).split(test, groups=test['Group_id']))\n",
    "valid = test.iloc[valid_inds].drop(\"Group_id\",axis=1)\n",
    "test = test.iloc[test_inds].drop(\"Group_id\",axis=1)\n",
    "test = test.loc[test[\"last\"]==1]\n",
    "\n",
    "print(\"train\",train.shape[0],\"valid\",valid.shape[0],\"test\",test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### filter for most frequent targets\n",
    "\n",
    "if KEEP_TOP_K_TARGETS > 0 :\n",
    "    print(df_feat.shape[0])\n",
    "    df_feat = df_feat.loc[df_feat[\"city_id\"].isin(TOP_TARGETS)]\n",
    "    print(df_feat.shape[0])    \n",
    "    assert df_feat[\"city_id\"].nunique() == KEEP_TOP_K_TARGETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########################\n",
    "# ## stratified train/test split by class - then drop any train rows with overlap wit htest IDs.  Could also stratify by users, but risks some classes being non present in test\n",
    "# ### split could maybe be by utrip ID ? \n",
    "# ### orig - split by group : \n",
    "\n",
    "# # train_inds, test_inds = next(GroupShuffleSplit(test_size=.2, n_splits=2, random_state = 7).split(df_feat, groups=df_feat['user_id']))\n",
    "# # X_train = df_feat.iloc[train_inds].drop(DROP_FEATS,axis=1,errors=\"ignore\")\n",
    "# # X_test = df_feat.iloc[test_inds].drop(DROP_FEATS,axis=1,errors=\"ignore\")\n",
    "# # assert (set(X_train[TARGET_COL].unique()) == set(X_test[TARGET_COL].unique()))\n",
    "# #################\n",
    "# ## alt: split by class. May be leaky! \n",
    "# X_train, X_test = train_test_split(df_feat,stratify=df_feat[TARGET_COL])\n",
    "\n",
    "# ########################\n",
    "# print(\"X_train\",X_train.shape[0])\n",
    "# ## get last row in trip only in test/eval set: \n",
    "# print(\"X_test\",X_test.shape[0])\n",
    "\n",
    "# ### following is for splitting by group - can't do so currebntly as group/user id col is missing\n",
    "# # X_test = X_test.loc[X_test[\"utrip_steps_from_end\"]==1] # last row per trip\n",
    "# # print(\"X_test after filtering for last instance per trip\",X_test.shape[0])\n",
    "\n",
    "# # y_train = X_train.pop(TARGET_COL)\n",
    "# # y_test = X_test.pop(TARGET_COL)\n",
    "\n",
    "# # print(\"# classes\",y_train.nunique())\n",
    "\n",
    "# # # ## check that same classes in train and test - \n",
    "# # # assert (set(y_train.unique()) == set(y_test.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
