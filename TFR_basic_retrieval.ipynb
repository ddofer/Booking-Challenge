{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X80i_girFR2o"
   },
   "source": [
    "##### Copyright 2020 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2020-11-19T23:54:09.961294Z",
     "iopub.status.busy": "2020-11-19T23:54:09.960622Z",
     "iopub.status.idle": "2020-11-19T23:54:09.963074Z",
     "shell.execute_reply": "2020-11-19T23:54:09.962474Z"
    },
    "id": "bB8gHCR3FVC0"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "\n",
    "from typing import Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "\n",
    "\n",
    "# from catboost import CatBoostClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "import numpy as np\n",
    "from tensorflow.keras.metrics import TopKCategoricalAccuracy, Precision, SparseTopKCategoricalAccuracy # @4\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 90)\n",
    "\n",
    "## https://www.tensorflow.org/tutorials/load_data/pandas_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEEP_COLS = [ 'device_class','affiliate_id', 'booker_country',\n",
    "             'hotel_country','city_id',\n",
    "             \"user_id\", # use for train/test split ? \n",
    "       'checkin_month', 'utrip_steps_from_end']\n",
    "\n",
    "MIN_TARGET_FREQ = 50 # drop target/city_id values that appear less than this many times, as final step's target \n",
    "KEEP_TOP_K_TARGETS = 200 # keep K most frequent city ID targets (redundnat with the above, )\n",
    "\n",
    "## (some) categorical variables that appear less than this many times will be replaced with a placeholder value!\n",
    "## Includes CITY id (but done after target filtering, to avoid creating a \"rare class\" target:\n",
    "LOW_COUNT_THRESH = 30 \n",
    "\n",
    "# RUN_TABNET = True\n",
    "max_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    123456.000000\n",
      "mean        178.741689\n",
      "std         270.470407\n",
      "min           1.000000\n",
      "25%          10.000000\n",
      "50%          56.000000\n",
      "75%         205.000000\n",
      "max        1236.000000\n",
      "Name: city_id_count, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>device_class</th>\n",
       "      <th>affiliate_id</th>\n",
       "      <th>booker_country</th>\n",
       "      <th>hotel_country</th>\n",
       "      <th>city_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>checkin_month</th>\n",
       "      <th>utrip_steps_from_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117277</th>\n",
       "      <td>desktop</td>\n",
       "      <td>9924</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Osterlich</td>\n",
       "      <td>52933</td>\n",
       "      <td>136</td>\n",
       "      <td>9</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117278</th>\n",
       "      <td>desktop</td>\n",
       "      <td>9924</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Osterlich</td>\n",
       "      <td>51685</td>\n",
       "      <td>136</td>\n",
       "      <td>9</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117281</th>\n",
       "      <td>desktop</td>\n",
       "      <td>9924</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Osterlich</td>\n",
       "      <td>46411</td>\n",
       "      <td>136</td>\n",
       "      <td>9</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117283</th>\n",
       "      <td>desktop</td>\n",
       "      <td>9924</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Osterlich</td>\n",
       "      <td>41113</td>\n",
       "      <td>136</td>\n",
       "      <td>9</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117410</th>\n",
       "      <td>desktop</td>\n",
       "      <td>3417</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Kangan</td>\n",
       "      <td>10485</td>\n",
       "      <td>149</td>\n",
       "      <td>2</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98852</th>\n",
       "      <td>desktop</td>\n",
       "      <td>9924</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>13931</td>\n",
       "      <td>6257762</td>\n",
       "      <td>10</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98853</th>\n",
       "      <td>desktop</td>\n",
       "      <td>9924</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>23921</td>\n",
       "      <td>6257762</td>\n",
       "      <td>10</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98854</th>\n",
       "      <td>tablet</td>\n",
       "      <td>3631</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>58413</td>\n",
       "      <td>6257762</td>\n",
       "      <td>10</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98855</th>\n",
       "      <td>desktop</td>\n",
       "      <td>9924</td>\n",
       "      <td>The Devilfire Empire</td>\n",
       "      <td>Cobra Island</td>\n",
       "      <td>29319</td>\n",
       "      <td>6257762</td>\n",
       "      <td>10</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64260</th>\n",
       "      <td>tablet</td>\n",
       "      <td>5755</td>\n",
       "      <td>Gondal</td>\n",
       "      <td>Rolisica</td>\n",
       "      <td>15470</td>\n",
       "      <td>6257973</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75471 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       device_class  affiliate_id        booker_country hotel_country  \\\n",
       "117277      desktop          9924  The Devilfire Empire     Osterlich   \n",
       "117278      desktop          9924  The Devilfire Empire     Osterlich   \n",
       "117281      desktop          9924  The Devilfire Empire     Osterlich   \n",
       "117283      desktop          9924  The Devilfire Empire     Osterlich   \n",
       "117410      desktop          3417  The Devilfire Empire        Kangan   \n",
       "...             ...           ...                   ...           ...   \n",
       "98852       desktop          9924  The Devilfire Empire  Cobra Island   \n",
       "98853       desktop          9924  The Devilfire Empire  Cobra Island   \n",
       "98854        tablet          3631  The Devilfire Empire  Cobra Island   \n",
       "98855       desktop          9924  The Devilfire Empire  Cobra Island   \n",
       "64260        tablet          5755                Gondal      Rolisica   \n",
       "\n",
       "        city_id  user_id  checkin_month  utrip_steps_from_end  \n",
       "117277    52933      136              9              0.142857  \n",
       "117278    51685      136              9              0.285714  \n",
       "117281    46411      136              9              0.714286  \n",
       "117283    41113      136              9              1.000000  \n",
       "117410    10485      149              2              0.250000  \n",
       "...         ...      ...            ...                   ...  \n",
       "98852     13931  6257762             10              0.600000  \n",
       "98853     23921  6257762             10              0.700000  \n",
       "98854     58413  6257762             10              0.800000  \n",
       "98855     29319  6257762             10              0.900000  \n",
       "64260     15470  6257973              8              1.000000  \n",
       "\n",
       "[75471 rows x 8 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"booking_train_set.csv\",\n",
    "                 nrows=123456,\n",
    "                 parse_dates=[\"checkin\"],infer_datetime_format=True, # ,\"checkout\"\n",
    "                ).drop([\"Unnamed: 0\"],axis=1).drop_duplicates()\n",
    "\n",
    "df.sort_values([\"user_id\",\"checkin\",\"checkout\"],inplace=True)\n",
    "\n",
    "df[\"checkin_month\"] = df[\"checkin\"].dt.month\n",
    "\n",
    "df[\"utrip_steps_from_end\"] = df.groupby(\"utrip_id\")[\"checkin\"].rank(ascending=True,pct=True)\n",
    "\n",
    "## filter out rare cities/targets\n",
    "freq = df[\"city_id\"].value_counts()\n",
    "df[\"city_id_count\"] = df[\"city_id\"].map(freq)\n",
    "print(df[\"city_id_count\"].describe())\n",
    "df = df.loc[df[\"city_id_count\"]>=LOW_COUNT_THRESH]\n",
    "df = df[KEEP_COLS]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['device_class', 'affiliate_id', 'booker_country', 'hotel_country',\n",
       "       'city_id', 'user_id', 'checkin_month', 'utrip_steps_from_end'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keep subset of df and rename it as movies, user, ratings and as tf.dataset, for easy use of tfr tutorial\n",
    "\n",
    "* https://www.tensorflow.org/tutorials/load_data/pandas_dataframe\n",
    "\n",
    "\n",
    "    target = df.pop('target')\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((df.values, target.values))\n",
    "    \n",
    "    \n",
    "    train_dataset = dataset.shuffle(len(df)).batch(1)\n",
    "    \n",
    "    \n",
    "* https://medium.com/when-i-work-data/converting-a-pandas-dataframe-into-a-tensorflow-dataset-752f3783c168\n",
    "\n",
    "* \"ratings\" <> city_id - our target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z17OmgavQfp4"
   },
   "source": [
    "# Recommending movies: retrieval\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/recommenders/examples/basic_retrieval\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/recommenders/blob/main/docs/examples/basic_retrieval.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/recommenders/blob/main/docs/examples/basic_retrieval.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/recommenders/docs/examples/basic_retrieval.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCeYA79m1DEX"
   },
   "source": [
    "Real-world recommender systems are often composed of two stages:\n",
    "\n",
    "1. The retrieval stage is responsible for selecting an initial set of hundreds of candidates from all possible candidates. The main objective of this model is to efficiently weed out all candidates that the user is not interested in. Because the retrieval model may be dealing with millions of candidates, it has to be computationally efficient.\n",
    "2. The ranking stage takes the outputs of the retrieval model and fine-tunes them to select the best possible handful of recommendations. Its task is to narrow down the set of items the user may be interested in to a shortlist of likely candidates.\n",
    "\n",
    "In this tutorial, we're going to focus on the first stage, retrieval. If you are interested in the ranking stage, have a look at our [ranking](basic_ranking) tutorial.\n",
    "\n",
    "Retrieval models are often composed of two sub-models:\n",
    "\n",
    "1. A query model computing the query representation (normally a fixed-dimensionality embedding vector) using query features.\n",
    "2. A candidate model computing the candidate representation (an equally-sized vector) using the candidate features\n",
    "\n",
    "The outputs of the two models are then multiplied together to give a query-candidate affinity score, with higher scores expressing a better match between the candidate and the query.\n",
    "\n",
    "In this tutorial, we're going to build and train such a two-tower model using the Movielens dataset.\n",
    "\n",
    "We're going to:\n",
    "\n",
    "1. Get our data and split it into a training and test set.\n",
    "2. Implement a retrieval model.\n",
    "3. Fit and evaluate it.\n",
    "4. Export it for efficient serving by building an approximate nearest neighbours (ANN) index.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7QZ3kkMQo48"
   },
   "source": [
    "## The dataset\n",
    "\n",
    "The Movielens dataset is a classic dataset from the [GroupLens](https://grouplens.org/datasets/movielens/) research group at the University of Minnesota. It contains a set of ratings given to movies by a set of users, and is a workhorse of recommender system research.\n",
    "\n",
    "The data can be treated in two ways:\n",
    "\n",
    "1. It can be interpreted as expressesing which movies the users watched (and rated), and which they did not. This is a form of implicit feedback, where users' watches tell us which things they prefer to see and which they'd rather not see.\n",
    "2. It can also be seen as expressesing how much the users liked the movies they did watch. This is a form of explicit feedback: given that a user watched a movie, we can tell roughly how much they liked by looking at the rating they have given.\n",
    "\n",
    "In this tutorial, we are focusing on a retrieval system: a model that predicts a set of movies from the catalogue that the user is likely to watch. Often, implicit data is more useful here, and so we are going to treat Movielens as an implicit system. This means that every movie a user watched is a positive example, and every movie they have not seen is an implicit negative example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sawo1x8kQk9b"
   },
   "source": [
    "## Imports\n",
    "\n",
    "\n",
    "Let's first get our imports out of the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-19T23:54:09.971545Z",
     "iopub.status.busy": "2020-11-19T23:54:09.968706Z",
     "iopub.status.idle": "2020-11-19T23:54:15.327905Z",
     "shell.execute_reply": "2020-11-19T23:54:15.327244Z"
    },
    "id": "0vJOdh9WbTpd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement scann\n",
      "ERROR: No matching distribution found for scann\n"
     ]
    }
   ],
   "source": [
    "!pip install -q tensorflow-recommenders\n",
    "!pip install -q --upgrade tensorflow-datasets\n",
    "# !pip install -q scann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-19T23:54:15.333605Z",
     "iopub.status.busy": "2020-11-19T23:54:15.332879Z",
     "iopub.status.idle": "2020-11-19T23:54:23.005957Z",
     "shell.execute_reply": "2020-11-19T23:54:23.005309Z"
    },
    "id": "SZGYDaF-m5wZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-19T23:54:23.010486Z",
     "iopub.status.busy": "2020-11-19T23:54:23.009830Z",
     "iopub.status.idle": "2020-11-19T23:54:23.373194Z",
     "shell.execute_reply": "2020-11-19T23:54:23.372587Z"
    },
    "id": "BxQ_hy7xPH3N"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PAqjR4a1RR4"
   },
   "source": [
    "## Preparing the dataset\n",
    "\n",
    "Let's first have a look at the data.\n",
    "\n",
    "We use the MovieLens dataset from [Tensorflow Datasets](https://www.tensorflow.org/datasets). Loading `movielens/100k_ratings` yields a `tf.data.Dataset` object containing the ratings data and loading `movielens/100k_movies` yields a `tf.data.Dataset` object containing only the movies data.\n",
    "\n",
    "Note that since the MovieLens dataset does not have predefined splits, all data are under `train` split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-19T23:54:23.378511Z",
     "iopub.status.busy": "2020-11-19T23:54:23.377856Z",
     "iopub.status.idle": "2020-11-19T23:55:34.157674Z",
     "shell.execute_reply": "2020-11-19T23:55:34.157128Z"
    },
    "id": "aaQhqcLGP0jL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset movielens/100k-ratings/0.1.0 (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\Dan Ofer\\tensorflow_datasets\\movielens\\100k-ratings\\0.1.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca3d0454f674d389b7495aaba49df63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Dl Completed...'), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb34162a83f43a4921c2752669d7b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Dl Size...'), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4307807fd8405294037b626b1049d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Extraction completed...'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to C:\\/Users/Dan Ofer/tensorflow_datasets/movielens/100k-ratings/0.1.0.incompleteA8VTUO/movielens-train.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2db2e2e7e2554f68aec8208a5a0cad17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset movielens downloaded and prepared to C:\\Users\\Dan Ofer\\tensorflow_datasets\\movielens\\100k-ratings\\0.1.0. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[1mDownloading and preparing dataset movielens/100k-movies/0.1.0 (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\Dan Ofer\\tensorflow_datasets\\movielens\\100k-movies\\0.1.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "565f4ab7f3434a858fc902ac5fb6b1f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Dl Completed...'), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c36cc790b9b1476fb6338ef5f5f865fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Dl Size...'), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25af2a26736d45588b882d0e823c1b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Extraction completed...'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to C:\\/Users/Dan Ofer/tensorflow_datasets/movielens/100k-movies/0.1.0.incomplete8B9QQX/movielens-train.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65688ddb0d9248e8a8b04a17ca9047ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1682.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset movielens downloaded and prepared to C:\\Users\\Dan Ofer\\tensorflow_datasets\\movielens\\100k-movies\\0.1.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# # Ratings data.\n",
    "# ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
    "# # Features of all the available movies.\n",
    "# movies = tfds.load(\"movielens/100k-movies\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: {bucketized_user_age: (), movie_genres: (None,), movie_id: (), movie_title: (), raw_user_age: (), timestamp: (), user_gender: (), user_id: (), user_occupation_label: (), user_occupation_text: (), user_rating: (), user_zip_code: ()}, types: {bucketized_user_age: tf.float32, movie_genres: tf.int64, movie_id: tf.string, movie_title: tf.string, raw_user_age: tf.float32, timestamp: tf.int64, user_gender: tf.bool, user_id: tf.string, user_occupation_label: tf.int64, user_occupation_text: tf.string, user_rating: tf.float32, user_zip_code: tf.string}>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: {movie_genres: (None,), movie_id: (), movie_title: ()}, types: {movie_genres: tf.int64, movie_id: tf.string, movie_title: tf.string}>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRHorm8W1yf3"
   },
   "source": [
    "The ratings dataset returns a dictionary of movie id, user id, the assigned rating, timestamp, movie information, and user information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-19T23:55:34.162932Z",
     "iopub.status.busy": "2020-11-19T23:55:34.162262Z",
     "iopub.status.idle": "2020-11-19T23:55:34.213467Z",
     "shell.execute_reply": "2020-11-19T23:55:34.213898Z"
    },
    "id": "_1-KQV2ynMdh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bucketized_user_age': 45.0,\n",
      " 'movie_genres': array([7], dtype=int64),\n",
      " 'movie_id': b'357',\n",
      " 'movie_title': b\"One Flew Over the Cuckoo's Nest (1975)\",\n",
      " 'raw_user_age': 46.0,\n",
      " 'timestamp': 879024327,\n",
      " 'user_gender': True,\n",
      " 'user_id': b'138',\n",
      " 'user_occupation_label': 4,\n",
      " 'user_occupation_text': b'doctor',\n",
      " 'user_rating': 4.0,\n",
      " 'user_zip_code': b'53211'}\n"
     ]
    }
   ],
   "source": [
    "for x in ratings.take(1).as_numpy_iterator():\n",
    "  pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGLGCjSt_q96"
   },
   "source": [
    "The movies dataset contains the movie id, movie title, and data on what genres it belongs to. Note that the genres are encoded with integer labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-19T23:55:34.218885Z",
     "iopub.status.busy": "2020-11-19T23:55:34.218204Z",
     "iopub.status.idle": "2020-11-19T23:55:34.248818Z",
     "shell.execute_reply": "2020-11-19T23:55:34.249261Z"
    },
    "id": "kHLsIHhw_x1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie_genres': array([4], dtype=int64),\n",
      " 'movie_id': b'1681',\n",
      " 'movie_title': b'You So Crazy (1994)'}\n"
     ]
    }
   ],
   "source": [
    "for x in movies.take(1).as_numpy_iterator():\n",
    "  pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUdT-f4RxMKs"
   },
   "source": [
    "In this example, we're going to focus on the ratings data. Other tutorials explore how to use the movie information data as well to improve the model quality.\n",
    "\n",
    "We keep only the `user_id`, and `movie_title` fields in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-19T23:55:34.255269Z",
     "iopub.status.busy": "2020-11-19T23:55:34.254434Z",
     "iopub.status.idle": "2020-11-19T23:55:34.303953Z",
     "shell.execute_reply": "2020-11-19T23:55:34.303149Z"
    },
    "id": "uhbEvPJqxLec"
   },
   "outputs": [],
   "source": [
    "ratings = ratings.map(lambda x: {\n",
    "    \"movie_title\": x[\"movie_title\"],\n",
    "    \"user_id\": x[\"user_id\"],\n",
    "})\n",
    "movies = movies.map(lambda x: x[\"movie_title\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iu4XSa_G1nyN"
   },
   "source": [
    "To fit and evaluate the model, we need to split it into a training and evaluation set. In an industrial recommender system, this would most likely be done by time: the data up to time $T$ would be used to predict interactions after $T$.\n",
    "\n",
    "\n",
    "In this simple example, however, let's use a random split, putting 80% of the ratings in the train set, and 20% in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-19T23:55:34.309737Z",
     "iopub.status.busy": "2020-11-19T23:55:34.308925Z",
     "iopub.status.idle": "2020-11-19T23:55:34.313579Z",
     "shell.execute_reply": "2020-11-19T23:55:34.313049Z"
    },
    "id": "rS0eDfkjnjJL"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVi1HJfR9D7H"
   },
   "source": [
    "Let's also figure out unique user ids and movie titles present in the data. \n",
    "\n",
    "This is important because we need to be able to map the raw values of our categorical features to embedding vectors in our models. To do that, we need a vocabulary that maps a raw feature value to an integer in a contiguous range: this allows us to look up the corresponding embeddings in our embedding tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-19T23:55:34.319472Z",
     "iopub.status.busy": "2020-11-19T23:55:34.318762Z",
     "iopub.status.idle": "2020-11-19T23:55:38.398905Z",
     "shell.execute_reply": "2020-11-19T23:55:38.399392Z"
    },
    "id": "MKROCiPo_5LJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b\"'Til There Was You (1997)\", b'1-900 (1994)',\n",
       "       b'101 Dalmatians (1996)', b'12 Angry Men (1957)', b'187 (1997)',\n",
       "       b'2 Days in the Valley (1996)',\n",
       "       b'20,000 Leagues Under the Sea (1954)',\n",
       "       b'2001: A Space Odyssey (1968)',\n",
       "       b'3 Ninjas: High Noon At Mega Mountain (1998)',\n",
       "       b'39 Steps, The (1935)'], dtype=object)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_titles = movies.batch(1_000)\n",
    "user_ids = ratings.batch(1_000_000).map(lambda x: x[\"user_id\"])\n",
    "\n",
    "unique_movie_titles = np.unique(np.concatenate(list(movie_titles)))\n",
    "unique_user_ids = np.unique(np.concatenate(list(user_ids)))\n",
    "\n",
    "unique_movie_titles[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCi-seR86qqa"
   },
   "source": [
    "## Implementing a model\n",
    "\n",
    "Choosing the architecure of our model a key part of modelling.\n",
    "\n",
    "Because we are building a two-tower retrieval model, we can build each tower separately and then combine them in the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z20PyfSXP3Um"
   },
   "source": [
    "### The query tower\n",
    "\n",
    "Let's start with the query tower.\n",
    "\n",
    "The first step is to decide on the dimensionality of the query and candidate representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-19T23:55:38.404100Z",
     "iopub.status.busy": "2020-11-19T23:55:38.403412Z",
     "iopub.status.idle": "2020-11-19T23:55:38.405427Z",
     "shell.execute_reply": "2020-11-19T23:55:38.405826Z"
    },
    "id": "QbIy1FP8aCTq"
   },
   "outputs": [],
   "source": [
    "embedding_dimension = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJYwjpLRaEzj"
   },
   "source": [
    "Higher values will correspond to models that may be more accurate, but will also be slower to fit and more prone to overfitting.\n",
    "\n",
    "The second is to define the model itself. Here, we're going to use Keras preprocessing layers to first convert user ids to integers, and then convert those to user embeddings via an `Embedding` layer. Note that we use the list of unique user ids we computed earlier as a vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-19T23:55:38.411279Z",
     "iopub.status.busy": "2020-11-19T23:55:38.410595Z",
     "iopub.status.idle": "2020-11-19T23:55:38.434393Z",
     "shell.execute_reply": "2020-11-19T23:55:38.434901Z"
    },
    "id": "kHQZJEhXP93N"
   },
   "outputs": [],
   "source": [
    "user_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "      vocabulary=unique_user_ids, mask_token=None),\n",
    "  # We add an additional embedding to account for unknown tokens.\n",
    "  tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qvo2pEcdaiec"
   },
   "source": [
    "A simple model like this corresponds exactly to a classic [matrix factorization](https://ieeexplore.ieee.org/abstract/document/4781121) approach. While defining a subclass of `tf.keras.Model` for this simple model might be overkill, we can easily extend it to an arbitrarily complex model using standard Keras components, as long as we return an `embedding_dimension`-wide output at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dG4YFy9SQ08d"
   },
   "source": [
    "### The candidate tower\n",
    "\n",
    "We can do the same with the candidate tower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-19T23:55:38.442915Z",
     "iopub.status.busy": "2020-11-19T23:55:38.442232Z",
     "iopub.status.idle": "2020-11-19T23:55:38.450293Z",
     "shell.execute_reply": "2020-11-19T23:55:38.450728Z"
    },
    "id": "qNUwfIJTQ332"
   },
   "outputs": [],
   "source": [
    "movie_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "      vocabulary=unique_movie_titles, mask_token=None),\n",
    "  tf.keras.layers.Embedding(len(unique_movie_titles) + 1, embedding_dimension)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r10RiPtqVIAl"
   },
   "source": [
    "### Metrics\n",
    "\n",
    "In our training data we have positive (user, movie) pairs. To figure out how good our model is, we need to compare the affinity score that the model calculates for this pair to the scores of all the other possible candidates: if the score for the positive pair is higher than for all other candidates, our model is highly accurate.\n",
    "\n",
    "To do this, we can use the `tfrs.metrics.FactorizedTopK` metric. The metric has one required argument: the dataset of candidates that are used as implicit negatives for evaluation.\n",
    "\n",
    "In our case, that's the `movies` dataset, converted into embeddings via our movie model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-19T23:55:38.455869Z",
     "iopub.status.busy": "2020-11-19T23:55:38.455118Z",
     "iopub.status.idle": "2020-11-19T23:55:38.526325Z",
     "shell.execute_reply": "2020-11-19T23:55:38.525724Z"
    },
    "id": "1dLDL6pZVPO8"
   },
   "outputs": [],
   "source": [
    "metrics = tfrs.metrics.FactorizedTopK(\n",
    "  candidates=movies.batch(128).map(movie_model)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCaCqJsXSkCo"
   },
   "source": [
    "### Loss\n",
    "\n",
    "The next component is the loss used to train our model. TFRS has several loss layers and tasks to make this easy.\n",
    "\n",
    "In this instance, we'll make use of the `Retrieval` task object: a convenience wrapper that bundles together the loss function and metric computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-19T23:55:38.534258Z",
     "iopub.status.busy": "2020-11-19T23:55:38.533537Z",
     "iopub.status.idle": "2020-11-19T23:55:38.535402Z",
     "shell.execute_reply": "2020-11-19T23:55:38.535806Z"
    },
    "id": "tJ61Iz2QTBw3"
   },
   "outputs": [],
   "source": [
    "task = tfrs.tasks.Retrieval(\n",
    "  metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-3xFC-1cbz0"
   },
   "source": [
    "The task itself is a Keras layer that takes the query and candidate embeddings as arguments, and returns the computed loss: we'll use that to implement the model's training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZUFeSlWRHGx"
   },
   "source": [
    "### The full model\n",
    "\n",
    "We can now put it all together into a model. TFRS exposes a base model class (`tfrs.models.Model`) which streamlines bulding models: all we need to do is to set up the components in the `__init__` method, and implement the `compute_loss` method, taking in the raw features and returning a loss value.\n",
    "\n",
    "The base model will then take care of creating the appropriate training loop to fit our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-19T23:55:38.542749Z",
     "iopub.status.busy": "2020-11-19T23:55:38.542072Z",
     "iopub.status.idle": "2020-11-19T23:55:38.543933Z",
     "shell.execute_reply": "2020-11-19T23:55:38.544345Z"
    },
    "id": "8n7c5CHFp0ow"
   },
   "outputs": [],
   "source": [
    "class MovielensModel(tfrs.Model):\n",
    "\n",
    "  def __init__(self, user_model, movie_model):\n",
    "    super().__init__()\n",
    "    self.movie_model: tf.keras.Model = movie_model\n",
    "    self.user_model: tf.keras.Model = user_model\n",
    "    self.task: tf.keras.layers.Layer = task\n",
    "\n",
    "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "    # We pick out the user features and pass them into the user model.\n",
    "    user_embeddings = self.user_model(features[\"user_id\"])\n",
    "    # And pick out the movie features and pass them into the movie model,\n",
    "    # getting embeddings back.\n",
    "    positive_movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
    "\n",
    "    # The task computes the loss and the metrics.\n",
    "    return self.task(user_embeddings, positive_movie_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7B8PdfNqyuN"
   },
   "source": [
    "The `tfrs.Model` base class is a simply convenience class: it allows us to compute both training and test losses using the same method.\n",
    "\n",
    "Under the hood, it's still a plain Keras model. You could achieve the same functionality by inheriting from `tf.keras.Model` and overriding the `train_step` and `test_step` functions (see [the guide](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit) for details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-19T23:55:38.555145Z",
     "iopub.status.busy": "2020-11-19T23:55:38.554389Z",
     "iopub.status.idle": "2020-11-19T23:55:38.556388Z",
     "shell.execute_reply": "2020-11-19T23:55:38.556792Z"
    },
    "id": "Z3QywMjqsH4F"
   },
   "outputs": [],
   "source": [
    "class NoBaseClassMovielensModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, user_model, movie_model):\n",
    "    super().__init__()\n",
    "    self.movie_model: tf.keras.Model = movie_model\n",
    "    self.user_model: tf.keras.Model = user_model\n",
    "    self.task: tf.keras.layers.Layer = task\n",
    "\n",
    "  def train_step(self, features: Dict[Text, tf.Tensor]) -> tf.Tensor:\n",
    "\n",
    "    # Set up a gradient tape to record gradients.\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "      # Loss computation.\n",
    "      user_embeddings = self.user_model(features[\"user_id\"])\n",
    "      positive_movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
    "      loss = self.task(user_embeddings, positive_movie_embeddings)\n",
    "\n",
    "      # Handle regularization losses as well.\n",
    "      regularization_loss = sum(self.losses)\n",
    "\n",
    "      total_loss = loss + regularization_loss\n",
    "\n",
    "    gradients = tape.gradient(total_loss, self.trainable_variables)\n",
    "    self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "    metrics = {metric.name: metric.result() for metric in self.metrics}\n",
    "    metrics[\"loss\"] = loss\n",
    "    metrics[\"regularization_loss\"] = regularization_loss\n",
    "    metrics[\"total_loss\"] = total_loss\n",
    "\n",
    "    return metrics\n",
    "\n",
    "  def test_step(self, features: Dict[Text, tf.Tensor]) -> tf.Tensor:\n",
    "\n",
    "    # Loss computation.\n",
    "    user_embeddings = self.user_model(features[\"user_id\"])\n",
    "    positive_movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
    "    loss = self.task(user_embeddings, positive_movie_embeddings)\n",
    "\n",
    "    # Handle regularization losses as well.\n",
    "    regularization_loss = sum(self.losses)\n",
    "\n",
    "    total_loss = loss + regularization_loss\n",
    "\n",
    "    metrics = {metric.name: metric.result() for metric in self.metrics}\n",
    "    metrics[\"loss\"] = loss\n",
    "    metrics[\"regularization_loss\"] = regularization_loss\n",
    "    metrics[\"total_loss\"] = total_loss\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHnl2nHMs_E0"
   },
   "source": [
    "In these tutorials, however, we stick to using the `tfrs.Model` base class to keep our focus on modelling and abstract away some of the boilerplate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDN_LJGlnRGo"
   },
   "source": [
    "## Fitting and evaluating\n",
    "\n",
    "After defining the model, we can use standard Keras fitting and evaluation routines to fit and evaluate the model.\n",
    "\n",
    "Let's first instantiate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-19T23:55:38.563618Z",
     "iopub.status.busy": "2020-11-19T23:55:38.562890Z",
     "iopub.status.idle": "2020-11-19T23:55:38.580740Z",
     "shell.execute_reply": "2020-11-19T23:55:38.581184Z"
    },
    "id": "aW63YaqP2wCf"
   },
   "outputs": [],
   "source": [
    "model = MovielensModel(user_model, movie_model)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nma0vc2XdN5g"
   },
   "source": [
    "Then shuffle, batch, and cache the training and evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-19T23:55:38.585846Z",
     "iopub.status.busy": "2020-11-19T23:55:38.585173Z",
     "iopub.status.idle": "2020-11-19T23:55:38.589474Z",
     "shell.execute_reply": "2020-11-19T23:55:38.588968Z"
    },
    "id": "53QJwY1gUnfv"
   },
   "outputs": [],
   "source": [
    "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8mHTxKAdTJO"
   },
   "source": [
    "Then train the  model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-19T23:55:38.595867Z",
     "iopub.status.busy": "2020-11-19T23:55:38.595164Z",
     "iopub.status.idle": "2020-11-19T23:55:48.304441Z",
     "shell.execute_reply": "2020-11-19T23:55:48.303920Z"
    },
    "id": "ZxPntlT8EFOZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['counter:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['counter:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['counter:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['counter:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 2s 238ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0011 - factorized_top_k/top_5_categorical_accuracy: 0.0090 - factorized_top_k/top_10_categorical_accuracy: 0.0190 - factorized_top_k/top_50_categorical_accuracy: 0.0961 - factorized_top_k/top_100_categorical_accuracy: 0.1735 - loss: 69885.1072 - regularization_loss: 0.0000e+00 - total_loss: 69885.1072\n",
      "Epoch 2/3\n",
      "10/10 [==============================] - 2s 224ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0028 - factorized_top_k/top_5_categorical_accuracy: 0.0187 - factorized_top_k/top_10_categorical_accuracy: 0.0374 - factorized_top_k/top_50_categorical_accuracy: 0.1686 - factorized_top_k/top_100_categorical_accuracy: 0.2919 - loss: 67523.3707 - regularization_loss: 0.0000e+00 - total_loss: 67523.3707\n",
      "Epoch 3/3\n",
      "10/10 [==============================] - 2s 219ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0035 - factorized_top_k/top_5_categorical_accuracy: 0.0222 - factorized_top_k/top_10_categorical_accuracy: 0.0457 - factorized_top_k/top_50_categorical_accuracy: 0.1877 - factorized_top_k/top_100_categorical_accuracy: 0.3158 - loss: 66302.9609 - regularization_loss: 0.0000e+00 - total_loss: 66302.9609\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f04c679ca20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(cached_train, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsluR8audV9W"
   },
   "source": [
    "As the model trains, the loss is falling and a set of top-k retrieval metrics is updated. These tell us whether the true positive is in the top-k retrieved items from the entire candidate set. For example, a top-5 categorical accuracy metric of 0.2 would tell us that, on average, the true positive is in the top 5 retrieved items 20% of the time.\n",
    "\n",
    "Note that, in this example, we evaluate the metrics during training as well as evaluation. Because this can be quite slow with large candidate sets, it may be prudent to turn metric calculation off in training, and only run it in evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Gxp5RLFcv64"
   },
   "source": [
    "Finally, we can evaluate our model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-19T23:55:48.310172Z",
     "iopub.status.busy": "2020-11-19T23:55:48.309127Z",
     "iopub.status.idle": "2020-11-19T23:55:49.757116Z",
     "shell.execute_reply": "2020-11-19T23:55:49.757537Z"
    },
    "id": "W-zu6HLODNeI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 118ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0010 - factorized_top_k/top_5_categorical_accuracy: 0.0097 - factorized_top_k/top_10_categorical_accuracy: 0.0226 - factorized_top_k/top_50_categorical_accuracy: 0.1244 - factorized_top_k/top_100_categorical_accuracy: 0.2327 - loss: 31079.0628 - regularization_loss: 0.0000e+00 - total_loss: 31079.0628\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factorized_top_k/top_1_categorical_accuracy': 0.0010000000474974513,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 0.009650000371038914,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.022600000724196434,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 0.12439999729394913,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 0.23270000517368317,\n",
       " 'loss': 28244.76953125,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 28244.76953125}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKZyP9A1dxit"
   },
   "source": [
    "Test set performance is much worse than training performance. This is due to two factors:\n",
    "\n",
    "1. Our model is likely to perform better on the data that it has seen, simply because it can memorize it. This overfitting phenomenon is especially strong when models have many parameters. It can be mediated by model regularization and use of user and movie features that help the model generalize better to unseen data.\n",
    "2. The model is re-recommending some of users' already watched movies. These known-positive watches can crowd out test movies out of top K recommendations.\n",
    "\n",
    "The second phenomenon can be tackled by excluding previously seen movies from test recommendations. This approach is relatively common in the recommender systems literature, but we don't follow it in these tutorials. If not recommending past watches is important, we should expect appropriately specified models to learn this behaviour automatically from past user history and contextual information. Additionally, it is often appropriate to recommend the same item multiple times (say, an evergreen TV series or a regularly purchased item)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NB2v43NJU3Xf"
   },
   "source": [
    "## Making predictions\n",
    "\n",
    "Now that we have a model, we would like to be able to make predictions. We can use the `tfrs.layers.factorized_top_k.BruteForce` layer to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-19T23:55:49.783831Z",
     "iopub.status.busy": "2020-11-19T23:55:49.782958Z",
     "iopub.status.idle": "2020-11-19T23:55:49.986051Z",
     "shell.execute_reply": "2020-11-19T23:55:49.986495Z"
    },
    "id": "IRD6bEtZW_8j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for user 42: [b'Bridges of Madison County, The (1995)'\n",
      " b'Father of the Bride Part II (1995)' b'Rudy (1993)']\n"
     ]
    }
   ],
   "source": [
    "# Create a model that takes in raw query features, and\n",
    "index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
    "# recommends movies out of the entire movies dataset.\n",
    "index.index(movies.batch(100).map(model.movie_model), movies)\n",
    "\n",
    "# Get recommendations.\n",
    "_, titles = index(tf.constant([\"42\"]))\n",
    "print(f\"Recommendations for user 42: {titles[0, :3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOoLv6ZMKg0L"
   },
   "source": [
    "Of course, the `BruteForce` layer is going to be too slow to serve a model with many possible candidates. The following sections shows how to speed this up by using an approximate retrieval index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFlvp5DK4Ow8"
   },
   "source": [
    "## Model serving\n",
    "\n",
    "After the model is trained, we need a way to deploy it.\n",
    "\n",
    "In a two-tower retrieval model, serving has two components:\n",
    "\n",
    "- a serving query model, taking in features of the query and transforming them into a query embedding, and\n",
    "- a serving candidate model. This most often takes the form of an approximate nearest neighbours (ANN) index which allows fast approximate lookup of candidates in response to a query produced by the query model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmhfltdpWZ06"
   },
   "source": [
    "In TFRS, both components can be packaged into a single exportable model, giving us a model that takes the raw user id and returns the titles of top movies for that user. This is done via exporting the model to a `SavedModel` format, which makes it possible to serve using [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving).\n",
    "\n",
    "To deploy a model like this, we simply export the `BruteForce` layer we created above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-19T23:55:49.993195Z",
     "iopub.status.busy": "2020-11-19T23:55:49.991304Z",
     "iopub.status.idle": "2020-11-19T23:55:50.846812Z",
     "shell.execute_reply": "2020-11-19T23:55:50.847256Z"
    },
    "id": "oJkRNBfCW5_E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp6r6itle7/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp6r6itle7/model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations: [b'Bridges of Madison County, The (1995)'\n",
      " b'Father of the Bride Part II (1995)' b'Rudy (1993)']\n"
     ]
    }
   ],
   "source": [
    "# # Export the query model.\n",
    "# with tempfile.TemporaryDirectory() as tmp:\n",
    "#   path = os.path.join(tmp, \"model\")\n",
    "\n",
    "#   # Save the index.\n",
    "#   index.save(path)\n",
    "\n",
    "#   # Load it back; can also be done in TensorFlow Serving.\n",
    "#   loaded = tf.keras.models.load_model(path)\n",
    "\n",
    "#   # Pass a user id in, get top predicted movie titles back.\n",
    "#   scores, titles = loaded([\"42\"])\n",
    "\n",
    "#   print(f\"Recommendations: {titles[0][:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBBGodbE5ENC"
   },
   "source": [
    "We can also export an approximate retrieval index to speed up predictions. This will make it possible to efficiently surface recommendations from sets of tens of millions of candidates.\n",
    "\n",
    "To do so, we can use the `scann` package. This is an optional dependency of TFRS, and we installed it separately at the beginning of this tutorial by calling `!pip install -q scann`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15PtZqoO5k_k"
   },
   "source": [
    "Once installed we can use the TFRS `ScaNN` layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-19T23:55:50.871341Z",
     "iopub.status.busy": "2020-11-19T23:55:50.870436Z",
     "iopub.status.idle": "2020-11-19T23:55:51.206268Z",
     "shell.execute_reply": "2020-11-19T23:55:51.206720Z"
    },
    "id": "rTz8yxyp5uwU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_recommenders.layers.factorized_top_k.ScaNN at 0x7f04c6617dd8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scann_index = tfrs.layers.factorized_top_k.ScaNN(model.user_model)\n",
    "# scann_index.index(movies.batch(100).map(model.movie_model), movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpLnoUv256bS"
   },
   "source": [
    "This layer will perform _approximate_ lookups: this makes retrieval slightly less accurate, but orders of magnitude faster on large candidate sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-19T23:55:51.216049Z",
     "iopub.status.busy": "2020-11-19T23:55:51.215339Z",
     "iopub.status.idle": "2020-11-19T23:55:51.274544Z",
     "shell.execute_reply": "2020-11-19T23:55:51.275002Z"
    },
    "id": "Te_MGu1Q6JrU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for user 42: [b'Bridges of Madison County, The (1995)'\n",
      " b'Father of the Bride Part II (1995)' b'Rudy (1993)']\n"
     ]
    }
   ],
   "source": [
    "# # Get recommendations.\n",
    "# _, titles = scann_index(tf.constant([\"42\"]))\n",
    "# print(f\"Recommendations for user 42: {titles[0, :3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Izo0IUMA6TQm"
   },
   "source": [
    "Exporting it for serving is as easy as exporting the `BruteForce` layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-19T23:55:51.281935Z",
     "iopub.status.busy": "2020-11-19T23:55:51.281218Z",
     "iopub.status.idle": "2020-11-19T23:55:52.228819Z",
     "shell.execute_reply": "2020-11-19T23:55:52.229297Z"
    },
    "id": "K7NUqgxU6W_T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpduzk4yez/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpduzk4yez/model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations: [b'Bridges of Madison County, The (1995)'\n",
      " b'Father of the Bride Part II (1995)' b'Rudy (1993)']\n"
     ]
    }
   ],
   "source": [
    "# # Export the query model.\n",
    "# with tempfile.TemporaryDirectory() as tmp:\n",
    "#   path = os.path.join(tmp, \"model\")\n",
    "\n",
    "#   # Save the index.\n",
    "#   scann_index.save(\n",
    "#       path,\n",
    "#       options=tf.saved_model.SaveOptions(namespace_whitelist=[\"Scann\"])\n",
    "#   )\n",
    "\n",
    "#   # Load it back; can also be done in TensorFlow Serving.\n",
    "#   loaded = tf.keras.models.load_model(path)\n",
    "\n",
    "#   # Pass a user id in, get top predicted movie titles back.\n",
    "#   scores, titles = loaded([\"42\"])\n",
    "\n",
    "#   print(f\"Recommendations: {titles[0][:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAoYm6Rk6xLI"
   },
   "source": [
    "To learn more about using and tuning fast approximate retrieval models, have a look at our [efficient serving](https://tensorflow.org/recommenders/examples/efficient_serving) tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ws4w5jQ3fX87"
   },
   "source": [
    "## Next steps\n",
    "\n",
    "This concludes the retrieval tutorial.\n",
    "\n",
    "To expand on what is presented here, have a look at:\n",
    "\n",
    "1. Learning multi-task models: jointly optimizing for ratings and clicks.\n",
    "2. Using movie metadata: building a more complex movie model to alleviate cold-start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0UFyyocksOF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "basic_retrieval.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
